20230607 Machine Learning* Ensemble (앙상불) - 단일 머신러닝보다 좋은 성능 (정확도, 속도) - 다수의 약한 머신러닝 모델을 종합하여 최종 모델로 사용 - 다수의 모델 구축 -> 학습데이터 분할 / 재사용 필요 - 단일 모델들의 취합 방법, 연결 방법에 따라 종류 구분  * Bagging - with replacement, independent    -> 데이터가 중복해서 여러 번 반영될 수 있도록 데이터를 셋 설정  * Boosting - without replacement, sequential    -> 동일하게 데이터 셋을 설정하고 각각의 데이터에 가중치를 다양하게 줘서 모델별로 직렬 처리한 다음 가장 좋은 것을 뽑아냄* k-Fold Cross Validation - 학습데이터(X_train)를 k개로 분할 - 검증용 분할 데이터로 순환식 사용 - 다수의 머신러닝 결과를 (통상) 평균하여 결과 도출* Bootstrapping - 학습데이터로부터 n개의 학습데이터 세트 구성 - 랜덤 샘플링, with replacement  * 동일 데이터 ,동일 데이터 세트 중복 포함 가능! -> Bootstrap Aggregation (Bagging)을 적용해 구성한 모델의 취합    => Bagging    목적 : 과적합(Overfit) 회피    Weak(약한, 정확도 낮은) 모델 여러 개를 개발하고 취합    중복 적용이 될 경우 사용되지 않는 데이터 발생    => Out of Bag (OOB) 데이터 - Variance vs bias (Variance : 예측값의 범위 / Bias : 예측/실제 차이)   * Overfit : high variance, low bias   * Underfit : low variance, high bias* Boosting - 다수의 학습데이터 셋 구성, without replacement - 병렬 처리가 힘든 모델! - 순차적 적용  * 모델1 개발, 개발 결과를 모델2 개발 입력값으로 활용  * 정확도가 낮은 feature에 대해 가중치 증가  * 증가된 가중치 -> 적극적인 학습 유도( (AdaBoost, Adaptive Boosting) - Gradient Tree Boosting   * 순차적 모델 개발 과정에서의 가중치 조절에 Gradient Decent 사용 - Histogram-Based Gradient Boosting   * 분포도 사용, 예측값 구간화 -> 속도 향상* Voting - 다수의 개발 모델의 취합 방법   * 개별 모델의 예측값을 기초로 '투표' 실시     최종 예측값 결정 - 개별 모델 / 예측값별 다른 가중치 사용 가능   * score 값이 높은 모델의 투표값을 더 신뢰 - 다양한 종류의 갭려 모델 개발, voting 가능* Stacked - 다수의 개발 모델의 취합 방법   * 개별 모델의 예측값 축적   * 최종 적용 모델의 입력값으로 사용 - 다양한 종류의 개별 모델 stacking 가능* Clustering(군집화) - 비지도 학습(Density Estimation, Manifold Learning...) - Target, Label 값이 없는 데이터(비정형 데이터) - K-Means   * 군집 갯수를 미리 정해주어야 함   * 정해진 군집 갯수만큼의 랜덤 샘플로부터 시작   * 데이터를 더해가면서 평균값(centroid)과 가까운 것들끼리 군집화   * 평균값 지속적 업데이트   * 군집 크기 비숫, 군집 모양 원형 - MiniBatch K-Means   * 데이터 개수가 많은 경우 배치(여러 개)로 묶어서 한꺼번에 처리 - Affinity Propagation  * preference : 기준 데이터 허용 갯수  * damping : 기준 데이터 적합성 점수 영향력 조절  * 기준 데이터(exemplar)로부터 가까운 것들끼리 군집화  * 데이터를 더해가면서 기준 데이터의 적합성 검토, 업데이트 - Mean Shift  * bandwidth : 검사 영역 설정  * 평균값에서 bandwidth만큼의 범위 내의 값으로 군집화  * 개수를 미리 정하지 않는 부분이 K-Means와의 차이 - Spectral Clustering  * 군집의 갯수를 입력 (k-means와 유사)  * 근접 행렬(affinity matrix)를 만들어 k-means와 같은 방식 적용 - Agglomerate Clustering  * 계층화된 방식(tree)을 이용하여 군집 형성, 상향식!(아래 -> 위)  * BIRCH - DBSCAN   * min_samples : 밀집의 정도를 결정  * eps : 데이터 사이 거리  * 데이터의 밀집도(Density)에 따라 군집 형성  * 다양한 군집 형태  * OPTICS : eps 범주화(range) (DBSCAN : eps를 하나의 값으로 준다) - Scores(평가), sklearn.metrics  * Homogeneity : 하나의 군집(Cluster)에 하나의 분류(Class,Label)가 들어있음  * Completeness : 하나의 분류(Class, Label)에 속하는 데이터가 하나의 군집(Cluster)에 속함  * 범위 : 0.0 ~ 1.0  * 높을수록 좋은 점수