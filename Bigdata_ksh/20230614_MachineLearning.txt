20230614 Machine Learning* Deep Learing (Keras)- CNNInput -> (Convolution -> Pooling) 반복 -> FLATTEN -> Fully Connected -> SOFTMAX  - convolution 옵션  * stride : 이동을 몇 칸씩 할지 결정  * padding : "same"을 적용하면 input, output 사이즈가 동일!  * data_format : channels_last가 디폴트, channels_first로 변경 가능                           ex> (가로, 세로, 채널) -> (채널, 가로, 세로)  - convolution, pooling 목적 : 개수를 줄이면서 패턴의 특징을 담아내기 위함* CNN에서 많이 쓰는 형태 - LeNet-5 - AlexNet - GoogleLeNet - ResNet - VGGNet - DNN 복습* Optimizer로 최근 Adam을 쓰는 이유 - 에러 곡선으로 봤을 때 에러값이 더 낮은 구간이 있으나 앞의 얕은 곡선의    최저값을 가리키게 되는 경우 learning_rate를 조절해 더 낮을 구간에 도달할     수 있도록 조정이 가능하므로 Adam을 많이 사용한다!* loss function - 어느 방향으로 예측을 해나가야 하는지를 결정* softmax : 결과값들은 0~1 사이 값으로 정의되며 결과값의 전체 합은 1이된다	    logit이 아니므로 loss함수의 from_logits를 False로 둔다* one-hot encode -> CategoricalCrossentropy 활용 가능!   (SparseCategoricalCrossentropy는 1차원만 가능하므로 엔코딩된 것은 적용되지 않는다!)* model.fit에서 fit의 매개변수 - callbacks=[]  콜백함수는 리스트 형태로 주도록 되어 있음! - 함수 종류  EarlyStopping(monitor='', mode='', min_delta=0.001) LambdaCallback(on_epoch_begin=lambda batch, logs: print('Test batch #', batch))  accuracy는 max 값이 지향점이므로 mode가 max가 된다 loss의 경우 min 값이 지향점이므로 mode가 min이 된다