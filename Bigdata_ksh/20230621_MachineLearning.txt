20230621 Machine Learning* Deep Learing (Keras)- CNN  이미지, 시각화 처리에 주로 활용됨  CNN은 병렬처리가 가능하다- RNN  NLP 자연어 처리에 주로 활용됨  unit 개수 : 여러가지 복잡한 패턴 속에서 적절한 가중치를 찾기 위해                unit 개수를 늘려 다양한 경우를 고려할 수 있음  RNN은 병렬처리가 어려움 (이전 값과의 관계성이 크다)  시그널을 멀리 보내는데 한계가 있다  recurrent kernel 개수는 unit의 제곱 만큼 생성- LSTM, Transformer  GRU : 연산 개수를 줄이고 싶을 때 활용          LSTM만큼의 성능을 낸다  Transformer  step 개념이 없어진다(각각 모든 input의 값을 고려함) => 순서 상관 x  -> 따라서 병렬처리가 가능해진다(RNN 단점 보완)  Attention weights 생성  -> 쿼리에 대한 결과값을 도출하는 정보를 갖고 있는 데이터뭉치  쿼리로 들어오는 것 자체를 Key, Value로 활용  -> Self Attention layer : input의 패턴을 확인하기 위해 활용된다  쿼리로 들어오는 것과 Key, Value가 별도인 경우  -> Base Attention layer  Auto Regressor(자가 회귀 모델)* Generative Adversarial Network (GAN) -> ChatGPT 출시 이후 인기가 떨어짐