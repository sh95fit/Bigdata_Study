{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymysql\n",
    "import dotenv\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalization/Standardization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 경고 무시 코드 추가\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pltconfig_default() :\n",
    "  sns.reset_defaults()\n",
    "  %matplotlib inline\n",
    "\n",
    "pltconfig_default()\n",
    "\n",
    "matplotlib.rcParams\n",
    "\n",
    "matplotlib.rcParams['font.family']\n",
    "\n",
    "current_font_list = matplotlib.rcParams['font.family']\n",
    "\n",
    "font_path = 'C:\\\\Windows\\\\Fonts\\\\batang.ttc'\n",
    "\n",
    "kfont = matplotlib.font_manager.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "matplotlib.rcParams['font.family'] = [kfont] + current_font_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17518, 9) (17518, 45) (17518, 9) (17518, 45) (52578, 9) (52578, 9)\n"
     ]
    }
   ],
   "source": [
    "with open('StandardScalar_final_data', 'rb') as file :\n",
    "  St_NotEncode_data = pickle.load(file)\n",
    "  \n",
    "with open('encoded_StandardScalar_final_data', 'rb') as file :\n",
    "  St_Encode_data = pickle.load(file)\n",
    "  \n",
    "with open('MinMaxScaler_final_data', 'rb') as file :\n",
    "  MM_NotEncode_data = pickle.load(file)\n",
    "\n",
    "with open('encoded_MinMaxScalar_final_data', 'rb') as file :\n",
    "  MM_Encode_data = pickle.load(file)\n",
    "\n",
    "with open('Basic_StandardScalar_final_data', 'rb') as file :\n",
    "  St_Base_data = pickle.load(file)\n",
    "\n",
    "with open('Basic_MinMaxScalar_final_data', 'rb') as file :\n",
    "  MM_Base_data = pickle.load(file)\n",
    "\n",
    "print(St_NotEncode_data.shape, St_Encode_data.shape, MM_NotEncode_data.shape, MM_Encode_data.shape, St_Base_data.shape, MM_Base_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler Not Encode Data\n",
      "(17518, 8) (17518,)\n",
      "(14014, 8) (3504, 8) (14014,) (3504,)\n",
      "=======================================\n",
      "StandardScaler Encode Data\n",
      "(17518, 44) (17518,)\n",
      "(14014, 44) (3504, 44) (14014,) (3504,)\n",
      "=======================================\n",
      "MinMaxScaler Not Encode Data\n",
      "(17518, 8) (17518,)\n",
      "(14014, 8) (3504, 8) (14014,) (3504,)\n",
      "=======================================\n",
      "MinMaxScaler Encode Data\n",
      "(17518, 44) (17518,)\n",
      "(14014, 44) (3504, 44) (14014,) (3504,)\n",
      "=======================================\n",
      "StandardScaler Not Encode Data\n",
      "(52578, 8) (52578,)\n",
      "(42062, 8) (10516, 8) (42062,) (10516,)\n",
      "=======================================\n",
      "MinMaxScaler Not Encode Data\n",
      "(52578, 8) (52578,)\n",
      "(42062, 8) (10516, 8) (42062,) (10516,)\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# Feature와 Label 분리하기\n",
    "def Feature_Label(datafile) :\n",
    "    X = datafile.iloc[:,:-1]\n",
    "    y = datafile.iloc[:,-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "print(\"StandardScaler Not Encode Data\")\n",
    "SNE_X, SNE_y = Feature_Label(St_NotEncode_data)\n",
    "print(SNE_X.shape, SNE_y.shape)\n",
    "SNE_X_train, SNE_X_test, SNE_y_train, SNE_y_test = train_test_split(SNE_X, SNE_y, test_size=0.2, random_state=10)\n",
    "print(SNE_X_train.shape, SNE_X_test.shape, SNE_y_train.shape, SNE_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"StandardScaler Encode Data\")\n",
    "SE_X, SE_y = Feature_Label(St_Encode_data)\n",
    "print(SE_X.shape, SE_y.shape)\n",
    "SE_X_train, SE_X_test, SE_y_train, SE_y_test = train_test_split(SE_X, SE_y, test_size=0.2, random_state=10)\n",
    "print(SE_X_train.shape, SE_X_test.shape, SE_y_train.shape, SE_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Not Encode Data\")\n",
    "MMNE_X, MMNE_y = Feature_Label(MM_NotEncode_data)\n",
    "print(MMNE_X.shape, MMNE_y.shape)\n",
    "MMNE_X_train, MMNE_X_test, MMNE_y_train, MMNE_y_test = train_test_split(MMNE_X, MMNE_y, test_size=0.2, random_state=10)\n",
    "print(MMNE_X_train.shape, MMNE_X_test.shape, MMNE_y_train.shape, MMNE_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Encode Data\")\n",
    "MME_X, MME_y = Feature_Label(MM_Encode_data)\n",
    "print(MME_X.shape, MME_y.shape)\n",
    "MME_X_train, MME_X_test, MME_y_train, MME_y_test = train_test_split(MME_X, MME_y, test_size=0.2, random_state=10)\n",
    "print(MME_X_train.shape, MME_X_test.shape, MME_y_train.shape, MME_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"StandardScaler Not Encode Data\")\n",
    "SB_X, SB_y = Feature_Label(St_Base_data)\n",
    "print(SB_X.shape, SB_y.shape)\n",
    "SB_X_train, SB_X_test, SB_y_train, SB_y_test = train_test_split(SB_X, SB_y, test_size=0.2, random_state=10)\n",
    "print(SB_X_train.shape, SB_X_test.shape, SB_y_train.shape, SB_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Not Encode Data\")\n",
    "MMB_X, MMB_y = Feature_Label(MM_Base_data)\n",
    "print(MMB_X.shape, MMB_y.shape)\n",
    "MMB_X_train, MMB_X_test, MMB_y_train, MMB_y_test = train_test_split(MMB_X, MMB_y, test_size=0.2, random_state=10)\n",
    "print(MMB_X_train.shape, MMB_X_test.shape, MMB_y_train.shape, MMB_y_test.shape)\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   44.4052892   -221.11688554   212.52065969 -1876.82016067\n",
      "  8209.02029403   473.13994401  -146.16192902  1762.41715978] 7560.84163112865\n",
      "train score 0.7504642442985028\n",
      "test score 0.7617197657499287\n",
      "=========================================================\n",
      "[   19.24229072  -230.86102868   224.47297876 -1820.39370288\n",
      "  8119.17161671   538.6774515   -169.05498961  1775.80612077] [7413.95333571]\n",
      "train score 0.7500917363992088\n",
      "test score 0.7621429591492306\n",
      "=========================================================\n",
      "[ 1631.59926088 -1742.51745089   577.77515945 -8237.26533606\n",
      " 19949.8977416   1185.43510533 -4235.20923609  9299.63302146] 3207.333982102059\n",
      "train score 0.7504642442985028\n",
      "test score 0.7617197657499287\n",
      "=========================================================\n",
      "[ 1.55614568e+01 -7.27639629e+02  4.65717517e+02 -7.50462210e+03\n",
      "  2.00176929e+04  1.16754186e+03 -6.03012829e+00  8.89914788e+03] [2448.43736052]\n",
      "train score 0.750088233056232\n",
      "test score 0.7617875930831883\n",
      "=========================================================\n",
      "[ 0.16557035  0.03415983 -0.01167191 -0.2723274   1.32830857 10.9063008\n",
      "  0.03251048  1.25167298] 9.131986023428711\n",
      "train score 0.9026441883996681\n",
      "test score 0.9076004904984571\n",
      "=========================================================\n",
      "[ 2.10770108e-01  3.70941209e-03  1.43282805e-02 -2.39608691e-01\n",
      "  1.33519411e+00  1.09879626e+01  6.52556541e-02  1.30119414e+00] [9.16381792]\n",
      "train score 0.9025745768410021\n",
      "test score 0.9074341085517272\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression, SGDRegressor (시계열을 고려하지 않은 경우)\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=1000)] :\n",
    "    m.fit(SNE_X_train, SNE_y_train)\n",
    "\n",
    "    print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(SNE_X_train, SNE_y_train))\n",
    "    print('test score', m.score(SNE_X_test, SNE_y_test))\n",
    "    print('=========================================================')\n",
    "\n",
    "\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=1000)] :\n",
    "    m.fit(MMNE_X_train, MMNE_y_train)\n",
    "\n",
    "    print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(MMNE_X_train, MMNE_y_train))\n",
    "    print('test score', m.score(MMNE_X_test, MMNE_y_test))\n",
    "    print('=========================================================')\n",
    "\n",
    "\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=1000)] :\n",
    "    m.fit(SB_X_train, SB_y_train)\n",
    "\n",
    "    print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(SB_X_train, SB_y_train))\n",
    "    print('test score', m.score(SB_X_test, SB_y_test))\n",
    "    print('=========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate_data 파라미터 세팅\n",
    "timesteps_length = 24\n",
    "New_train_split = 14014 # train set과 test set 비율을 80:20으로 맞췄을 때 경계값\n",
    "Basic_train_split = 42062 # train set과 test set 비율을 80:20으로 맞췄을 때 경계값\n",
    "future_target = 1\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변량 시계열 데이터 (Multivariate Time Series Data)\n",
    "# dataset : 원본 시계열 데이터 (2D Dataframe)\n",
    "# target : 예측 대상 변수 (1D Dataframe)\n",
    "# start_index : 학습 데이터의 시작 인덱스 / end_index : 학습 데이터의 끝 인덱스\n",
    "# history_size : 과거 정보로 사용할 데이터 포인트 수\n",
    "# target_size : 예측 대상으로 사용할 데이터 포인트 수\n",
    "# step : 데이터 포인트 간의 간격\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler Not Encode Data\n",
      "(13990, 24, 8) (3479, 24, 8)\n",
      "=======================================\n",
      "StandardScaler Encode Data\n",
      "(13990, 24, 44) (3479, 24, 44)\n",
      "=======================================\n",
      "MinMaxScaler Not Encode Data\n",
      "(13990, 24, 8) (3479, 24, 8)\n",
      "=======================================\n",
      "MinMaxScaler Encode Data\n",
      "(13990, 24, 44) (3479, 24, 44)\n",
      "=======================================\n",
      "StandardScaler Not Encode Basic Data\n",
      "(42038, 24, 8) (10491, 24, 8)\n",
      "=======================================\n",
      "MinMaxScaler Not Encode Basic Data\n",
      "(42038, 24, 8) (10491, 24, 8)\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임은 열 이름과 인덱스를 가지고 있으나 넘파이 배열의 경우 순수한 다차원 배열로 되어 있어 순서에 따라 접근이 가능하므로 to_numpy 적용이 필요하다\n",
    "# 데이터프레임을 그대로 적용할 경우 KeyError가 발생하지만 넘파이 배열로 변경하게 되면 해당 부분이 문제가 되지 않는다\n",
    "\n",
    "def  to_timeseries_data(X, y, train_split):\n",
    "  X_train, y_train = multivariate_data(X.to_numpy(), y, 0, train_split, timesteps_length, future_target, STEP, single_step=True)\n",
    "  X_test, y_test = multivariate_data(X.to_numpy(), y, train_split, None, timesteps_length, future_target, STEP, single_step=True)\n",
    "  \n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"StandardScaler Not Encode Data\")\n",
    "SNE_X_train, SNE_X_test, SNE_y_train, SNE_y_test = to_timeseries_data(SNE_X, SNE_y, New_train_split)\n",
    "print(SNE_X_train.shape, SNE_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"StandardScaler Encode Data\")\n",
    "SE_X_train, SE_X_test, SE_y_train, SE_y_test = to_timeseries_data(SE_X, SE_y, New_train_split)\n",
    "print(SE_X_train.shape, SE_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Not Encode Data\")\n",
    "MMNE_X_train, MMNE_X_test, MMNE_y_train, MMNE_y_test = to_timeseries_data(MMNE_X, MMNE_y, New_train_split)\n",
    "print(MMNE_X_train.shape, MMNE_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Encode Data\")\n",
    "MME_X_train, MME_X_test, MME_y_train, MME_y_test = to_timeseries_data(MME_X, MME_y, New_train_split)\n",
    "print(MME_X_train.shape, MME_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"StandardScaler Not Encode Basic Data\")\n",
    "SB_X_train, SB_X_test, SB_y_train, SB_y_test = to_timeseries_data(SB_X, SB_y, Basic_train_split)\n",
    "print(SB_X_train.shape, SB_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Not Encode Basic Data\")\n",
    "MMB_X_train, MMB_X_test, MMB_y_train, MMB_y_test = to_timeseries_data(MMB_X, MMB_y, Basic_train_split)\n",
    "print(MMB_X_train.shape, MMB_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "\n",
    "# 적용 테스트\n",
    "\n",
    "# X_train, y_train = multivariate_data(SNE_X.to_numpy(), SNE_y, 0, New_train_split, timesteps_length, future_target, STEP, single_step=True)\n",
    "# X_test, y_test = multivariate_data(SNE_X.to_numpy(), SNE_y, New_train_split, None, timesteps_length, future_target, STEP, single_step=True)\n",
    "\n",
    "# print(X_train.shape, X_test.shape)\n",
    "# print(X_train.shape[0]+X_test.shape[0]) # 기존 17518\n",
    "# print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.8139884208513661\n",
      "test score 0.7747974544539296\n",
      "=========================================================\n",
      "train score 0.8079524498230634\n",
      "test score 0.7682629070028423\n",
      "=========================================================\n",
      "train score 0.8139884078479647\n",
      "test score 0.7747946241856856\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# LinearRegression (시계열 데이터 적용)\n",
    "\n",
    "# SNE(StandardScaler Not Encode)\n",
    "SNE_X_train_2d = SNE_X_train.reshape((SNE_X_train.shape[0], -1))\n",
    "SNE_X_test_2d = SNE_X_test.reshape((SNE_X_test.shape[0], -1))\n",
    "\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=10000), Ridge(alpha=0.1), Lasso(alpha=0.1)] :\n",
    "    m.fit(SNE_X_train_2d, SNE_y_train)\n",
    "\n",
    "    # print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(SNE_X_train_2d, SNE_y_train))\n",
    "    print('test score', m.score(SNE_X_test_2d, SNE_y_test))\n",
    "    print('=========================================================')\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "# SB(StandardScaler Basic)\n",
    "SB_X_train_2d = SB_X_train.reshape((SB_X_train.shape[0], -1))\n",
    "SB_X_test_2d = SB_X_test.reshape((SB_X_test.shape[0], -1))\n",
    "\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=10000), Ridge(alpha=0.1), Lasso(alpha=0.1)] :\n",
    "    m.fit(SB_X_train_2d, SB_y_train)\n",
    "\n",
    "    # print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(SB_X_train_2d, SB_y_train))\n",
    "    print('test score', m.score(SB_X_test_2d, SB_y_test))\n",
    "    print('=========================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
