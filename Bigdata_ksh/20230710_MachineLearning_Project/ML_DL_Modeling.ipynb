{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymysql\n",
    "import dotenv\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalization/Standardization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# 경고 무시 코드 추가\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pltconfig_default() :\n",
    "  sns.reset_defaults()\n",
    "  %matplotlib inline\n",
    "\n",
    "pltconfig_default()\n",
    "\n",
    "matplotlib.rcParams\n",
    "\n",
    "matplotlib.rcParams['font.family']\n",
    "\n",
    "current_font_list = matplotlib.rcParams['font.family']\n",
    "\n",
    "font_path = 'C:\\\\Windows\\\\Fonts\\\\batang.ttc'\n",
    "\n",
    "kfont = matplotlib.font_manager.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "matplotlib.rcParams['font.family'] = [kfont] + current_font_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17422, 9) (17518, 45) (17422, 9) (17518, 45) (52578, 9) (52578, 9)\n"
     ]
    }
   ],
   "source": [
    "# with open('StandardScalar_final_data', 'rb') as file :\n",
    "#   St_NotEncode_data = pickle.load(file)\n",
    "\n",
    "St_NotEncode_data = pd.read_pickle(\"StandardScalar_final_data\")\n",
    "  \n",
    "# with open('encoded_StandardScalar_final_data', 'rb') as file :\n",
    "#   St_Encode_data = pickle.load(file)\n",
    "\n",
    "St_Encode_data = pd.read_pickle(\"encoded_StandardScalar_final_data\")\n",
    "\n",
    "# with open('MinMaxScaler_final_data', 'rb') as file :\n",
    "#   MM_NotEncode_data = pickle.load(file)\n",
    "\n",
    "MM_NotEncode_data = pd.read_pickle(\"MinMaxScaler_final_data\")\n",
    "\n",
    "# with open('encoded_MinMaxScalar_final_data', 'rb') as file :\n",
    "#   MM_Encode_data = pickle.load(file)\n",
    "\n",
    "MM_Encode_data = pd.read_pickle(\"encoded_MinMaxScalar_final_data\")\n",
    "\n",
    "# with open('Basic_StandardScalar_final_data', 'rb') as file :\n",
    "#   St_Base_data = pickle.load(file)\n",
    "\n",
    "St_Base_data = pd.read_pickle(\"Basic_StandardScalar_final_data\")\n",
    "\n",
    "# with open('Basic_MinMaxScalar_final_data', 'rb') as file :\n",
    "#   MM_Base_data = pickle.load(file)\n",
    "\n",
    "MM_Base_data = pd.read_pickle(\"Basic_MinMaxScalar_final_data\")\n",
    "\n",
    "print(St_NotEncode_data.shape, St_Encode_data.shape, MM_NotEncode_data.shape, MM_Encode_data.shape, St_Base_data.shape, MM_Base_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler Not Encode Data\n",
      "(17422, 8) (17422,)\n",
      "(13937, 8) (3485, 8) (13937,) (3485,)\n",
      "=======================================\n",
      "StandardScaler Encode Data\n",
      "(17518, 44) (17518,)\n",
      "(14014, 44) (3504, 44) (14014,) (3504,)\n",
      "=======================================\n",
      "MinMaxScaler Not Encode Data\n",
      "(17422, 8) (17422,)\n",
      "(13937, 8) (3485, 8) (13937,) (3485,)\n",
      "=======================================\n",
      "MinMaxScaler Encode Data\n",
      "(17518, 44) (17518,)\n",
      "(14014, 44) (3504, 44) (14014,) (3504,)\n",
      "=======================================\n",
      "StandardScaler Not Encode Data\n",
      "(52578, 8) (52578,)\n",
      "(42062, 8) (10516, 8) (42062,) (10516,)\n",
      "=======================================\n",
      "MinMaxScaler Not Encode Data\n",
      "(52578, 8) (52578,)\n",
      "(42062, 8) (10516, 8) (42062,) (10516,)\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# Feature와 Label 분리하기\n",
    "def Feature_Label(datafile) :\n",
    "    X = datafile.iloc[:,:-1]\n",
    "    y = datafile.iloc[:,-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "print(\"StandardScaler Not Encode Data\")\n",
    "SNE_X, SNE_y = Feature_Label(St_NotEncode_data)\n",
    "print(SNE_X.shape, SNE_y.shape)\n",
    "SNE_X_train, SNE_X_test, SNE_y_train, SNE_y_test = train_test_split(SNE_X, SNE_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(SNE_X_train.shape, SNE_X_test.shape, SNE_y_train.shape, SNE_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"StandardScaler Encode Data\")\n",
    "SE_X, SE_y = Feature_Label(St_Encode_data)\n",
    "print(SE_X.shape, SE_y.shape)\n",
    "SE_X_train, SE_X_test, SE_y_train, SE_y_test = train_test_split(SE_X, SE_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(SE_X_train.shape, SE_X_test.shape, SE_y_train.shape, SE_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Not Encode Data\")\n",
    "MMNE_X, MMNE_y = Feature_Label(MM_NotEncode_data)\n",
    "print(MMNE_X.shape, MMNE_y.shape)\n",
    "MMNE_X_train, MMNE_X_test, MMNE_y_train, MMNE_y_test = train_test_split(MMNE_X, MMNE_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(MMNE_X_train.shape, MMNE_X_test.shape, MMNE_y_train.shape, MMNE_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Encode Data\")\n",
    "MME_X, MME_y = Feature_Label(MM_Encode_data)\n",
    "print(MME_X.shape, MME_y.shape)\n",
    "MME_X_train, MME_X_test, MME_y_train, MME_y_test = train_test_split(MME_X, MME_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(MME_X_train.shape, MME_X_test.shape, MME_y_train.shape, MME_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"StandardScaler Not Encode Data\")\n",
    "SB_X, SB_y = Feature_Label(St_Base_data)\n",
    "print(SB_X.shape, SB_y.shape)\n",
    "SB_X_train, SB_X_test, SB_y_train, SB_y_test = train_test_split(SB_X, SB_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(SB_X_train.shape, SB_X_test.shape, SB_y_train.shape, SB_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Not Encode Data\")\n",
    "MMB_X, MMB_y = Feature_Label(MM_Base_data)\n",
    "print(MMB_X.shape, MMB_y.shape)\n",
    "MMB_X_train, MMB_X_test, MMB_y_train, MMB_y_test = train_test_split(MMB_X, MMB_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(MMB_X_train.shape, MMB_X_test.shape, MMB_y_train.shape, MMB_y_test.shape)\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   49.18407054  -103.45456061   120.60243718 -1941.81009496\n",
      "  8141.22549073   564.88544171  -287.62035153  1667.01842258] 7425.3787527212335\n",
      "train score 0.7696485398274073\n",
      "test score 0.7938091082951102\n",
      "=========================================================\n",
      "[   70.74190471  -112.2246157    169.58598632 -1915.68706933\n",
      "  7986.79022785   585.40008823  -304.01126569  1531.39333717] [7254.79716548]\n",
      "train score 0.7688820234033877\n",
      "test score 0.7891077764577978\n",
      "=========================================================\n",
      "[ 1802.29279414  -814.4245227    328.1175026  -8531.95600009\n",
      " 19788.63629879  1415.2835542  -8329.76732422  8791.96226849] 3570.8199519890286\n",
      "train score 0.7696485398274072\n",
      "test score 0.7938091082951103\n",
      "=========================================================\n",
      "[  306.63409961  -591.52480874   346.0970192  -7896.30542692\n",
      " 19933.8920644   1406.58679693 -1365.87339058  8501.6163492 ] [2938.69613007]\n",
      "train score 0.7693055297740112\n",
      "test score 0.7955843809431984\n",
      "=========================================================\n",
      "[ 0.06658913  0.11952966 -0.11987678 -0.34369012  1.42340424  9.91542242\n",
      "  0.07038231  0.86249268] 8.664674965852141\n",
      "train score 0.9195133813482879\n",
      "test score 0.8456680915378074\n",
      "=========================================================\n",
      "[ 0.08794073  0.11616142 -0.10961988 -0.3306452   1.40724281  9.87001897\n",
      "  0.03125391  0.86873002] [8.63471166]\n",
      "train score 0.9194697640034972\n",
      "test score 0.8433453860861057\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression, SGDRegressor (시계열을 고려하지 않은 경우)\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=1000)] :\n",
    "    m.fit(SNE_X_train, SNE_y_train)\n",
    "\n",
    "    print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(SNE_X_train, SNE_y_train))\n",
    "    print('test score', m.score(SNE_X_test, SNE_y_test))\n",
    "    print('=========================================================')\n",
    "\n",
    "\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=1000)] :\n",
    "    m.fit(MMNE_X_train, MMNE_y_train)\n",
    "\n",
    "    print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(MMNE_X_train, MMNE_y_train))\n",
    "    print('test score', m.score(MMNE_X_test, MMNE_y_test))\n",
    "    print('=========================================================')\n",
    "\n",
    "\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=1000)] :\n",
    "    m.fit(SB_X_train, SB_y_train)\n",
    "\n",
    "    print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(SB_X_train, SB_y_train))\n",
    "    print('test score', m.score(SB_X_test, SB_y_test))\n",
    "    print('=========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate_data 파라미터 세팅\n",
    "timesteps_length = 24*7\n",
    "New_train_split = 14014 # train set과 test set 비율을 80:20으로 맞췄을 때 경계값\n",
    "Basic_train_split = 42062 # train set과 test set 비율을 80:20으로 맞췄을 때 경계값\n",
    "future_target = 24 # 다음 24시간 미래 예측\n",
    "STEP = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변량 시계열 데이터 (Multivariate Time Series Data)\n",
    "# dataset : 원본 시계열 데이터 (2D Dataframe)\n",
    "# target : 예측 대상 변수 (1D Dataframe)\n",
    "# start_index : 학습 데이터의 시작 인덱스 / end_index : 학습 데이터의 끝 인덱스\n",
    "# history_size : 과거 정보로 사용할 데이터 포인트 수\n",
    "# target_size : 예측 대상으로 사용할 데이터 포인트 수\n",
    "# step : 데이터 포인트 간의 간격\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler Not Encode Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13846, 84, 8) (3216, 84, 8)\n",
      "=======================================\n",
      "StandardScaler Encode Data\n",
      "(13846, 84, 44) (3312, 84, 44)\n",
      "=======================================\n",
      "MinMaxScaler Not Encode Data\n",
      "(13846, 84, 8) (3216, 84, 8)\n",
      "=======================================\n",
      "MinMaxScaler Encode Data\n",
      "(13846, 84, 44) (3312, 84, 44)\n",
      "=======================================\n",
      "StandardScaler Not Encode Basic Data\n",
      "(41894, 84, 8) (10324, 84, 8)\n",
      "=======================================\n",
      "MinMaxScaler Not Encode Basic Data\n",
      "(41894, 84, 8) (10324, 84, 8)\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임은 열 이름과 인덱스를 가지고 있으나 넘파이 배열의 경우 순수한 다차원 배열로 되어 있어 순서에 따라 접근이 가능하므로 to_numpy 적용이 필요하다\n",
    "# 데이터프레임을 그대로 적용할 경우 KeyError가 발생하지만 넘파이 배열로 변경하게 되면 해당 부분이 문제가 되지 않는다\n",
    "\n",
    "def  to_timeseries_data(X, y, train_split):\n",
    "  X_train, y_train = multivariate_data(X.to_numpy(), y, 0, train_split, timesteps_length, future_target, STEP, single_step=True)\n",
    "  X_test, y_test = multivariate_data(X.to_numpy(), y, train_split, None, timesteps_length, future_target, STEP, single_step=True)\n",
    "  \n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"StandardScaler Not Encode Data\")\n",
    "SNE_X_train, SNE_X_test, SNE_y_train, SNE_y_test = to_timeseries_data(SNE_X, SNE_y, New_train_split)\n",
    "print(SNE_X_train.shape, SNE_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"StandardScaler Encode Data\")\n",
    "SE_X_train, SE_X_test, SE_y_train, SE_y_test = to_timeseries_data(SE_X, SE_y, New_train_split)\n",
    "print(SE_X_train.shape, SE_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Not Encode Data\")\n",
    "MMNE_X_train, MMNE_X_test, MMNE_y_train, MMNE_y_test = to_timeseries_data(MMNE_X, MMNE_y, New_train_split)\n",
    "print(MMNE_X_train.shape, MMNE_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Encode Data\")\n",
    "MME_X_train, MME_X_test, MME_y_train, MME_y_test = to_timeseries_data(MME_X, MME_y, New_train_split)\n",
    "print(MME_X_train.shape, MME_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"StandardScaler Not Encode Basic Data\")\n",
    "SB_X_train, SB_X_test, SB_y_train, SB_y_test = to_timeseries_data(SB_X, SB_y, Basic_train_split)\n",
    "print(SB_X_train.shape, SB_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"MinMaxScaler Not Encode Basic Data\")\n",
    "MMB_X_train, MMB_X_test, MMB_y_train, MMB_y_test = to_timeseries_data(MMB_X, MMB_y, Basic_train_split)\n",
    "print(MMB_X_train.shape, MMB_X_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "\n",
    "# 적용 테스트\n",
    "\n",
    "# X_train, y_train = multivariate_data(SNE_X.to_numpy(), SNE_y, 0, New_train_split, timesteps_length, future_target, STEP, single_step=True)\n",
    "# X_test, y_test = multivariate_data(SNE_X.to_numpy(), SNE_y, New_train_split, None, timesteps_length, future_target, STEP, single_step=True)\n",
    "\n",
    "# print(X_train.shape, X_test.shape)\n",
    "# print(X_train.shape[0]+X_test.shape[0]) # 기존 17518\n",
    "# print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7891672399824664\n",
      "test score 0.741432371892979\n",
      "=========================================================\n",
      "train score 0.7720749691255354\n",
      "test score 0.7117143473801443\n",
      "=========================================================\n",
      "train score 0.7891672391229285\n",
      "test score 0.741433395462092\n",
      "=========================================================\n",
      "train score 0.7891657488201584\n",
      "test score 0.7414638811311859\n",
      "=========================================================\n",
      "*******************************************************************************************\n",
      "train score 0.7398523454458616\n",
      "test score 0.7114171969692256\n",
      "=========================================================\n",
      "train score 0.7240438269358176\n",
      "test score 0.694809689600254\n",
      "=========================================================\n",
      "train score 0.739852345433374\n",
      "test score 0.7114170772960173\n",
      "=========================================================\n",
      "train score 0.7280295074941062\n",
      "test score 0.6961241280214607\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# LinearRegression (시계열 데이터 적용)\n",
    "\n",
    "# 차원 축소로 인해 예측률이 떨어질 수 있음!\n",
    "\n",
    "# SNE(StandardScaler Not Encode)  \n",
    "SNE_X_train_2d = SNE_X_train.reshape((SNE_X_train.shape[0], -1))\n",
    "SNE_X_test_2d = SNE_X_test.reshape((SNE_X_test.shape[0], -1))\n",
    "\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=10000), Ridge(alpha=0.1), Lasso(alpha=0.1)] :\n",
    "    m.fit(SNE_X_train_2d, SNE_y_train)\n",
    "\n",
    "    # print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(SNE_X_train_2d, SNE_y_train))\n",
    "    print('test score', m.score(SNE_X_test_2d, SNE_y_test))\n",
    "    print('=========================================================')\n",
    "\n",
    "\n",
    "print(\"*******************************************************************************************\")\n",
    "\n",
    "# SB(StandardScaler Basic)\n",
    "SB_X_train_2d = SB_X_train.reshape((SB_X_train.shape[0], -1))\n",
    "SB_X_test_2d = SB_X_test.reshape((SB_X_test.shape[0], -1))\n",
    "\n",
    "for m in [LinearRegression(), SGDRegressor(max_iter=10000), Ridge(alpha=0.1), Lasso(alpha=0.1)] :\n",
    "    m.fit(SB_X_train_2d, SB_y_train)\n",
    "\n",
    "    # print(m.coef_, m.intercept_)\n",
    "    print('train score', m.score(SB_X_train_2d, SB_y_train))\n",
    "    print('test score', m.score(SB_X_test_2d, SB_y_test))\n",
    "    print('=========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 28\n",
    "\n",
    "# SNB_train_data = tf.data.Dataset.from_tensor_slices((SNE_X_train, SNE_y_train)).cache().batch(BATCH_SIZE)\n",
    "# SNE_test_data = tf.data.Dataset.from_tensor_slices((SNE_X_test, SNE_y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "BATCH_SIZE = 28\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "SNB_train_data = tf.data.Dataset.from_tensor_slices((SNE_X_train, SNE_y_train)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "SNE_test_data = tf.data.Dataset.from_tensor_slices((SNE_X_test, SNE_y_test)).batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 84, 32)            5248      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 16)                3136      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,792\n",
      "Trainable params: 8,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "(28, 24)\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 6s 46ms/step - loss: 6854.3276 - mae: 6854.3276 - val_loss: 8085.7358 - val_mae: 8085.7358\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 2s 37ms/step - loss: 8149.3037 - mae: 8149.3037 - val_loss: 8085.7295 - val_mae: 8085.7295\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 7865.7202 - mae: 7865.7202 - val_loss: 8085.7231 - val_mae: 8085.7231\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 2s 38ms/step - loss: 6373.5723 - mae: 6373.5723 - val_loss: 8085.7197 - val_mae: 8085.7197\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 2s 37ms/step - loss: 5815.0054 - mae: 5815.0054 - val_loss: 8085.7188 - val_mae: 8085.7188\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 2s 37ms/step - loss: 7400.7271 - mae: 7400.7271 - val_loss: 8085.7178 - val_mae: 8085.7178\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 2s 38ms/step - loss: 7970.0107 - mae: 7970.0107 - val_loss: 8085.7153 - val_mae: 8085.7153\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 2s 37ms/step - loss: 9152.1445 - mae: 9152.1445 - val_loss: 8085.7114 - val_mae: 8085.7114\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 2s 39ms/step - loss: 9202.1426 - mae: 9202.1426 - val_loss: 8085.7100 - val_mae: 8085.7100\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 2s 38ms/step - loss: 7536.7959 - mae: 7536.7959 - val_loss: 8085.7080 - val_mae: 8085.7080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2716b8aa3d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVALUATION_INTERVAL = 50\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "\n",
    "simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(32, input_shape=SNE_X_train.shape[-2:], recurrent_dropout=0.5, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(24)\n",
    "])\n",
    "\n",
    "# simple_lstm_model.compile(optimizer='adam', loss='mae')\n",
    "simple_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mae', metrics=['mae'])\n",
    "print(simple_lstm_model.summary())\n",
    "\n",
    "for x, y in SNE_test_data.take(1):\n",
    "    print (simple_lstm_model.predict(x).shape)\n",
    "\n",
    "simple_lstm_model.fit(SNB_train_data, epochs=EPOCHS, batch_size=BATCH_SIZE, steps_per_epoch=EVALUATION_INTERVAL, validation_data=SNE_test_data, validation_steps=10)\n",
    "\n",
    "\n",
    "# print(SNE_X_train.shape, SNE_y_train.shape)\n",
    "# print(SNE_X_train.shape[-2:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
