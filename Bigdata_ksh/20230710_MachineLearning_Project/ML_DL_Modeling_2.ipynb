{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymysql\n",
    "import dotenv\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalization/Standardization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout, Conv1D, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.saving import save_model\n",
    "\n",
    "# 경고 무시 코드 추가\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pltconfig_default() :\n",
    "  sns.reset_defaults()\n",
    "  %matplotlib inline\n",
    "\n",
    "pltconfig_default()\n",
    "\n",
    "matplotlib.rcParams\n",
    "\n",
    "matplotlib.rcParams['font.family']\n",
    "\n",
    "current_font_list = matplotlib.rcParams['font.family']\n",
    "\n",
    "font_path = 'C:\\\\Windows\\\\Fonts\\\\batang.ttc'\n",
    "\n",
    "kfont = matplotlib.font_manager.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "matplotlib.rcParams['font.family'] = [kfont] + current_font_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17422, 9)\n"
     ]
    }
   ],
   "source": [
    "# with open('StandardScalar_final_data', 'rb') as file :\n",
    "#   St_NotEncode_data = pickle.load(file)\n",
    "\n",
    "St_NotEncode_data = pd.read_pickle(\"StandardScalar_final_data\")\n",
    "  \n",
    "print(St_NotEncode_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler Not Encode Data\n",
      "(17422, 8) (17422,)\n",
      "(13937, 8) (3485, 8) (13937,) (3485,)\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# Feature와 Label 분리하기\n",
    "def Feature_Label(datafile) :\n",
    "    X = datafile.iloc[:,:-1]\n",
    "    y = datafile.iloc[:,-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "print(\"StandardScaler Not Encode Data\")\n",
    "SNE_X, SNE_y = Feature_Label(St_NotEncode_data)\n",
    "print(SNE_X.shape, SNE_y.shape)\n",
    "SNE_X_train, SNE_X_test, SNE_y_train, SNE_y_test = train_test_split(SNE_X, SNE_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(SNE_X_train.shape, SNE_X_test.shape, SNE_y_train.shape, SNE_y_test.shape)\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='8'>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig, axe = plt.subplots(figsize=(16, 6), layout='constrained')\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "sns.lineplot(y=SNE_y, x=SNE_X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNE_X_train, SNE_X_test, SNE_y_train, SNE_y_test = train_test_split(SNE_X, SNE_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "\n",
    "# print(SNE_X_train.shape, SNE_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#일변량 시계열 (발전량만을 활용한 시계열 데이터셋)\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle) :\n",
    "  series = tf.expand_dims(series, axis=-1)\n",
    "  ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "  # window_size : 몇 개의 데이터로 다음 데이터를 예측할지 설정, stride : 윈도우 데이터셋을 몇 칸씩 이동할지 결정, drop_remainder : 뒤에 잔여 데이터 미처리 적용(데이터셋 사이즈가 달라지는 것을 방지)\n",
    "  ds = ds.window(window_size + 1, stride=1, shift=1, drop_remainder=True) # window_size + 1 : X(feature)와 y(target) 값을 포함하는 범위(즉 +1은 y(target) 포함을 의미)\n",
    "  # flat_map : map 함수와 동일하게 맵핑을 해주지만 flat한 결과값을 준다!(즉, 차원 -1)\n",
    "  ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "  if shuffle :\n",
    "    ds = ds.shuffle(1000)\n",
    "  ds = ds.map(lambda w : (w[:-1], w[-1])) # w[:-1] : 학습 데이터 , w[-1] : 예측 데이터\n",
    "  # batch : 배치를 구성해주는 함수 / prefetch : 미리 데이터를 fetch하는 개수 (병렬 처리하므로 학습 속도 개선 효과)\n",
    "  return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE=3\n",
    "BATCH_SIZE=32 # 한번에 여러 개의 데이터를 처리하는 병렬 처리 형태로 계산 효율을 높일 수 있다. 즉, 성능, 메모리 측면에서 유리 + 또한 가중치 업데이트 횟수를 제어할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = windowed_dataset(SNE_y_train, WINDOW_SIZE, BATCH_SIZE, True) # 기존 적용\n",
    "train_data = windowed_dataset(SNE_y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
    "test_data = windowed_dataset(SNE_y_test, WINDOW_SIZE, BATCH_SIZE, False)\n",
    "train_data_ns = windowed_dataset(SNE_y_train, WINDOW_SIZE, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 1)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "for data in train_data.take(1) :  # take : 1개의 배치만 가져오기\n",
    "  print(f'{data[0].shape}')\n",
    "  print(f'{data[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = Sequential([\n",
    "#   Conv1D(filters=32, kernel_size=30,\n",
    "#          padding='causal', # 시계열 데이터와 같은 순차 데이터를 다룰 때 유용한 방법으로 입력 데이터 주변에 가상의 값을 추가하여 출력 크기를 조정하거나 경계 효과를 제어하는데 사용된다\n",
    "#          activation='relu',\n",
    "#          input_shape=[WINDOW_SIZE, 1]),\n",
    "#   LSTM(256, recurrent_dropout=0.5, activation='tanh', return_sequences=True),  \n",
    "#   # tanh함수는 출력 범위가 -1~1로 제한되며 양수와 음수를 모두 다룰 수 있음\n",
    "#   # ReLU함수는 출력 범위가 0~1로 제한되며 음수의 경우 0으로 출력하여 음수값을 제거하는 효과가 있음, But, 음수를 처리하지 못해 그래디언트 소실 문제가 발생할 수 있음\n",
    "#   LSTM(128, activation='tanh', return_sequences=True),\n",
    "#   LSTM(64, activation='tanh', return_sequences=True),\n",
    "#   LSTM(32, activation='tanh'),\n",
    "#   Dense(8, activation='relu'),\n",
    "#   # Dropout(0.5), # 과적합을 방지하기 위해 사용되는 정규화 기법 중 하나로 학습 과정에서 일부 뉴런의 출력을 랜덤하게 0으로 만들어 모델의 일반화 능력을 향상시킨다\n",
    "#   Dense(1)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재까지 loss가 가장 낮은 모델\n",
    "\n",
    "lstm_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal', # 시계열 데이터와 같은 순차 데이터를 다룰 때 유용한 방법으로 입력 데이터 주변에 가상의 값을 추가하여 출력 크기를 조정하거나 경계 효과를 제어하는데 사용된다\n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 1]),\n",
    "  LSTM(128, activation='tanh', return_sequences=True),  \n",
    "  # tanh함수는 출력 범위가 -1~1로 제한되며 양수와 음수를 모두 다룰 수 있음\n",
    "  # ReLU함수는 출력 범위가 0~1로 제한되며 음수의 경우 0으로 출력하여 음수값을 제거하는 효과가 있음, But, 음수를 처리하지 못해 그래디언트 소실 문제가 발생할 수 있음\n",
    "  LSTM(64, activation='tanh', return_sequences=True),\n",
    "  LSTM(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  # Dropout(0.5), # 과적합을 방지하기 위해 사용되는 정규화 기법 중 하나로 학습 과정에서 일부 뉴런의 출력을 랜덤하게 0으로 만들어 모델의 일반화 능력을 향상시킨다\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal', # 시계열 데이터와 같은 순차 데이터를 다룰 때 유용한 방법으로 입력 데이터 주변에 가상의 값을 추가하여 출력 크기를 조정하거나 경계 효과를 제어하는데 사용된다\n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 1]),\n",
    "  SimpleRNN(128, activation='tanh', return_sequences=True),  \n",
    "  # tanh함수는 출력 범위가 -1~1로 제한되며 양수와 음수를 모두 다룰 수 있음\n",
    "  # ReLU함수는 출력 범위가 0~1로 제한되며 음수의 경우 0으로 출력하여 음수값을 제거하는 효과가 있음, But, 음수를 처리하지 못해 그래디언트 소실 문제가 발생할 수 있음\n",
    "  SimpleRNN(64, activation='tanh', return_sequences=True),\n",
    "  SimpleRNN(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  # Dropout(0.5), # 과적합을 방지하기 위해 사용되는 정규화 기법 중 하나로 학습 과정에서 일부 뉴런의 출력을 랜덤하게 0으로 만들어 모델의 일반화 능력을 향상시킨다\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal', # 시계열 데이터와 같은 순차 데이터를 다룰 때 유용한 방법으로 입력 데이터 주변에 가상의 값을 추가하여 출력 크기를 조정하거나 경계 효과를 제어하는데 사용된다\n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 1]),\n",
    "  GRU(128, activation='tanh', return_sequences=True),  \n",
    "  # tanh함수는 출력 범위가 -1~1로 제한되며 양수와 음수를 모두 다룰 수 있음\n",
    "  # ReLU함수는 출력 범위가 0~1로 제한되며 음수의 경우 0으로 출력하여 음수값을 제거하는 효과가 있음, But, 음수를 처리하지 못해 그래디언트 소실 문제가 발생할 수 있음\n",
    "  GRU(64, activation='tanh', return_sequences=True),\n",
    "  GRU(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  # Dropout(0.5), # 과적합을 방지하기 위해 사용되는 정규화 기법 중 하나로 학습 과정에서 일부 뉴런의 출력을 랜덤하게 0으로 만들어 모델의 일반화 능력을 향상시킨다\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Huber() # MSE(Mean Squared Error)와 MAE(Mean Absolute Error)의 특성을 혼합한 것으로 이상치에 민감하지 않으면서도 큰 오차에 덜 민감한 특성을 가진다\n",
    "optimizer = Adam(learning_rate=0.001) # 0.0005\n",
    "lstm_model.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "rnn_model.compile(loss=loss, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse', 'mae'])\n",
    "gru_model.compile(loss=loss, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse', 'mae'])\n",
    "# test_model.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau : 학습 도중 learning_rate를 동적으로 조정하는 데 사용된다 (에코크 동안 지정된 지표(monitor)에 대한 개선이 없는 경우 learning_rate를 감소시킨다)\n",
    "# 주요 파라미터 : monitor - 모니터링할 지표(검증 손실(val_loss), val_rmse, val_mae 등), factor - 학습률을 감소시킬 비율, patience : 지정된 에포크 동안 개선이 없을 경우 학습률 감소, mode - 판단 방식('auto', #                          'min', 'max'), min_lr - learning_rate 하한값 지정\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "\n",
    "# EarlyStopping : 모델의 성능이 개선되지 않을 경우 학습을 조기 종료하여 불필요한 계산을 줄이고 과적합을 방지\n",
    "# 주요 파라미터 : monitor - 모니터링할 지표, patience - 지정된 에포크 동안 개선이 없을 경우 종료, mode - 모니터링 지표의 개선 여부 판단 방식, verbose - EarlyStopping 동작 상황 출력 여부 지정\n",
    "#                restore_best_weights - 적용 후 최적의 모델 가중치를 복원할지 여부를 지정                  \n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# ModelCheckpoint : 학습 도중 지정된 지표(monitor) 기반하여 가장 성능이 좋은 모델 가중치를 저장 (지표가 개선되었을 때만 가중치 저장)\n",
    "#                   filepath : 모델 가중치 저장 경로 지정 (`{epoch:02d}`와 같은 형식을 사용해 에포크 번호 등을 동적으로 포맷팅 가능), save_best_only - True 설정 시 가장 좋은 성능을 보인 모델 가중치만 저장\n",
    "\n",
    "# mc = ModelCheckpoint('test_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_lstm = ModelCheckpoint('new_stne_lstm_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_lstm_ns = ModelCheckpoint('new_stne_lstm_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_rnn = ModelCheckpoint('new_stne_rnn_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_rnn_ns = ModelCheckpoint('new_stne_rnn_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_gru = ModelCheckpoint('new_stne_gru_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_gru_ns = ModelCheckpoint('new_stne_gru_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "\n",
    "# callback = [reduce_lr, es, mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_history = test_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    436/Unknown - 10s 7ms/step - loss: 7620.8555 - mse: 183686128.0000 - mae: 7621.2158\n",
      "Epoch 1: val_loss improved from inf to 7108.02148, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 12s 10ms/step - loss: 7620.8555 - mse: 183686128.0000 - mae: 7621.2158 - val_loss: 7108.0215 - val_mse: 171552256.0000 - val_mae: 7108.4321 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 7573.4360 - mse: 182107744.0000 - mae: 7573.7803\n",
      "Epoch 2: val_loss improved from 7108.02148 to 7054.41992, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 7580.5415 - mse: 182240784.0000 - mae: 7580.8857 - val_loss: 7054.4199 - val_mse: 169658336.0000 - val_mae: 7054.7202 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 7508.5376 - mse: 179655168.0000 - mae: 7508.8398\n",
      "Epoch 3: val_loss improved from 7054.41992 to 6980.90820, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 7508.5376 - mse: 179655168.0000 - mae: 7508.8398 - val_loss: 6980.9082 - val_mse: 166749392.0000 - val_mae: 6981.1953 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 7402.7881 - mse: 175840592.0000 - mae: 7403.1050\n",
      "Epoch 4: val_loss improved from 6980.90820 to 6870.72656, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 7410.1318 - mse: 176098208.0000 - mae: 7410.4487 - val_loss: 6870.7266 - val_mse: 163030432.0000 - val_mae: 6871.0239 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 7297.1968 - mse: 172005392.0000 - mae: 7297.5132\n",
      "Epoch 5: val_loss improved from 6870.72656 to 6748.26758, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 7290.4111 - mse: 171759712.0000 - mae: 7290.7275 - val_loss: 6748.2676 - val_mse: 158747424.0000 - val_mae: 6748.5571 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 7136.9385 - mse: 166860896.0000 - mae: 7137.2402\n",
      "Epoch 6: val_loss improved from 6748.26758 to 6599.39307, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 7135.0693 - mse: 166772416.0000 - mae: 7135.3721 - val_loss: 6599.3931 - val_mse: 153803264.0000 - val_mae: 6599.7271 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 6986.4287 - mse: 161446256.0000 - mae: 6986.7329\n",
      "Epoch 7: val_loss improved from 6599.39307 to 6458.82422, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 6984.2051 - mse: 161288832.0000 - mae: 6984.5098 - val_loss: 6458.8242 - val_mse: 148537888.0000 - val_mae: 6459.1436 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 6833.0239 - mse: 155430288.0000 - mae: 6833.3340\n",
      "Epoch 8: val_loss improved from 6458.82422 to 6305.66797, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 6834.0747 - mse: 155373728.0000 - mae: 6834.3848 - val_loss: 6305.6680 - val_mse: 142456048.0000 - val_mae: 6305.9810 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 6639.3887 - mse: 148520848.0000 - mae: 6639.6929\n",
      "Epoch 9: val_loss improved from 6305.66797 to 6129.44385, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 6636.7676 - mse: 148404336.0000 - mae: 6637.0713 - val_loss: 6129.4438 - val_mse: 135528144.0000 - val_mae: 6129.7354 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 6439.2563 - mse: 141320976.0000 - mae: 6439.5654\n",
      "Epoch 10: val_loss improved from 6129.44385 to 5885.35596, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 6439.2563 - mse: 141320976.0000 - mae: 6439.5654 - val_loss: 5885.3560 - val_mse: 128375280.0000 - val_mae: 5885.6606 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 6185.3682 - mse: 132924480.0000 - mae: 6185.6748\n",
      "Epoch 11: val_loss improved from 5885.35596 to 5795.11719, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 6193.4844 - mse: 133038816.0000 - mae: 6193.7915 - val_loss: 5795.1172 - val_mse: 121490936.0000 - val_mae: 5795.4404 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 6023.9209 - mse: 126078368.0000 - mae: 6024.2231\n",
      "Epoch 12: val_loss improved from 5795.11719 to 5501.70996, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 6038.0859 - mse: 126272024.0000 - mae: 6038.3896 - val_loss: 5501.7100 - val_mse: 114442368.0000 - val_mae: 5502.0640 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 5833.4014 - mse: 118427624.0000 - mae: 5833.7017\n",
      "Epoch 13: val_loss improved from 5501.70996 to 5328.00342, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 5833.4014 - mse: 118427624.0000 - mae: 5833.7017 - val_loss: 5328.0034 - val_mse: 107423896.0000 - val_mae: 5328.2812 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 5599.5073 - mse: 110935032.0000 - mae: 5599.8271\n",
      "Epoch 14: val_loss improved from 5328.00342 to 5138.41797, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 5602.3555 - mse: 111029360.0000 - mae: 5602.6753 - val_loss: 5138.4180 - val_mse: 100602992.0000 - val_mae: 5138.7642 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 5395.6475 - mse: 103513016.0000 - mae: 5395.9761\n",
      "Epoch 15: val_loss improved from 5138.41797 to 5091.65820, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 5383.8442 - mse: 103182376.0000 - mae: 5384.1724 - val_loss: 5091.6582 - val_mse: 93696168.0000 - val_mae: 5091.9131 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 5159.7524 - mse: 95381448.0000 - mae: 5160.0635\n",
      "Epoch 16: val_loss improved from 5091.65820 to 4730.24512, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 5153.6763 - mse: 95224888.0000 - mae: 5153.9878 - val_loss: 4730.2451 - val_mse: 85801528.0000 - val_mae: 4730.5410 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 4984.9966 - mse: 88045744.0000 - mae: 4985.3018\n",
      "Epoch 17: val_loss improved from 4730.24512 to 4649.79053, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 4988.9146 - mse: 88128208.0000 - mae: 4989.2207 - val_loss: 4649.7905 - val_mse: 79192752.0000 - val_mae: 4650.1113 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 4767.6436 - mse: 80765016.0000 - mae: 4767.9565\n",
      "Epoch 18: val_loss improved from 4649.79053 to 4396.74365, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 4774.3452 - mse: 80832296.0000 - mae: 4774.6592 - val_loss: 4396.7437 - val_mse: 72811824.0000 - val_mae: 4397.0479 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 4611.3306 - mse: 74663608.0000 - mae: 4611.6401\n",
      "Epoch 19: val_loss improved from 4396.74365 to 4246.84229, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 4611.3306 - mse: 74663608.0000 - mae: 4611.6401 - val_loss: 4246.8423 - val_mse: 67146280.0000 - val_mae: 4247.1382 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 4465.5083 - mse: 69027248.0000 - mae: 4465.8276\n",
      "Epoch 20: val_loss improved from 4246.84229 to 4121.98828, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 4468.9590 - mse: 69068192.0000 - mae: 4469.2788 - val_loss: 4121.9883 - val_mse: 62259664.0000 - val_mae: 4122.2812 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 4318.8901 - mse: 63777540.0000 - mae: 4319.2041\n",
      "Epoch 21: val_loss improved from 4121.98828 to 4066.09424, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 4319.0332 - mse: 63763548.0000 - mae: 4319.3472 - val_loss: 4066.0942 - val_mse: 58329956.0000 - val_mae: 4066.5164 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 4130.5220 - mse: 58150696.0000 - mae: 4130.8462\n",
      "Epoch 22: val_loss improved from 4066.09424 to 3839.94580, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 4129.4585 - mse: 58052804.0000 - mae: 4129.7822 - val_loss: 3839.9458 - val_mse: 52573432.0000 - val_mae: 3840.2263 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 3978.9932 - mse: 53829864.0000 - mae: 3979.3040\n",
      "Epoch 23: val_loss improved from 3839.94580 to 3701.28345, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3972.8198 - mse: 53712120.0000 - mae: 3973.1304 - val_loss: 3701.2834 - val_mse: 48434228.0000 - val_mae: 3701.5671 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3800.8564 - mse: 48942140.0000 - mae: 3801.1687\n",
      "Epoch 24: val_loss improved from 3701.28345 to 3686.95605, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3809.1343 - mse: 49076908.0000 - mae: 3809.4468 - val_loss: 3686.9561 - val_mse: 46561936.0000 - val_mae: 3687.2671 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3701.2437 - mse: 45959696.0000 - mae: 3701.5601\n",
      "Epoch 25: val_loss improved from 3686.95605 to 3555.50781, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3701.9761 - mse: 45909556.0000 - mae: 3702.2927 - val_loss: 3555.5078 - val_mse: 42919784.0000 - val_mae: 3555.8311 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 3620.1709 - mse: 43281620.0000 - mae: 3620.4775\n",
      "Epoch 26: val_loss improved from 3555.50781 to 3457.10742, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3631.2773 - mse: 43381888.0000 - mae: 3631.5850 - val_loss: 3457.1074 - val_mse: 39621940.0000 - val_mae: 3457.4397 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3507.5349 - mse: 40705412.0000 - mae: 3507.8462\n",
      "Epoch 27: val_loss did not improve from 3457.10742\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3505.4968 - mse: 40648816.0000 - mae: 3505.8088 - val_loss: 3507.5916 - val_mse: 41091628.0000 - val_mae: 3507.8667 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 3421.2415 - mse: 38616304.0000 - mae: 3421.5571\n",
      "Epoch 28: val_loss improved from 3457.10742 to 3385.42285, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3426.3777 - mse: 38670408.0000 - mae: 3426.6938 - val_loss: 3385.4229 - val_mse: 37894892.0000 - val_mae: 3385.6934 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3449.7319 - mse: 38129988.0000 - mae: 3450.0645\n",
      "Epoch 29: val_loss improved from 3385.42285 to 3368.76489, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3448.6187 - mse: 38112484.0000 - mae: 3448.9507 - val_loss: 3368.7649 - val_mse: 35573616.0000 - val_mae: 3369.0959 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 3718.5339 - mse: 43265592.0000 - mae: 3718.8696\n",
      "Epoch 30: val_loss did not improve from 3368.76489\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3718.5339 - mse: 43265592.0000 - mae: 3718.8696 - val_loss: 3401.7993 - val_mse: 35578480.0000 - val_mae: 3402.0977 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 3448.3455 - mse: 37577096.0000 - mae: 3448.6765\n",
      "Epoch 31: val_loss improved from 3368.76489 to 3264.62036, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3455.7729 - mse: 37687012.0000 - mae: 3456.1040 - val_loss: 3264.6204 - val_mse: 34402208.0000 - val_mae: 3264.9075 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3308.0200 - mse: 35281064.0000 - mae: 3308.3486\n",
      "Epoch 32: val_loss did not improve from 3264.62036\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3318.6970 - mse: 35457808.0000 - mae: 3319.0259 - val_loss: 3733.4512 - val_mse: 39167144.0000 - val_mae: 3733.7864 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 3514.8174 - mse: 39527368.0000 - mae: 3515.1570\n",
      "Epoch 33: val_loss improved from 3264.62036 to 3244.50439, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3515.9255 - mse: 39539720.0000 - mae: 3516.2651 - val_loss: 3244.5044 - val_mse: 34505428.0000 - val_mae: 3244.8875 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3234.4758 - mse: 34454816.0000 - mae: 3234.8076\n",
      "Epoch 34: val_loss improved from 3244.50439 to 3154.85400, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3234.6560 - mse: 34413324.0000 - mae: 3234.9883 - val_loss: 3154.8540 - val_mse: 32050332.0000 - val_mae: 3155.1599 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3234.4272 - mse: 34458864.0000 - mae: 3234.7446\n",
      "Epoch 35: val_loss improved from 3154.85400 to 3020.99048, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3234.5015 - mse: 34461444.0000 - mae: 3234.8191 - val_loss: 3020.9905 - val_mse: 30352348.0000 - val_mae: 3021.3652 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3140.2144 - mse: 32937982.0000 - mae: 3140.5327\n",
      "Epoch 36: val_loss did not improve from 3020.99048\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3138.9812 - mse: 32917794.0000 - mae: 3139.2996 - val_loss: 3071.5640 - val_mse: 32819100.0000 - val_mae: 3071.8694 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 3123.1624 - mse: 32762522.0000 - mae: 3123.4839\n",
      "Epoch 37: val_loss did not improve from 3020.99048\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3123.1624 - mse: 32762522.0000 - mae: 3123.4839 - val_loss: 3128.8000 - val_mse: 33366582.0000 - val_mae: 3129.1135 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3151.0779 - mse: 33786568.0000 - mae: 3151.4187\n",
      "Epoch 38: val_loss improved from 3020.99048 to 3003.22925, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3152.4146 - mse: 33776576.0000 - mae: 3152.7556 - val_loss: 3003.2292 - val_mse: 30789408.0000 - val_mae: 3003.5229 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3095.2390 - mse: 32346372.0000 - mae: 3095.5574\n",
      "Epoch 39: val_loss did not improve from 3003.22925\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3101.0005 - mse: 32435242.0000 - mae: 3101.3188 - val_loss: 3015.4639 - val_mse: 31675800.0000 - val_mae: 3015.7705 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3000.6599 - mse: 30743750.0000 - mae: 3000.9797\n",
      "Epoch 40: val_loss improved from 3003.22925 to 2963.86865, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3003.2490 - mse: 30790916.0000 - mae: 3003.5693 - val_loss: 2963.8687 - val_mse: 30907576.0000 - val_mae: 2964.1907 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3075.5864 - mse: 31852514.0000 - mae: 3075.9028\n",
      "Epoch 41: val_loss did not improve from 2963.86865\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3073.8123 - mse: 31831754.0000 - mae: 3074.1287 - val_loss: 3371.0195 - val_mse: 39130028.0000 - val_mae: 3371.2844 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3254.5681 - mse: 35547764.0000 - mae: 3254.9141\n",
      "Epoch 42: val_loss did not improve from 2963.86865\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3254.2390 - mse: 35545808.0000 - mae: 3254.5850 - val_loss: 3211.0208 - val_mse: 35844956.0000 - val_mae: 3211.3435 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3090.5684 - mse: 32469586.0000 - mae: 3090.8982\n",
      "Epoch 43: val_loss did not improve from 2963.86865\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3093.3115 - mse: 32550090.0000 - mae: 3093.6411 - val_loss: 2988.3960 - val_mse: 32456280.0000 - val_mae: 2988.6802 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3028.1689 - mse: 31663024.0000 - mae: 3028.5002\n",
      "Epoch 44: val_loss improved from 2963.86865 to 2873.27661, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3025.4258 - mse: 31591352.0000 - mae: 3025.7573 - val_loss: 2873.2766 - val_mse: 30713566.0000 - val_mae: 2873.6338 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2956.5449 - mse: 30925542.0000 - mae: 2956.8770\n",
      "Epoch 45: val_loss did not improve from 2873.27661\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2960.3782 - mse: 30971914.0000 - mae: 2960.7102 - val_loss: 2890.2031 - val_mse: 31372340.0000 - val_mae: 2890.5640 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2986.2341 - mse: 30983846.0000 - mae: 2986.5664\n",
      "Epoch 46: val_loss did not improve from 2873.27661\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2985.1580 - mse: 30946456.0000 - mae: 2985.4907 - val_loss: 2916.1160 - val_mse: 30470034.0000 - val_mae: 2916.4893 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 3017.2349 - mse: 31881936.0000 - mae: 3017.5603\n",
      "Epoch 47: val_loss did not improve from 2873.27661\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3023.3994 - mse: 31991234.0000 - mae: 3023.7251 - val_loss: 3070.3936 - val_mse: 33107290.0000 - val_mae: 3070.7441 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2949.6865 - mse: 31305566.0000 - mae: 2950.0181\n",
      "Epoch 48: val_loss improved from 2873.27661 to 2860.45435, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2949.6865 - mse: 31305566.0000 - mae: 2950.0181 - val_loss: 2860.4543 - val_mse: 32360854.0000 - val_mae: 2860.7458 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2984.1765 - mse: 32144062.0000 - mae: 2984.4988\n",
      "Epoch 49: val_loss did not improve from 2860.45435\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2992.1914 - mse: 32324348.0000 - mae: 2992.5144 - val_loss: 2873.4966 - val_mse: 30981976.0000 - val_mae: 2873.7944 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2987.8103 - mse: 32121586.0000 - mae: 2988.1282\n",
      "Epoch 50: val_loss did not improve from 2860.45435\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2988.1626 - mse: 32116920.0000 - mae: 2988.4805 - val_loss: 3063.4614 - val_mse: 32663644.0000 - val_mae: 3063.7529 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 3142.0012 - mse: 33680516.0000 - mae: 3142.3154\n",
      "Epoch 51: val_loss did not improve from 2860.45435\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3138.8245 - mse: 33636000.0000 - mae: 3139.1389 - val_loss: 3081.2310 - val_mse: 30526044.0000 - val_mae: 3081.5547 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2937.8167 - mse: 29950436.0000 - mae: 2938.1345\n",
      "Epoch 52: val_loss improved from 2860.45435 to 2710.20630, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2937.8167 - mse: 29950436.0000 - mae: 2938.1345 - val_loss: 2710.2063 - val_mse: 27581700.0000 - val_mae: 2710.5554 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2906.4417 - mse: 29978450.0000 - mae: 2906.7603\n",
      "Epoch 53: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2908.2629 - mse: 30040098.0000 - mae: 2908.5823 - val_loss: 2910.5618 - val_mse: 29934606.0000 - val_mae: 2910.8572 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2998.0750 - mse: 31187920.0000 - mae: 2998.4106\n",
      "Epoch 54: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2996.5916 - mse: 31164232.0000 - mae: 2996.9272 - val_loss: 2799.2236 - val_mse: 29077416.0000 - val_mae: 2799.5500 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2810.4905 - mse: 28578562.0000 - mae: 2810.8247\n",
      "Epoch 55: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2814.2434 - mse: 28651524.0000 - mae: 2814.5776 - val_loss: 2865.4712 - val_mse: 29780372.0000 - val_mae: 2865.8604 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 3010.6128 - mse: 32321942.0000 - mae: 3010.9448\n",
      "Epoch 56: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3014.1782 - mse: 32345898.0000 - mae: 3014.5107 - val_loss: 2904.6951 - val_mse: 29258316.0000 - val_mae: 2905.0730 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2854.5796 - mse: 29360052.0000 - mae: 2854.9011\n",
      "Epoch 57: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2860.2659 - mse: 29445608.0000 - mae: 2860.5879 - val_loss: 2989.2278 - val_mse: 31859118.0000 - val_mae: 2989.5190 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2864.4844 - mse: 29467296.0000 - mae: 2864.7993\n",
      "Epoch 58: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2868.1565 - mse: 29514696.0000 - mae: 2868.4717 - val_loss: 2808.3899 - val_mse: 30765902.0000 - val_mae: 2808.7310 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3191.7664 - mse: 35271904.0000 - mae: 3192.0896\n",
      "Epoch 59: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3192.8794 - mse: 35304120.0000 - mae: 3193.2026 - val_loss: 3034.3206 - val_mse: 33806256.0000 - val_mae: 3034.5984 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3072.3096 - mse: 33806236.0000 - mae: 3072.6296\n",
      "Epoch 60: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3073.4724 - mse: 33813024.0000 - mae: 3073.7927 - val_loss: 2836.3191 - val_mse: 28353710.0000 - val_mae: 2836.6526 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 3027.9619 - mse: 31953338.0000 - mae: 3028.2932\n",
      "Epoch 61: val_loss did not improve from 2710.20630\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3037.4333 - mse: 32029846.0000 - mae: 3037.7651 - val_loss: 3056.8083 - val_mse: 33989548.0000 - val_mae: 3057.1030 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2959.8184 - mse: 31546988.0000 - mae: 2960.1367\n",
      "Epoch 62: val_loss improved from 2710.20630 to 2697.05200, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2971.9417 - mse: 31796988.0000 - mae: 2972.2603 - val_loss: 2697.0520 - val_mse: 28004494.0000 - val_mae: 2697.3669 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2947.4534 - mse: 31236134.0000 - mae: 2947.7671\n",
      "Epoch 63: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2964.7334 - mse: 31533078.0000 - mae: 2965.0481 - val_loss: 2818.2112 - val_mse: 30108694.0000 - val_mae: 2818.5388 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2736.0193 - mse: 27249586.0000 - mae: 2736.3318\n",
      "Epoch 64: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2737.3425 - mse: 27216060.0000 - mae: 2737.6555 - val_loss: 2759.4343 - val_mse: 28459544.0000 - val_mae: 2759.7136 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2730.6096 - mse: 27880204.0000 - mae: 2730.9224\n",
      "Epoch 65: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2730.6096 - mse: 27880204.0000 - mae: 2730.9224 - val_loss: 2717.0439 - val_mse: 28391284.0000 - val_mae: 2717.2954 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2783.7400 - mse: 28978786.0000 - mae: 2784.0493\n",
      "Epoch 66: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2783.7400 - mse: 28978786.0000 - mae: 2784.0493 - val_loss: 2746.3083 - val_mse: 28533972.0000 - val_mae: 2746.6094 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2701.3042 - mse: 27516636.0000 - mae: 2701.6133\n",
      "Epoch 67: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2703.8501 - mse: 27497610.0000 - mae: 2704.1602 - val_loss: 2850.2952 - val_mse: 31914246.0000 - val_mae: 2850.5994 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2739.0713 - mse: 28441040.0000 - mae: 2739.3918\n",
      "Epoch 68: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2738.7529 - mse: 28434434.0000 - mae: 2739.0737 - val_loss: 2863.0374 - val_mse: 30261180.0000 - val_mae: 2863.3435 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2719.0876 - mse: 27185130.0000 - mae: 2719.4084\n",
      "Epoch 69: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2719.0876 - mse: 27185130.0000 - mae: 2719.4084 - val_loss: 2723.4929 - val_mse: 28703820.0000 - val_mae: 2723.7961 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2856.8303 - mse: 29668076.0000 - mae: 2857.1526\n",
      "Epoch 70: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2863.8357 - mse: 29736278.0000 - mae: 2864.1584 - val_loss: 2947.3730 - val_mse: 30592768.0000 - val_mae: 2947.6724 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2834.9597 - mse: 29126056.0000 - mae: 2835.2727\n",
      "Epoch 71: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2834.6313 - mse: 29123314.0000 - mae: 2834.9441 - val_loss: 2754.6582 - val_mse: 29131934.0000 - val_mae: 2754.9202 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2733.8530 - mse: 27946718.0000 - mae: 2734.1648\n",
      "Epoch 72: val_loss did not improve from 2697.05200\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2740.8345 - mse: 28064760.0000 - mae: 2741.1467 - val_loss: 2715.6016 - val_mse: 28540386.0000 - val_mae: 2715.9067 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2693.3345 - mse: 27097432.0000 - mae: 2693.6506\n",
      "Epoch 73: val_loss improved from 2697.05200 to 2595.66089, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2693.3345 - mse: 27097432.0000 - mae: 2693.6506 - val_loss: 2595.6609 - val_mse: 26379630.0000 - val_mae: 2595.9409 - lr: 5.0000e-04\n",
      "Epoch 74/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2648.1816 - mse: 26571122.0000 - mae: 2648.4861\n",
      "Epoch 74: val_loss improved from 2595.66089 to 2581.24927, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2652.3059 - mse: 26612422.0000 - mae: 2652.6108 - val_loss: 2581.2493 - val_mse: 26194806.0000 - val_mae: 2581.5430 - lr: 5.0000e-04\n",
      "Epoch 75/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2590.2773 - mse: 25690902.0000 - mae: 2590.5801\n",
      "Epoch 75: val_loss improved from 2581.24927 to 2547.71680, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2596.6938 - mse: 25782266.0000 - mae: 2596.9966 - val_loss: 2547.7168 - val_mse: 26049110.0000 - val_mae: 2548.0256 - lr: 5.0000e-04\n",
      "Epoch 76/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2659.4963 - mse: 26686816.0000 - mae: 2659.8032\n",
      "Epoch 76: val_loss did not improve from 2547.71680\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2663.4658 - mse: 26774306.0000 - mae: 2663.7729 - val_loss: 2565.5664 - val_mse: 25512768.0000 - val_mae: 2565.8506 - lr: 5.0000e-04\n",
      "Epoch 77/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2598.8589 - mse: 25677748.0000 - mae: 2599.1650\n",
      "Epoch 77: val_loss did not improve from 2547.71680\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2600.0654 - mse: 25696042.0000 - mae: 2600.3718 - val_loss: 2595.9631 - val_mse: 25571782.0000 - val_mae: 2596.2568 - lr: 5.0000e-04\n",
      "Epoch 78/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2564.2136 - mse: 25131612.0000 - mae: 2564.5183\n",
      "Epoch 78: val_loss did not improve from 2547.71680\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2574.2422 - mse: 25275074.0000 - mae: 2574.5471 - val_loss: 2600.7000 - val_mse: 25913322.0000 - val_mae: 2600.9622 - lr: 5.0000e-04\n",
      "Epoch 79/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2605.5781 - mse: 25864782.0000 - mae: 2605.8855\n",
      "Epoch 79: val_loss did not improve from 2547.71680\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2612.9067 - mse: 25982110.0000 - mae: 2613.2146 - val_loss: 2613.9507 - val_mse: 26552052.0000 - val_mae: 2614.2180 - lr: 5.0000e-04\n",
      "Epoch 80/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2614.0654 - mse: 26358302.0000 - mae: 2614.3704\n",
      "Epoch 80: val_loss did not improve from 2547.71680\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2620.5344 - mse: 26400126.0000 - mae: 2620.8396 - val_loss: 2559.0427 - val_mse: 25109202.0000 - val_mae: 2559.3220 - lr: 5.0000e-04\n",
      "Epoch 81/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2598.4460 - mse: 25645176.0000 - mae: 2598.7488\n",
      "Epoch 81: val_loss improved from 2547.71680 to 2536.60767, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2606.5176 - mse: 25697806.0000 - mae: 2606.8206 - val_loss: 2536.6077 - val_mse: 25581476.0000 - val_mae: 2536.8577 - lr: 5.0000e-04\n",
      "Epoch 82/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2568.0840 - mse: 25233396.0000 - mae: 2568.3899\n",
      "Epoch 82: val_loss did not improve from 2536.60767\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2568.8945 - mse: 25234306.0000 - mae: 2569.2004 - val_loss: 2585.0239 - val_mse: 25824774.0000 - val_mae: 2585.2959 - lr: 5.0000e-04\n",
      "Epoch 83/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2608.1040 - mse: 25708958.0000 - mae: 2608.4126\n",
      "Epoch 83: val_loss did not improve from 2536.60767\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2602.7122 - mse: 25645520.0000 - mae: 2603.0205 - val_loss: 2604.0190 - val_mse: 26211400.0000 - val_mae: 2604.3008 - lr: 5.0000e-04\n",
      "Epoch 84/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2595.5151 - mse: 26185484.0000 - mae: 2595.8276\n",
      "Epoch 84: val_loss improved from 2536.60767 to 2519.07275, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2599.4021 - mse: 26269584.0000 - mae: 2599.7146 - val_loss: 2519.0728 - val_mse: 25637438.0000 - val_mae: 2519.3459 - lr: 5.0000e-04\n",
      "Epoch 85/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2564.2141 - mse: 25067874.0000 - mae: 2564.5259\n",
      "Epoch 85: val_loss improved from 2519.07275 to 2467.97949, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2566.5220 - mse: 25066448.0000 - mae: 2566.8340 - val_loss: 2467.9795 - val_mse: 23833978.0000 - val_mae: 2468.2556 - lr: 5.0000e-04\n",
      "Epoch 86/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2549.4541 - mse: 24863228.0000 - mae: 2549.7610\n",
      "Epoch 86: val_loss did not improve from 2467.97949\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2549.2576 - mse: 24859502.0000 - mae: 2549.5642 - val_loss: 2510.9512 - val_mse: 24673478.0000 - val_mae: 2511.2275 - lr: 5.0000e-04\n",
      "Epoch 87/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2638.0215 - mse: 26445788.0000 - mae: 2638.3318\n",
      "Epoch 87: val_loss did not improve from 2467.97949\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2644.6038 - mse: 26525452.0000 - mae: 2644.9146 - val_loss: 2670.8889 - val_mse: 26623236.0000 - val_mae: 2671.1953 - lr: 5.0000e-04\n",
      "Epoch 88/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2660.4456 - mse: 26391456.0000 - mae: 2660.7517\n",
      "Epoch 88: val_loss did not improve from 2467.97949\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2660.5898 - mse: 26375744.0000 - mae: 2660.8965 - val_loss: 2633.3953 - val_mse: 26755352.0000 - val_mae: 2633.6624 - lr: 5.0000e-04\n",
      "Epoch 89/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2576.5874 - mse: 25416690.0000 - mae: 2576.8884\n",
      "Epoch 89: val_loss did not improve from 2467.97949\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2584.6267 - mse: 25580160.0000 - mae: 2584.9277 - val_loss: 2616.9380 - val_mse: 26734480.0000 - val_mae: 2617.2424 - lr: 5.0000e-04\n",
      "Epoch 90/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2601.8611 - mse: 25743512.0000 - mae: 2602.1660\n",
      "Epoch 90: val_loss did not improve from 2467.97949\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2605.3564 - mse: 25785256.0000 - mae: 2605.6621 - val_loss: 2667.8394 - val_mse: 27429714.0000 - val_mae: 2668.1531 - lr: 5.0000e-04\n",
      "Epoch 91/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2581.3511 - mse: 25524794.0000 - mae: 2581.6548\n",
      "Epoch 91: val_loss improved from 2467.97949 to 2455.38818, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2594.6782 - mse: 25730574.0000 - mae: 2594.9822 - val_loss: 2455.3882 - val_mse: 23970578.0000 - val_mae: 2455.6860 - lr: 5.0000e-04\n",
      "Epoch 92/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2554.3989 - mse: 24825838.0000 - mae: 2554.7051\n",
      "Epoch 92: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2553.7202 - mse: 24810664.0000 - mae: 2554.0266 - val_loss: 2539.0781 - val_mse: 25994420.0000 - val_mae: 2539.3716 - lr: 5.0000e-04\n",
      "Epoch 93/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2573.6511 - mse: 25259118.0000 - mae: 2573.9624\n",
      "Epoch 93: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2573.0798 - mse: 25216460.0000 - mae: 2573.3921 - val_loss: 2682.8154 - val_mse: 26347176.0000 - val_mae: 2683.1143 - lr: 5.0000e-04\n",
      "Epoch 94/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2583.6218 - mse: 25307064.0000 - mae: 2583.9275\n",
      "Epoch 94: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2584.5613 - mse: 25273984.0000 - mae: 2584.8660 - val_loss: 2563.7659 - val_mse: 24836938.0000 - val_mae: 2564.0146 - lr: 5.0000e-04\n",
      "Epoch 95/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2714.4951 - mse: 27397590.0000 - mae: 2714.7966\n",
      "Epoch 95: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2719.5037 - mse: 27484610.0000 - mae: 2719.8052 - val_loss: 2784.0498 - val_mse: 29868344.0000 - val_mae: 2784.3167 - lr: 5.0000e-04\n",
      "Epoch 96/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2685.6882 - mse: 27580130.0000 - mae: 2685.9861\n",
      "Epoch 96: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2690.4578 - mse: 27615724.0000 - mae: 2690.7566 - val_loss: 2669.0012 - val_mse: 27679112.0000 - val_mae: 2669.2717 - lr: 5.0000e-04\n",
      "Epoch 97/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2642.5728 - mse: 26891464.0000 - mae: 2642.8687\n",
      "Epoch 97: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2654.6921 - mse: 27083930.0000 - mae: 2654.9895 - val_loss: 2701.7671 - val_mse: 28231664.0000 - val_mae: 2702.0835 - lr: 5.0000e-04\n",
      "Epoch 98/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2571.8672 - mse: 25588632.0000 - mae: 2572.1660\n",
      "Epoch 98: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2570.2583 - mse: 25564232.0000 - mae: 2570.5571 - val_loss: 2506.7471 - val_mse: 24370902.0000 - val_mae: 2507.0410 - lr: 5.0000e-04\n",
      "Epoch 99/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2566.4458 - mse: 25360178.0000 - mae: 2566.7473\n",
      "Epoch 99: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2577.4731 - mse: 25509054.0000 - mae: 2577.7751 - val_loss: 2553.0017 - val_mse: 25620802.0000 - val_mae: 2553.3101 - lr: 5.0000e-04\n",
      "Epoch 100/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2561.9385 - mse: 25231940.0000 - mae: 2562.2422\n",
      "Epoch 100: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2561.9385 - mse: 25231940.0000 - mae: 2562.2422 - val_loss: 2560.4414 - val_mse: 25506182.0000 - val_mae: 2560.7478 - lr: 5.0000e-04\n",
      "Epoch 101/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2619.5820 - mse: 26428590.0000 - mae: 2619.8872\n",
      "Epoch 101: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2621.8364 - mse: 26442960.0000 - mae: 2622.1416 - val_loss: 2586.8860 - val_mse: 26315028.0000 - val_mae: 2587.1812 - lr: 5.0000e-04\n",
      "Epoch 102/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2578.6479 - mse: 25754582.0000 - mae: 2578.9561\n",
      "Epoch 102: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2580.6929 - mse: 25787264.0000 - mae: 2581.0010 - val_loss: 2531.1243 - val_mse: 25159638.0000 - val_mae: 2531.4175 - lr: 2.5000e-04\n",
      "Epoch 103/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2584.8071 - mse: 25703364.0000 - mae: 2585.1101\n",
      "Epoch 103: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2589.7783 - mse: 25801644.0000 - mae: 2590.0818 - val_loss: 2615.0493 - val_mse: 26377714.0000 - val_mae: 2615.3530 - lr: 2.5000e-04\n",
      "Epoch 104/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2553.8330 - mse: 25238930.0000 - mae: 2554.1338\n",
      "Epoch 104: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2558.3494 - mse: 25281256.0000 - mae: 2558.6509 - val_loss: 2468.6431 - val_mse: 24513066.0000 - val_mae: 2468.9138 - lr: 2.5000e-04\n",
      "Epoch 105/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2506.8550 - mse: 24846630.0000 - mae: 2507.1555\n",
      "Epoch 105: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2506.8459 - mse: 24780702.0000 - mae: 2507.1475 - val_loss: 2512.5005 - val_mse: 26152950.0000 - val_mae: 2512.7805 - lr: 2.5000e-04\n",
      "Epoch 106/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2536.7256 - mse: 25457166.0000 - mae: 2537.0251\n",
      "Epoch 106: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2537.3552 - mse: 25490892.0000 - mae: 2537.6545 - val_loss: 2488.3782 - val_mse: 25213144.0000 - val_mae: 2488.6487 - lr: 2.5000e-04\n",
      "Epoch 107/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2508.7383 - mse: 24246942.0000 - mae: 2509.0452\n",
      "Epoch 107: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2510.6912 - mse: 24282684.0000 - mae: 2510.9985 - val_loss: 2551.9119 - val_mse: 25850018.0000 - val_mae: 2552.2024 - lr: 2.5000e-04\n",
      "Epoch 108/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2521.2163 - mse: 24770072.0000 - mae: 2521.5227\n",
      "Epoch 108: val_loss did not improve from 2455.38818\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2522.2085 - mse: 24771576.0000 - mae: 2522.5146 - val_loss: 2493.5510 - val_mse: 25287624.0000 - val_mae: 2493.8521 - lr: 2.5000e-04\n",
      "Epoch 109/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2495.3506 - mse: 24837970.0000 - mae: 2495.6519\n",
      "Epoch 109: val_loss improved from 2455.38818 to 2443.43384, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2497.5359 - mse: 24810142.0000 - mae: 2497.8379 - val_loss: 2443.4338 - val_mse: 24106106.0000 - val_mae: 2443.7161 - lr: 2.5000e-04\n",
      "Epoch 110/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2528.8462 - mse: 24987126.0000 - mae: 2529.1511\n",
      "Epoch 110: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2528.8462 - mse: 24987126.0000 - mae: 2529.1511 - val_loss: 2533.5156 - val_mse: 25729052.0000 - val_mae: 2533.7937 - lr: 2.5000e-04\n",
      "Epoch 111/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2510.5815 - mse: 24571002.0000 - mae: 2510.8757\n",
      "Epoch 111: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2510.5815 - mse: 24571002.0000 - mae: 2510.8757 - val_loss: 2493.6965 - val_mse: 25055730.0000 - val_mae: 2493.9917 - lr: 2.5000e-04\n",
      "Epoch 112/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2492.0872 - mse: 24263366.0000 - mae: 2492.3860\n",
      "Epoch 112: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2492.4307 - mse: 24267188.0000 - mae: 2492.7295 - val_loss: 2486.2170 - val_mse: 25385348.0000 - val_mae: 2486.5063 - lr: 2.5000e-04\n",
      "Epoch 113/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2469.9880 - mse: 24418960.0000 - mae: 2470.2881\n",
      "Epoch 113: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2475.2507 - mse: 24455446.0000 - mae: 2475.5510 - val_loss: 2483.7368 - val_mse: 24715432.0000 - val_mae: 2484.0361 - lr: 2.5000e-04\n",
      "Epoch 114/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2498.6160 - mse: 24707518.0000 - mae: 2498.9167\n",
      "Epoch 114: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2502.6003 - mse: 24775110.0000 - mae: 2502.9014 - val_loss: 2541.1404 - val_mse: 25841316.0000 - val_mae: 2541.4429 - lr: 2.5000e-04\n",
      "Epoch 115/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2476.4402 - mse: 24117212.0000 - mae: 2476.7424\n",
      "Epoch 115: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2473.0989 - mse: 24102224.0000 - mae: 2473.4011 - val_loss: 2446.3381 - val_mse: 24646876.0000 - val_mae: 2446.6389 - lr: 2.5000e-04\n",
      "Epoch 116/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2478.5066 - mse: 24385928.0000 - mae: 2478.8091\n",
      "Epoch 116: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2480.7410 - mse: 24431404.0000 - mae: 2481.0435 - val_loss: 2552.3193 - val_mse: 26331052.0000 - val_mae: 2552.5964 - lr: 2.5000e-04\n",
      "Epoch 117/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2467.1858 - mse: 24076058.0000 - mae: 2467.4832\n",
      "Epoch 117: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2472.9148 - mse: 24132082.0000 - mae: 2473.2124 - val_loss: 2505.1274 - val_mse: 25346088.0000 - val_mae: 2505.4055 - lr: 2.5000e-04\n",
      "Epoch 118/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2487.1855 - mse: 24409838.0000 - mae: 2487.4868\n",
      "Epoch 118: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2488.2444 - mse: 24405886.0000 - mae: 2488.5459 - val_loss: 2608.1255 - val_mse: 26854520.0000 - val_mae: 2608.4036 - lr: 2.5000e-04\n",
      "Epoch 119/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2482.2810 - mse: 24287870.0000 - mae: 2482.5798\n",
      "Epoch 119: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2481.5920 - mse: 24266340.0000 - mae: 2481.8909 - val_loss: 2504.4856 - val_mse: 24992326.0000 - val_mae: 2504.7595 - lr: 2.5000e-04\n",
      "Epoch 120/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2459.8176 - mse: 24243528.0000 - mae: 2460.1155\n",
      "Epoch 120: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2470.1169 - mse: 24386620.0000 - mae: 2470.4153 - val_loss: 2522.8801 - val_mse: 25764558.0000 - val_mae: 2523.1711 - lr: 1.2500e-04\n",
      "Epoch 121/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2474.2603 - mse: 24352236.0000 - mae: 2474.5596\n",
      "Epoch 121: val_loss did not improve from 2443.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2473.5913 - mse: 24344080.0000 - mae: 2473.8906 - val_loss: 2461.4221 - val_mse: 24667828.0000 - val_mae: 2461.7104 - lr: 1.2500e-04\n",
      "Epoch 122/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2508.5630 - mse: 24635488.0000 - mae: 2508.8613\n",
      "Epoch 122: val_loss improved from 2443.43384 to 2434.06860, saving model to new_stne_lstm_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2508.1987 - mse: 24627744.0000 - mae: 2508.4968 - val_loss: 2434.0686 - val_mse: 24042398.0000 - val_mae: 2434.3401 - lr: 1.2500e-04\n",
      "Epoch 123/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2493.5183 - mse: 24572250.0000 - mae: 2493.8186\n",
      "Epoch 123: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2493.5183 - mse: 24572250.0000 - mae: 2493.8186 - val_loss: 2436.7690 - val_mse: 24500960.0000 - val_mae: 2437.0547 - lr: 1.2500e-04\n",
      "Epoch 124/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2501.2185 - mse: 24890502.0000 - mae: 2501.5132\n",
      "Epoch 124: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2501.7026 - mse: 24872382.0000 - mae: 2501.9971 - val_loss: 2449.7993 - val_mse: 24364586.0000 - val_mae: 2450.0706 - lr: 1.2500e-04\n",
      "Epoch 125/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2480.1125 - mse: 23930590.0000 - mae: 2480.4099\n",
      "Epoch 125: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2478.9221 - mse: 23891150.0000 - mae: 2479.2195 - val_loss: 2447.9136 - val_mse: 24251654.0000 - val_mae: 2448.1985 - lr: 1.2500e-04\n",
      "Epoch 126/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2479.1565 - mse: 24238704.0000 - mae: 2479.4558\n",
      "Epoch 126: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2478.8340 - mse: 24231798.0000 - mae: 2479.1331 - val_loss: 2496.6008 - val_mse: 25249890.0000 - val_mae: 2496.8911 - lr: 1.2500e-04\n",
      "Epoch 127/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2484.6711 - mse: 24240444.0000 - mae: 2484.9680\n",
      "Epoch 127: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2487.2378 - mse: 24263276.0000 - mae: 2487.5347 - val_loss: 2497.8049 - val_mse: 24642494.0000 - val_mae: 2498.1011 - lr: 1.2500e-04\n",
      "Epoch 128/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2502.3284 - mse: 24249588.0000 - mae: 2502.6287\n",
      "Epoch 128: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2501.8916 - mse: 24267008.0000 - mae: 2502.1917 - val_loss: 2436.6411 - val_mse: 23919676.0000 - val_mae: 2436.9280 - lr: 1.2500e-04\n",
      "Epoch 129/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2456.7874 - mse: 23447866.0000 - mae: 2457.0884\n",
      "Epoch 129: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2460.4338 - mse: 23503760.0000 - mae: 2460.7349 - val_loss: 2435.4495 - val_mse: 23981338.0000 - val_mae: 2435.7327 - lr: 1.2500e-04\n",
      "Epoch 130/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2470.6951 - mse: 23691620.0000 - mae: 2470.9922\n",
      "Epoch 130: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2472.9189 - mse: 23699658.0000 - mae: 2473.2163 - val_loss: 2453.5210 - val_mse: 23727472.0000 - val_mae: 2453.7932 - lr: 1.2500e-04\n",
      "Epoch 131/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2477.2708 - mse: 23812268.0000 - mae: 2477.5662\n",
      "Epoch 131: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2483.9397 - mse: 23865274.0000 - mae: 2484.2356 - val_loss: 2460.1753 - val_mse: 24116286.0000 - val_mae: 2460.4626 - lr: 1.2500e-04\n",
      "Epoch 132/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2473.3440 - mse: 23940636.0000 - mae: 2473.6416\n",
      "Epoch 132: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2474.5034 - mse: 23961088.0000 - mae: 2474.8015 - val_loss: 2496.6704 - val_mse: 25155368.0000 - val_mae: 2496.9619 - lr: 1.2500e-04\n",
      "Epoch 133/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2463.0488 - mse: 23963718.0000 - mae: 2463.3479\n",
      "Epoch 133: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2463.0488 - mse: 23963718.0000 - mae: 2463.3479 - val_loss: 2484.0291 - val_mse: 24617910.0000 - val_mae: 2484.3145 - lr: 6.2500e-05\n",
      "Epoch 134/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2445.0591 - mse: 23500678.0000 - mae: 2445.3584\n",
      "Epoch 134: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2455.7019 - mse: 23652702.0000 - mae: 2456.0015 - val_loss: 2444.0798 - val_mse: 24141876.0000 - val_mae: 2444.3740 - lr: 6.2500e-05\n",
      "Epoch 135/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2432.4685 - mse: 23189502.0000 - mae: 2432.7676\n",
      "Epoch 135: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2435.4580 - mse: 23222078.0000 - mae: 2435.7568 - val_loss: 2441.7192 - val_mse: 23726474.0000 - val_mae: 2442.0059 - lr: 6.2500e-05\n",
      "Epoch 136/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2440.3103 - mse: 23288184.0000 - mae: 2440.6104\n",
      "Epoch 136: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2444.8176 - mse: 23333722.0000 - mae: 2445.1177 - val_loss: 2462.3386 - val_mse: 24221746.0000 - val_mae: 2462.6350 - lr: 6.2500e-05\n",
      "Epoch 137/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2441.0894 - mse: 23546214.0000 - mae: 2441.3894\n",
      "Epoch 137: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2446.1670 - mse: 23620624.0000 - mae: 2446.4675 - val_loss: 2451.9771 - val_mse: 24404800.0000 - val_mae: 2452.2732 - lr: 6.2500e-05\n",
      "Epoch 138/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2459.3704 - mse: 23705134.0000 - mae: 2459.6711\n",
      "Epoch 138: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2459.6846 - mse: 23709998.0000 - mae: 2459.9858 - val_loss: 2440.0320 - val_mse: 23875168.0000 - val_mae: 2440.3162 - lr: 6.2500e-05\n",
      "Epoch 139/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2437.4407 - mse: 23311820.0000 - mae: 2437.7407\n",
      "Epoch 139: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2437.4407 - mse: 23311820.0000 - mae: 2437.7407 - val_loss: 2467.6252 - val_mse: 24156976.0000 - val_mae: 2467.9185 - lr: 6.2500e-05\n",
      "Epoch 140/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2424.9597 - mse: 23177336.0000 - mae: 2425.2566\n",
      "Epoch 140: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2426.6318 - mse: 23167320.0000 - mae: 2426.9299 - val_loss: 2492.6560 - val_mse: 24382914.0000 - val_mae: 2492.9502 - lr: 6.2500e-05\n",
      "Epoch 141/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2433.6162 - mse: 23329538.0000 - mae: 2433.9146\n",
      "Epoch 141: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2436.2847 - mse: 23300976.0000 - mae: 2436.5845 - val_loss: 2472.0032 - val_mse: 24264598.0000 - val_mae: 2472.2932 - lr: 6.2500e-05\n",
      "Epoch 142/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2433.5747 - mse: 23377756.0000 - mae: 2433.8735\n",
      "Epoch 142: val_loss did not improve from 2434.06860\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2435.6526 - mse: 23417636.0000 - mae: 2435.9514 - val_loss: 2480.4131 - val_mse: 24166770.0000 - val_mae: 2480.7024 - lr: 6.2500e-05\n",
      "Epoch 1/500\n",
      "    432/Unknown - 3s 7ms/step - loss: 2428.5410 - mse: 23383288.0000 - mae: 2428.8372\n",
      "Epoch 1: val_loss improved from inf to 2468.99902, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2436.1465 - mse: 23469336.0000 - mae: 2436.4426 - val_loss: 2468.9990 - val_mse: 24020850.0000 - val_mae: 2469.2937 - lr: 3.1250e-05\n",
      "Epoch 2/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2421.3394 - mse: 23245774.0000 - mae: 2421.6357\n",
      "Epoch 2: val_loss improved from 2468.99902 to 2462.27246, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2429.7061 - mse: 23332826.0000 - mae: 2430.0032 - val_loss: 2462.2725 - val_mse: 23818278.0000 - val_mae: 2462.5674 - lr: 3.1250e-05\n",
      "Epoch 3/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2422.8855 - mse: 23293454.0000 - mae: 2423.1819\n",
      "Epoch 3: val_loss did not improve from 2462.27246\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2430.2559 - mse: 23385220.0000 - mae: 2430.5532 - val_loss: 2469.7068 - val_mse: 23887124.0000 - val_mae: 2470.0002 - lr: 3.1250e-05\n",
      "Epoch 4/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2429.2451 - mse: 23343044.0000 - mae: 2429.5420\n",
      "Epoch 4: val_loss improved from 2462.27246 to 2435.00171, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2435.3049 - mse: 23409116.0000 - mae: 2435.6023 - val_loss: 2435.0017 - val_mse: 23654788.0000 - val_mae: 2435.2981 - lr: 3.1250e-05\n",
      "Epoch 5/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2436.0854 - mse: 23334798.0000 - mae: 2436.3828\n",
      "Epoch 5: val_loss did not improve from 2435.00171\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2437.3040 - mse: 23349142.0000 - mae: 2437.6016 - val_loss: 2435.9326 - val_mse: 23651888.0000 - val_mae: 2436.2263 - lr: 3.1250e-05\n",
      "Epoch 6/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2426.1008 - mse: 23207140.0000 - mae: 2426.3999\n",
      "Epoch 6: val_loss did not improve from 2435.00171\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2432.7195 - mse: 23274410.0000 - mae: 2433.0186 - val_loss: 2437.4893 - val_mse: 23569160.0000 - val_mae: 2437.7830 - lr: 3.1250e-05\n",
      "Epoch 7/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2427.7642 - mse: 23126418.0000 - mae: 2428.0593\n",
      "Epoch 7: val_loss did not improve from 2435.00171\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2428.2927 - mse: 23125282.0000 - mae: 2428.5881 - val_loss: 2442.9102 - val_mse: 23577718.0000 - val_mae: 2443.2034 - lr: 3.1250e-05\n",
      "Epoch 8/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2415.9832 - mse: 22964492.0000 - mae: 2416.2791\n",
      "Epoch 8: val_loss improved from 2435.00171 to 2431.35229, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2422.7178 - mse: 23030832.0000 - mae: 2423.0142 - val_loss: 2431.3523 - val_mse: 23449498.0000 - val_mae: 2431.6460 - lr: 3.1250e-05\n",
      "Epoch 9/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2418.7217 - mse: 22958118.0000 - mae: 2419.0166\n",
      "Epoch 9: val_loss did not improve from 2431.35229\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2419.0645 - mse: 22956164.0000 - mae: 2419.3599 - val_loss: 2443.2019 - val_mse: 23722648.0000 - val_mae: 2443.4951 - lr: 3.1250e-05\n",
      "Epoch 10/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2408.0430 - mse: 22967200.0000 - mae: 2408.3384\n",
      "Epoch 10: val_loss did not improve from 2431.35229\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2411.1052 - mse: 22993026.0000 - mae: 2411.4009 - val_loss: 2456.0457 - val_mse: 23779762.0000 - val_mae: 2456.3411 - lr: 3.1250e-05\n",
      "Epoch 11/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2404.0947 - mse: 22911772.0000 - mae: 2404.3906\n",
      "Epoch 11: val_loss did not improve from 2431.35229\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2410.3752 - mse: 22971374.0000 - mae: 2410.6714 - val_loss: 2449.3687 - val_mse: 23654706.0000 - val_mae: 2449.6648 - lr: 3.1250e-05\n",
      "Epoch 12/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2412.8459 - mse: 22954154.0000 - mae: 2413.1436\n",
      "Epoch 12: val_loss did not improve from 2431.35229\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2413.1360 - mse: 22950150.0000 - mae: 2413.4338 - val_loss: 2440.5271 - val_mse: 23554094.0000 - val_mae: 2440.8242 - lr: 3.1250e-05\n",
      "Epoch 13/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2409.8181 - mse: 22888440.0000 - mae: 2410.1145\n",
      "Epoch 13: val_loss improved from 2431.35229 to 2424.31055, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2416.7688 - mse: 22964118.0000 - mae: 2417.0657 - val_loss: 2424.3105 - val_mse: 23556532.0000 - val_mae: 2424.6060 - lr: 3.1250e-05\n",
      "Epoch 14/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2415.5769 - mse: 22932556.0000 - mae: 2415.8723\n",
      "Epoch 14: val_loss did not improve from 2424.31055\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2415.9072 - mse: 22925216.0000 - mae: 2416.2026 - val_loss: 2436.2051 - val_mse: 23619550.0000 - val_mae: 2436.4990 - lr: 3.1250e-05\n",
      "Epoch 15/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2420.1030 - mse: 23066358.0000 - mae: 2420.4001\n",
      "Epoch 15: val_loss did not improve from 2424.31055\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2420.5435 - mse: 23059856.0000 - mae: 2420.8406 - val_loss: 2443.1892 - val_mse: 23651532.0000 - val_mae: 2443.4839 - lr: 3.1250e-05\n",
      "Epoch 16/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2408.4070 - mse: 22840520.0000 - mae: 2408.7034\n",
      "Epoch 16: val_loss did not improve from 2424.31055\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2415.6199 - mse: 22907680.0000 - mae: 2415.9167 - val_loss: 2441.9524 - val_mse: 23630588.0000 - val_mae: 2442.2480 - lr: 3.1250e-05\n",
      "Epoch 17/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2410.3276 - mse: 22793656.0000 - mae: 2410.6243\n",
      "Epoch 17: val_loss improved from 2424.31055 to 2423.33887, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2410.3276 - mse: 22793656.0000 - mae: 2410.6243 - val_loss: 2423.3389 - val_mse: 23388292.0000 - val_mae: 2423.6335 - lr: 3.1250e-05\n",
      "Epoch 18/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2395.8267 - mse: 22620920.0000 - mae: 2396.1233\n",
      "Epoch 18: val_loss improved from 2423.33887 to 2416.67725, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2404.8950 - mse: 22704008.0000 - mae: 2405.1924 - val_loss: 2416.6772 - val_mse: 23361274.0000 - val_mae: 2416.9717 - lr: 3.1250e-05\n",
      "Epoch 19/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2392.1240 - mse: 22572992.0000 - mae: 2392.4221\n",
      "Epoch 19: val_loss improved from 2416.67725 to 2415.00708, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2399.3113 - mse: 22650806.0000 - mae: 2399.6099 - val_loss: 2415.0071 - val_mse: 23221326.0000 - val_mae: 2415.3015 - lr: 3.1250e-05\n",
      "Epoch 20/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2390.2437 - mse: 22439290.0000 - mae: 2390.5415\n",
      "Epoch 20: val_loss did not improve from 2415.00708\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2390.2437 - mse: 22439290.0000 - mae: 2390.5415 - val_loss: 2417.2444 - val_mse: 23266880.0000 - val_mae: 2417.5374 - lr: 3.1250e-05\n",
      "Epoch 21/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2380.1924 - mse: 22374340.0000 - mae: 2380.4868\n",
      "Epoch 21: val_loss improved from 2415.00708 to 2411.59912, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2388.6260 - mse: 22486834.0000 - mae: 2388.9209 - val_loss: 2411.5991 - val_mse: 23333554.0000 - val_mae: 2411.8938 - lr: 3.1250e-05\n",
      "Epoch 22/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2375.9512 - mse: 22283348.0000 - mae: 2376.2485\n",
      "Epoch 22: val_loss did not improve from 2411.59912\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2382.8787 - mse: 22348994.0000 - mae: 2383.1763 - val_loss: 2428.9939 - val_mse: 23426488.0000 - val_mae: 2429.2903 - lr: 3.1250e-05\n",
      "Epoch 23/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2383.7812 - mse: 22318176.0000 - mae: 2384.0786\n",
      "Epoch 23: val_loss did not improve from 2411.59912\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2393.3542 - mse: 22430270.0000 - mae: 2393.6521 - val_loss: 2435.8284 - val_mse: 23498624.0000 - val_mae: 2436.1191 - lr: 3.1250e-05\n",
      "Epoch 24/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2379.9072 - mse: 22301694.0000 - mae: 2380.2017\n",
      "Epoch 24: val_loss did not improve from 2411.59912\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2386.8103 - mse: 22378248.0000 - mae: 2387.1055 - val_loss: 2422.1592 - val_mse: 23478242.0000 - val_mae: 2422.4543 - lr: 3.1250e-05\n",
      "Epoch 25/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2389.7244 - mse: 22423088.0000 - mae: 2390.0247\n",
      "Epoch 25: val_loss did not improve from 2411.59912\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2389.7244 - mse: 22423088.0000 - mae: 2390.0247 - val_loss: 2412.6228 - val_mse: 23312484.0000 - val_mae: 2412.9207 - lr: 3.1250e-05\n",
      "Epoch 26/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2374.2607 - mse: 22255964.0000 - mae: 2374.5583\n",
      "Epoch 26: val_loss improved from 2411.59912 to 2397.63208, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2381.4832 - mse: 22344962.0000 - mae: 2381.7810 - val_loss: 2397.6321 - val_mse: 23180040.0000 - val_mae: 2397.9282 - lr: 3.1250e-05\n",
      "Epoch 27/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2376.9321 - mse: 22310004.0000 - mae: 2377.2322\n",
      "Epoch 27: val_loss improved from 2397.63208 to 2391.69482, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2380.2932 - mse: 22349530.0000 - mae: 2380.5938 - val_loss: 2391.6948 - val_mse: 23153638.0000 - val_mae: 2391.9907 - lr: 3.1250e-05\n",
      "Epoch 28/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2360.7952 - mse: 22121410.0000 - mae: 2361.0916\n",
      "Epoch 28: val_loss improved from 2391.69482 to 2372.51343, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2370.0464 - mse: 22232992.0000 - mae: 2370.3435 - val_loss: 2372.5134 - val_mse: 22851620.0000 - val_mae: 2372.8113 - lr: 3.1250e-05\n",
      "Epoch 29/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2366.1853 - mse: 22247800.0000 - mae: 2366.4841\n",
      "Epoch 29: val_loss did not improve from 2372.51343\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2366.6831 - mse: 22243278.0000 - mae: 2366.9824 - val_loss: 2386.7063 - val_mse: 22888506.0000 - val_mae: 2387.0034 - lr: 3.1250e-05\n",
      "Epoch 30/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2367.8577 - mse: 22272914.0000 - mae: 2368.1538\n",
      "Epoch 30: val_loss did not improve from 2372.51343\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2368.4036 - mse: 22264478.0000 - mae: 2368.7000 - val_loss: 2388.6624 - val_mse: 22869792.0000 - val_mae: 2388.9575 - lr: 3.1250e-05\n",
      "Epoch 31/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2369.0940 - mse: 22186600.0000 - mae: 2369.3911\n",
      "Epoch 31: val_loss did not improve from 2372.51343\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2376.3545 - mse: 22279244.0000 - mae: 2376.6521 - val_loss: 2407.8945 - val_mse: 23080400.0000 - val_mae: 2408.1875 - lr: 3.1250e-05\n",
      "Epoch 32/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2377.4119 - mse: 22383612.0000 - mae: 2377.7087\n",
      "Epoch 32: val_loss did not improve from 2372.51343\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2380.7839 - mse: 22425124.0000 - mae: 2381.0813 - val_loss: 2403.4673 - val_mse: 23101444.0000 - val_mae: 2403.7617 - lr: 3.1250e-05\n",
      "Epoch 33/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2375.7278 - mse: 22375594.0000 - mae: 2376.0249\n",
      "Epoch 33: val_loss did not improve from 2372.51343\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2375.7278 - mse: 22375594.0000 - mae: 2376.0249 - val_loss: 2401.0315 - val_mse: 23014328.0000 - val_mae: 2401.3259 - lr: 3.1250e-05\n",
      "Epoch 34/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2372.0051 - mse: 22232236.0000 - mae: 2372.3020\n",
      "Epoch 34: val_loss did not improve from 2372.51343\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2372.2253 - mse: 22221964.0000 - mae: 2372.5225 - val_loss: 2379.6770 - val_mse: 22875756.0000 - val_mae: 2379.9727 - lr: 3.1250e-05\n",
      "Epoch 35/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2367.7080 - mse: 22252482.0000 - mae: 2368.0078\n",
      "Epoch 35: val_loss improved from 2372.51343 to 2366.43506, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2368.2896 - mse: 22244380.0000 - mae: 2368.5896 - val_loss: 2366.4351 - val_mse: 22739460.0000 - val_mae: 2366.7327 - lr: 3.1250e-05\n",
      "Epoch 36/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2351.1130 - mse: 21983884.0000 - mae: 2351.4089\n",
      "Epoch 36: val_loss improved from 2366.43506 to 2365.59912, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2360.3013 - mse: 22081662.0000 - mae: 2360.5979 - val_loss: 2365.5991 - val_mse: 22589952.0000 - val_mae: 2365.8945 - lr: 3.1250e-05\n",
      "Epoch 37/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2341.2422 - mse: 21884998.0000 - mae: 2341.5383\n",
      "Epoch 37: val_loss did not improve from 2365.59912\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2345.0120 - mse: 21927070.0000 - mae: 2345.3083 - val_loss: 2377.3955 - val_mse: 23096242.0000 - val_mae: 2377.6895 - lr: 3.1250e-05\n",
      "Epoch 38/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2340.4099 - mse: 21863574.0000 - mae: 2340.7053\n",
      "Epoch 38: val_loss improved from 2365.59912 to 2365.05640, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2348.4353 - mse: 21962850.0000 - mae: 2348.7312 - val_loss: 2365.0564 - val_mse: 22524870.0000 - val_mae: 2365.3530 - lr: 3.1250e-05\n",
      "Epoch 39/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2354.5068 - mse: 21966492.0000 - mae: 2354.8049\n",
      "Epoch 39: val_loss did not improve from 2365.05640\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2354.9268 - mse: 21956682.0000 - mae: 2355.2249 - val_loss: 2380.5920 - val_mse: 22638542.0000 - val_mae: 2380.8887 - lr: 3.1250e-05\n",
      "Epoch 40/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2357.8567 - mse: 22024334.0000 - mae: 2358.1504\n",
      "Epoch 40: val_loss did not improve from 2365.05640\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2358.8396 - mse: 22030084.0000 - mae: 2359.1338 - val_loss: 2375.9285 - val_mse: 22753174.0000 - val_mae: 2376.2231 - lr: 3.1250e-05\n",
      "Epoch 41/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2336.1250 - mse: 21826536.0000 - mae: 2336.4229\n",
      "Epoch 41: val_loss improved from 2365.05640 to 2357.22241, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2343.5781 - mse: 21914244.0000 - mae: 2343.8760 - val_loss: 2357.2224 - val_mse: 22446358.0000 - val_mae: 2357.5183 - lr: 3.1250e-05\n",
      "Epoch 42/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2339.9578 - mse: 21852200.0000 - mae: 2340.2568\n",
      "Epoch 42: val_loss did not improve from 2357.22241\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2340.4985 - mse: 21853292.0000 - mae: 2340.7979 - val_loss: 2359.4680 - val_mse: 22438066.0000 - val_mae: 2359.7649 - lr: 3.1250e-05\n",
      "Epoch 43/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2338.9561 - mse: 21845284.0000 - mae: 2339.2537\n",
      "Epoch 43: val_loss improved from 2357.22241 to 2353.84204, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2346.3555 - mse: 21932962.0000 - mae: 2346.6536 - val_loss: 2353.8420 - val_mse: 22341492.0000 - val_mae: 2354.1389 - lr: 3.1250e-05\n",
      "Epoch 44/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2358.4033 - mse: 22066750.0000 - mae: 2358.7002\n",
      "Epoch 44: val_loss did not improve from 2353.84204\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2365.4756 - mse: 22152212.0000 - mae: 2365.7725 - val_loss: 2392.4934 - val_mse: 23179002.0000 - val_mae: 2392.7930 - lr: 3.1250e-05\n",
      "Epoch 45/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2339.5847 - mse: 21908570.0000 - mae: 2339.8843\n",
      "Epoch 45: val_loss did not improve from 2353.84204\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2346.7688 - mse: 21992686.0000 - mae: 2347.0684 - val_loss: 2381.4521 - val_mse: 22850952.0000 - val_mae: 2381.7539 - lr: 3.1250e-05\n",
      "Epoch 46/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2338.0198 - mse: 21844250.0000 - mae: 2338.3230\n",
      "Epoch 46: val_loss did not improve from 2353.84204\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2341.7842 - mse: 21885354.0000 - mae: 2342.0876 - val_loss: 2385.1526 - val_mse: 22810676.0000 - val_mae: 2385.4485 - lr: 3.1250e-05\n",
      "Epoch 47/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2340.1152 - mse: 21815624.0000 - mae: 2340.4150\n",
      "Epoch 47: val_loss did not improve from 2353.84204\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2347.0027 - mse: 21891350.0000 - mae: 2347.3027 - val_loss: 2374.6404 - val_mse: 22788320.0000 - val_mae: 2374.9360 - lr: 3.1250e-05\n",
      "Epoch 48/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2339.4768 - mse: 21779164.0000 - mae: 2339.7766\n",
      "Epoch 48: val_loss did not improve from 2353.84204\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2340.1587 - mse: 21771202.0000 - mae: 2340.4587 - val_loss: 2370.5876 - val_mse: 22745516.0000 - val_mae: 2370.8845 - lr: 3.1250e-05\n",
      "Epoch 49/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2332.1646 - mse: 21785322.0000 - mae: 2332.4639\n",
      "Epoch 49: val_loss did not improve from 2353.84204\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2338.9924 - mse: 21850828.0000 - mae: 2339.2920 - val_loss: 2380.9004 - val_mse: 22699392.0000 - val_mae: 2381.1953 - lr: 3.1250e-05\n",
      "Epoch 50/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2330.8625 - mse: 21682974.0000 - mae: 2331.1616\n",
      "Epoch 50: val_loss did not improve from 2353.84204\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2340.3831 - mse: 21781850.0000 - mae: 2340.6826 - val_loss: 2364.7888 - val_mse: 22341848.0000 - val_mae: 2365.0852 - lr: 3.1250e-05\n",
      "Epoch 51/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2324.7959 - mse: 21576272.0000 - mae: 2325.0935\n",
      "Epoch 51: val_loss improved from 2353.84204 to 2349.60107, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2334.2205 - mse: 21672570.0000 - mae: 2334.5183 - val_loss: 2349.6011 - val_mse: 22153024.0000 - val_mae: 2349.8948 - lr: 3.1250e-05\n",
      "Epoch 52/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2327.4580 - mse: 21664814.0000 - mae: 2327.7593\n",
      "Epoch 52: val_loss did not improve from 2349.60107\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2334.0576 - mse: 21731582.0000 - mae: 2334.3594 - val_loss: 2367.1877 - val_mse: 22480634.0000 - val_mae: 2367.4819 - lr: 3.1250e-05\n",
      "Epoch 53/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2325.2725 - mse: 21621690.0000 - mae: 2325.5713\n",
      "Epoch 53: val_loss did not improve from 2349.60107\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2328.7993 - mse: 21664118.0000 - mae: 2329.0986 - val_loss: 2354.7168 - val_mse: 22449190.0000 - val_mae: 2355.0088 - lr: 3.1250e-05\n",
      "Epoch 54/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2323.8853 - mse: 21590156.0000 - mae: 2324.1824\n",
      "Epoch 54: val_loss did not improve from 2349.60107\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2334.4529 - mse: 21714292.0000 - mae: 2334.7502 - val_loss: 2354.4749 - val_mse: 22205234.0000 - val_mae: 2354.7673 - lr: 3.1250e-05\n",
      "Epoch 55/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2337.5854 - mse: 21876282.0000 - mae: 2337.8816\n",
      "Epoch 55: val_loss did not improve from 2349.60107\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2344.8372 - mse: 21953934.0000 - mae: 2345.1340 - val_loss: 2364.4326 - val_mse: 22347084.0000 - val_mae: 2364.7229 - lr: 3.1250e-05\n",
      "Epoch 56/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2343.0178 - mse: 21983724.0000 - mae: 2343.3152\n",
      "Epoch 56: val_loss improved from 2349.60107 to 2347.20898, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2353.7703 - mse: 22090618.0000 - mae: 2354.0684 - val_loss: 2347.2090 - val_mse: 22490698.0000 - val_mae: 2347.5005 - lr: 3.1250e-05\n",
      "Epoch 57/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2347.6653 - mse: 22024734.0000 - mae: 2347.9631\n",
      "Epoch 57: val_loss did not improve from 2347.20898\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2347.9114 - mse: 22014462.0000 - mae: 2348.2092 - val_loss: 2361.4568 - val_mse: 22358838.0000 - val_mae: 2361.7478 - lr: 3.1250e-05\n",
      "Epoch 58/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2353.4758 - mse: 21941076.0000 - mae: 2353.7737\n",
      "Epoch 58: val_loss improved from 2347.20898 to 2337.86523, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2354.1155 - mse: 21937640.0000 - mae: 2354.4133 - val_loss: 2337.8652 - val_mse: 22138376.0000 - val_mae: 2338.1570 - lr: 3.1250e-05\n",
      "Epoch 59/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2343.4062 - mse: 21790472.0000 - mae: 2343.7070\n",
      "Epoch 59: val_loss did not improve from 2337.86523\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2347.5662 - mse: 21843644.0000 - mae: 2347.8669 - val_loss: 2358.5508 - val_mse: 22315316.0000 - val_mae: 2358.8450 - lr: 3.1250e-05\n",
      "Epoch 60/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2340.2456 - mse: 21805878.0000 - mae: 2340.5449\n",
      "Epoch 60: val_loss did not improve from 2337.86523\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2348.0854 - mse: 21899052.0000 - mae: 2348.3850 - val_loss: 2356.2236 - val_mse: 22276182.0000 - val_mae: 2356.5173 - lr: 3.1250e-05\n",
      "Epoch 61/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2337.0066 - mse: 21769746.0000 - mae: 2337.3069\n",
      "Epoch 61: val_loss did not improve from 2337.86523\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2337.3582 - mse: 21764836.0000 - mae: 2337.6587 - val_loss: 2356.6223 - val_mse: 22225902.0000 - val_mae: 2356.9155 - lr: 3.1250e-05\n",
      "Epoch 62/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2326.7991 - mse: 21625604.0000 - mae: 2327.0994\n",
      "Epoch 62: val_loss did not improve from 2337.86523\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2334.0044 - mse: 21704776.0000 - mae: 2334.3052 - val_loss: 2361.0000 - val_mse: 22383438.0000 - val_mae: 2361.2976 - lr: 3.1250e-05\n",
      "Epoch 63/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2331.6750 - mse: 21613476.0000 - mae: 2331.9761\n",
      "Epoch 63: val_loss did not improve from 2337.86523\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2338.8433 - mse: 21698470.0000 - mae: 2339.1448 - val_loss: 2376.3203 - val_mse: 22577368.0000 - val_mae: 2376.6160 - lr: 3.1250e-05\n",
      "Epoch 64/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2324.9934 - mse: 21506124.0000 - mae: 2325.2942\n",
      "Epoch 64: val_loss did not improve from 2337.86523\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2325.3140 - mse: 21496494.0000 - mae: 2325.6147 - val_loss: 2350.5605 - val_mse: 22275080.0000 - val_mae: 2350.8569 - lr: 3.1250e-05\n",
      "Epoch 65/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2320.2495 - mse: 21440692.0000 - mae: 2320.5500\n",
      "Epoch 65: val_loss improved from 2337.86523 to 2324.84082, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2320.2495 - mse: 21440692.0000 - mae: 2320.5500 - val_loss: 2324.8408 - val_mse: 22046322.0000 - val_mae: 2325.1333 - lr: 3.1250e-05\n",
      "Epoch 66/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2318.4326 - mse: 21492538.0000 - mae: 2318.7305\n",
      "Epoch 66: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2325.4568 - mse: 21571352.0000 - mae: 2325.7551 - val_loss: 2353.3374 - val_mse: 22258436.0000 - val_mae: 2353.6252 - lr: 3.1250e-05\n",
      "Epoch 67/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2322.1689 - mse: 21575184.0000 - mae: 2322.4653\n",
      "Epoch 67: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2331.1250 - mse: 21654460.0000 - mae: 2331.4219 - val_loss: 2349.1982 - val_mse: 22447676.0000 - val_mae: 2349.4873 - lr: 3.1250e-05\n",
      "Epoch 68/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2323.0215 - mse: 21667388.0000 - mae: 2323.3184\n",
      "Epoch 68: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2331.6858 - mse: 21739264.0000 - mae: 2331.9832 - val_loss: 2349.5828 - val_mse: 22439662.0000 - val_mae: 2349.8745 - lr: 3.1250e-05\n",
      "Epoch 69/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2325.5522 - mse: 21702620.0000 - mae: 2325.8491\n",
      "Epoch 69: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2326.2827 - mse: 21695984.0000 - mae: 2326.5798 - val_loss: 2356.0071 - val_mse: 22395702.0000 - val_mae: 2356.3000 - lr: 3.1250e-05\n",
      "Epoch 70/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2316.5557 - mse: 21591232.0000 - mae: 2316.8503\n",
      "Epoch 70: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2324.1943 - mse: 21678550.0000 - mae: 2324.4895 - val_loss: 2338.5696 - val_mse: 22204708.0000 - val_mae: 2338.8621 - lr: 3.1250e-05\n",
      "Epoch 71/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2316.9231 - mse: 21338018.0000 - mae: 2317.2195\n",
      "Epoch 71: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2321.6199 - mse: 21388922.0000 - mae: 2321.9165 - val_loss: 2354.9832 - val_mse: 22191124.0000 - val_mae: 2355.2742 - lr: 3.1250e-05\n",
      "Epoch 72/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2308.1753 - mse: 21331490.0000 - mae: 2308.4736\n",
      "Epoch 72: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2312.1597 - mse: 21377720.0000 - mae: 2312.4583 - val_loss: 2337.0273 - val_mse: 22059322.0000 - val_mae: 2337.3176 - lr: 3.1250e-05\n",
      "Epoch 73/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2312.8208 - mse: 21390246.0000 - mae: 2313.1187\n",
      "Epoch 73: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2313.4944 - mse: 21383164.0000 - mae: 2313.7927 - val_loss: 2337.4351 - val_mse: 22156862.0000 - val_mae: 2337.7261 - lr: 3.1250e-05\n",
      "Epoch 74/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2318.0491 - mse: 21489934.0000 - mae: 2318.3433\n",
      "Epoch 74: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2327.4868 - mse: 21591536.0000 - mae: 2327.7817 - val_loss: 2332.1226 - val_mse: 21974084.0000 - val_mae: 2332.4099 - lr: 3.1250e-05\n",
      "Epoch 75/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2318.1958 - mse: 21701590.0000 - mae: 2318.4934\n",
      "Epoch 75: val_loss did not improve from 2324.84082\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2318.9890 - mse: 21696648.0000 - mae: 2319.2869 - val_loss: 2333.7434 - val_mse: 22081552.0000 - val_mae: 2334.0352 - lr: 3.1250e-05\n",
      "Epoch 76/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2313.3696 - mse: 21592760.0000 - mae: 2313.6665\n",
      "Epoch 76: val_loss improved from 2324.84082 to 2299.81714, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2313.3696 - mse: 21592760.0000 - mae: 2313.6665 - val_loss: 2299.8171 - val_mse: 21585324.0000 - val_mae: 2300.1072 - lr: 1.5625e-05\n",
      "Epoch 77/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2300.4653 - mse: 21306572.0000 - mae: 2300.7625\n",
      "Epoch 77: val_loss did not improve from 2299.81714\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2310.0920 - mse: 21441578.0000 - mae: 2310.3901 - val_loss: 2303.6848 - val_mse: 21669868.0000 - val_mae: 2303.9778 - lr: 1.5625e-05\n",
      "Epoch 78/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2302.8091 - mse: 21373948.0000 - mae: 2303.1084\n",
      "Epoch 78: val_loss did not improve from 2299.81714\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2310.3408 - mse: 21459484.0000 - mae: 2310.6404 - val_loss: 2300.1143 - val_mse: 21540050.0000 - val_mae: 2300.4077 - lr: 1.5625e-05\n",
      "Epoch 79/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2295.6965 - mse: 21294968.0000 - mae: 2295.9954\n",
      "Epoch 79: val_loss did not improve from 2299.81714\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2303.2041 - mse: 21376616.0000 - mae: 2303.5034 - val_loss: 2301.6934 - val_mse: 21563004.0000 - val_mae: 2301.9866 - lr: 1.5625e-05\n",
      "Epoch 80/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2290.7354 - mse: 21158390.0000 - mae: 2291.0339\n",
      "Epoch 80: val_loss improved from 2299.81714 to 2295.46289, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2301.1108 - mse: 21306774.0000 - mae: 2301.4102 - val_loss: 2295.4629 - val_mse: 21380008.0000 - val_mae: 2295.7554 - lr: 1.5625e-05\n",
      "Epoch 81/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2292.5422 - mse: 21215762.0000 - mae: 2292.8413\n",
      "Epoch 81: val_loss did not improve from 2295.46289\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2299.8254 - mse: 21296140.0000 - mae: 2300.1248 - val_loss: 2298.7441 - val_mse: 21660540.0000 - val_mae: 2299.0371 - lr: 1.5625e-05\n",
      "Epoch 82/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2293.3525 - mse: 21195266.0000 - mae: 2293.6516\n",
      "Epoch 82: val_loss did not improve from 2295.46289\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2303.1326 - mse: 21299202.0000 - mae: 2303.4316 - val_loss: 2325.5117 - val_mse: 22256208.0000 - val_mae: 2325.8066 - lr: 1.5625e-05\n",
      "Epoch 83/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2295.6411 - mse: 21246726.0000 - mae: 2295.9397\n",
      "Epoch 83: val_loss did not improve from 2295.46289\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2296.1406 - mse: 21238554.0000 - mae: 2296.4392 - val_loss: 2306.0898 - val_mse: 21682740.0000 - val_mae: 2306.3838 - lr: 1.5625e-05\n",
      "Epoch 84/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2284.6748 - mse: 21064218.0000 - mae: 2284.9741\n",
      "Epoch 84: val_loss did not improve from 2295.46289\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2292.4241 - mse: 21161216.0000 - mae: 2292.7236 - val_loss: 2304.1333 - val_mse: 21616784.0000 - val_mae: 2304.4265 - lr: 1.5625e-05\n",
      "Epoch 85/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2282.9944 - mse: 21053032.0000 - mae: 2283.2937\n",
      "Epoch 85: val_loss did not improve from 2295.46289\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2292.4788 - mse: 21151332.0000 - mae: 2292.7786 - val_loss: 2300.5012 - val_mse: 21663438.0000 - val_mae: 2300.7949 - lr: 1.5625e-05\n",
      "Epoch 86/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2293.3921 - mse: 21217320.0000 - mae: 2293.6909\n",
      "Epoch 86: val_loss did not improve from 2295.46289\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2297.2834 - mse: 21265624.0000 - mae: 2297.5825 - val_loss: 2305.4326 - val_mse: 21744182.0000 - val_mae: 2305.7261 - lr: 1.5625e-05\n",
      "Epoch 87/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2293.2766 - mse: 21188244.0000 - mae: 2293.5757\n",
      "Epoch 87: val_loss improved from 2295.46289 to 2291.69897, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2294.0378 - mse: 21185624.0000 - mae: 2294.3372 - val_loss: 2291.6990 - val_mse: 21533082.0000 - val_mae: 2291.9907 - lr: 1.5625e-05\n",
      "Epoch 88/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2284.2769 - mse: 21087522.0000 - mae: 2284.5769\n",
      "Epoch 88: val_loss did not improve from 2291.69897\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2294.1208 - mse: 21229046.0000 - mae: 2294.4216 - val_loss: 2300.9841 - val_mse: 21682792.0000 - val_mae: 2301.2773 - lr: 1.5625e-05\n",
      "Epoch 89/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2282.2139 - mse: 21092238.0000 - mae: 2282.5122\n",
      "Epoch 89: val_loss did not improve from 2291.69897\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2289.6360 - mse: 21179210.0000 - mae: 2289.9346 - val_loss: 2298.3699 - val_mse: 21659726.0000 - val_mae: 2298.6621 - lr: 1.5625e-05\n",
      "Epoch 90/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2284.8965 - mse: 21159826.0000 - mae: 2285.1965\n",
      "Epoch 90: val_loss did not improve from 2291.69897\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2285.6355 - mse: 21155794.0000 - mae: 2285.9355 - val_loss: 2294.4294 - val_mse: 21583980.0000 - val_mae: 2294.7234 - lr: 1.5625e-05\n",
      "Epoch 91/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2281.4749 - mse: 21063700.0000 - mae: 2281.7720\n",
      "Epoch 91: val_loss did not improve from 2291.69897\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2288.5010 - mse: 21138064.0000 - mae: 2288.7986 - val_loss: 2303.9280 - val_mse: 21686704.0000 - val_mae: 2304.2195 - lr: 1.5625e-05\n",
      "Epoch 92/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2282.9871 - mse: 21067266.0000 - mae: 2283.2852\n",
      "Epoch 92: val_loss did not improve from 2291.69897\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2290.3748 - mse: 21155706.0000 - mae: 2290.6731 - val_loss: 2295.2969 - val_mse: 21504952.0000 - val_mae: 2295.5881 - lr: 1.5625e-05\n",
      "Epoch 93/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2280.0447 - mse: 21114896.0000 - mae: 2280.3428\n",
      "Epoch 93: val_loss improved from 2291.69897 to 2283.80518, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2283.7734 - mse: 21162860.0000 - mae: 2284.0720 - val_loss: 2283.8052 - val_mse: 21507744.0000 - val_mae: 2284.0972 - lr: 1.5625e-05\n",
      "Epoch 94/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2285.2029 - mse: 21114836.0000 - mae: 2285.5034\n",
      "Epoch 94: val_loss did not improve from 2283.80518\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2288.9043 - mse: 21162506.0000 - mae: 2289.2051 - val_loss: 2287.6714 - val_mse: 21514196.0000 - val_mae: 2287.9631 - lr: 1.5625e-05\n",
      "Epoch 95/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2290.7444 - mse: 21185374.0000 - mae: 2291.0435\n",
      "Epoch 95: val_loss improved from 2283.80518 to 2281.41895, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2291.3408 - mse: 21177688.0000 - mae: 2291.6401 - val_loss: 2281.4189 - val_mse: 21450872.0000 - val_mae: 2281.7102 - lr: 1.5625e-05\n",
      "Epoch 96/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2290.2847 - mse: 21153500.0000 - mae: 2290.5818\n",
      "Epoch 96: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2290.2847 - mse: 21153500.0000 - mae: 2290.5818 - val_loss: 2284.2908 - val_mse: 21451712.0000 - val_mae: 2284.5823 - lr: 1.5625e-05\n",
      "Epoch 97/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2279.8308 - mse: 20961418.0000 - mae: 2280.1270\n",
      "Epoch 97: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2288.8557 - mse: 21077076.0000 - mae: 2289.1528 - val_loss: 2292.4883 - val_mse: 21491620.0000 - val_mae: 2292.7810 - lr: 1.5625e-05\n",
      "Epoch 98/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2278.7483 - mse: 21001680.0000 - mae: 2279.0474\n",
      "Epoch 98: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2287.9580 - mse: 21115588.0000 - mae: 2288.2578 - val_loss: 2303.3032 - val_mse: 21580706.0000 - val_mae: 2303.5974 - lr: 1.5625e-05\n",
      "Epoch 99/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2276.4026 - mse: 21021474.0000 - mae: 2276.7029\n",
      "Epoch 99: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2284.8181 - mse: 21107292.0000 - mae: 2285.1189 - val_loss: 2290.8120 - val_mse: 21416546.0000 - val_mae: 2291.1040 - lr: 1.5625e-05\n",
      "Epoch 100/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2285.7209 - mse: 21088016.0000 - mae: 2286.0215\n",
      "Epoch 100: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2286.0513 - mse: 21081994.0000 - mae: 2286.3521 - val_loss: 2292.0654 - val_mse: 21440684.0000 - val_mae: 2292.3584 - lr: 1.5625e-05\n",
      "Epoch 101/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2278.1079 - mse: 20957422.0000 - mae: 2278.4065\n",
      "Epoch 101: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2287.0276 - mse: 21047236.0000 - mae: 2287.3267 - val_loss: 2296.1692 - val_mse: 21525806.0000 - val_mae: 2296.4602 - lr: 1.5625e-05\n",
      "Epoch 102/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2290.7876 - mse: 21084258.0000 - mae: 2291.0854\n",
      "Epoch 102: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2290.7876 - mse: 21084258.0000 - mae: 2291.0854 - val_loss: 2291.2239 - val_mse: 21535108.0000 - val_mae: 2291.5161 - lr: 1.5625e-05\n",
      "Epoch 103/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2275.9832 - mse: 20927130.0000 - mae: 2276.2817\n",
      "Epoch 103: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2286.3269 - mse: 21078396.0000 - mae: 2286.6257 - val_loss: 2287.7168 - val_mse: 21523088.0000 - val_mae: 2288.0093 - lr: 1.5625e-05\n",
      "Epoch 104/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2275.4365 - mse: 20958080.0000 - mae: 2275.7354\n",
      "Epoch 104: val_loss did not improve from 2281.41895\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2282.5574 - mse: 21052932.0000 - mae: 2282.8567 - val_loss: 2289.4329 - val_mse: 21533474.0000 - val_mae: 2289.7246 - lr: 1.5625e-05\n",
      "Epoch 105/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2270.0605 - mse: 20879736.0000 - mae: 2270.3582\n",
      "Epoch 105: val_loss improved from 2281.41895 to 2274.46802, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2280.3923 - mse: 21026180.0000 - mae: 2280.6902 - val_loss: 2274.4680 - val_mse: 21349588.0000 - val_mae: 2274.7607 - lr: 1.5625e-05\n",
      "Epoch 106/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2279.4004 - mse: 20982180.0000 - mae: 2279.6992\n",
      "Epoch 106: val_loss improved from 2274.46802 to 2268.43652, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2280.0457 - mse: 20981452.0000 - mae: 2280.3447 - val_loss: 2268.4365 - val_mse: 21348464.0000 - val_mae: 2268.7263 - lr: 1.5625e-05\n",
      "Epoch 107/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2275.8545 - mse: 21111082.0000 - mae: 2276.1523\n",
      "Epoch 107: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2283.7900 - mse: 21183270.0000 - mae: 2284.0884 - val_loss: 2277.1455 - val_mse: 21491560.0000 - val_mae: 2277.4373 - lr: 1.5625e-05\n",
      "Epoch 108/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2278.1958 - mse: 21024564.0000 - mae: 2278.4934\n",
      "Epoch 108: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2284.4543 - mse: 21098556.0000 - mae: 2284.7522 - val_loss: 2286.5359 - val_mse: 21753294.0000 - val_mae: 2286.8252 - lr: 1.5625e-05\n",
      "Epoch 109/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2274.8660 - mse: 21002842.0000 - mae: 2275.1658\n",
      "Epoch 109: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2282.9851 - mse: 21098594.0000 - mae: 2283.2856 - val_loss: 2280.1721 - val_mse: 21384624.0000 - val_mae: 2280.4631 - lr: 1.5625e-05\n",
      "Epoch 110/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2277.6990 - mse: 21006786.0000 - mae: 2277.9973\n",
      "Epoch 110: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2280.8628 - mse: 21041646.0000 - mae: 2281.1614 - val_loss: 2276.9692 - val_mse: 21413266.0000 - val_mae: 2277.2617 - lr: 1.5625e-05\n",
      "Epoch 111/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2282.2715 - mse: 21087632.0000 - mae: 2282.5720\n",
      "Epoch 111: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2288.9829 - mse: 21167352.0000 - mae: 2289.2837 - val_loss: 2288.0208 - val_mse: 21546372.0000 - val_mae: 2288.3120 - lr: 1.5625e-05\n",
      "Epoch 112/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2276.4993 - mse: 21049172.0000 - mae: 2276.7993\n",
      "Epoch 112: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2279.4805 - mse: 21083830.0000 - mae: 2279.7805 - val_loss: 2277.2551 - val_mse: 21406326.0000 - val_mae: 2277.5442 - lr: 1.5625e-05\n",
      "Epoch 113/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2278.1172 - mse: 21044898.0000 - mae: 2278.4177\n",
      "Epoch 113: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2278.5049 - mse: 21043142.0000 - mae: 2278.8057 - val_loss: 2275.5085 - val_mse: 21399640.0000 - val_mae: 2275.7986 - lr: 1.5625e-05\n",
      "Epoch 114/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2267.0583 - mse: 20870584.0000 - mae: 2267.3582\n",
      "Epoch 114: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2276.0396 - mse: 20982702.0000 - mae: 2276.3398 - val_loss: 2279.9465 - val_mse: 21399316.0000 - val_mae: 2280.2358 - lr: 1.5625e-05\n",
      "Epoch 115/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2268.9614 - mse: 20997858.0000 - mae: 2269.2593\n",
      "Epoch 115: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2277.4307 - mse: 21079684.0000 - mae: 2277.7290 - val_loss: 2275.8240 - val_mse: 21484858.0000 - val_mae: 2276.1152 - lr: 1.5625e-05\n",
      "Epoch 116/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2269.2417 - mse: 20974540.0000 - mae: 2269.5417\n",
      "Epoch 116: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2278.1235 - mse: 21081242.0000 - mae: 2278.4241 - val_loss: 2279.9373 - val_mse: 21481182.0000 - val_mae: 2280.2288 - lr: 1.5625e-05\n",
      "Epoch 117/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2274.1616 - mse: 21025402.0000 - mae: 2274.4622\n",
      "Epoch 117: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2274.7327 - mse: 21020764.0000 - mae: 2275.0334 - val_loss: 2280.0762 - val_mse: 21523458.0000 - val_mae: 2280.3655 - lr: 7.8125e-06\n",
      "Epoch 118/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2263.3367 - mse: 20910540.0000 - mae: 2263.6367\n",
      "Epoch 118: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2271.6699 - mse: 20992020.0000 - mae: 2271.9705 - val_loss: 2277.2576 - val_mse: 21496612.0000 - val_mae: 2277.5452 - lr: 7.8125e-06\n",
      "Epoch 119/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2258.8760 - mse: 20847092.0000 - mae: 2259.1750\n",
      "Epoch 119: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2268.4299 - mse: 20972094.0000 - mae: 2268.7295 - val_loss: 2270.4343 - val_mse: 21367922.0000 - val_mae: 2270.7236 - lr: 7.8125e-06\n",
      "Epoch 120/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2261.9092 - mse: 20910182.0000 - mae: 2262.2078\n",
      "Epoch 120: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2268.1067 - mse: 20972724.0000 - mae: 2268.4055 - val_loss: 2275.5444 - val_mse: 21440598.0000 - val_mae: 2275.8333 - lr: 7.8125e-06\n",
      "Epoch 121/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2258.8232 - mse: 20844760.0000 - mae: 2259.1221\n",
      "Epoch 121: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2265.4736 - mse: 20922182.0000 - mae: 2265.7727 - val_loss: 2276.3516 - val_mse: 21421514.0000 - val_mae: 2276.6406 - lr: 7.8125e-06\n",
      "Epoch 122/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2259.4424 - mse: 20858420.0000 - mae: 2259.7419\n",
      "Epoch 122: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2266.1665 - mse: 20937152.0000 - mae: 2266.4666 - val_loss: 2272.3228 - val_mse: 21383316.0000 - val_mae: 2272.6130 - lr: 7.8125e-06\n",
      "Epoch 123/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2255.7458 - mse: 20842234.0000 - mae: 2256.0449\n",
      "Epoch 123: val_loss did not improve from 2268.43652\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2262.0544 - mse: 20906568.0000 - mae: 2262.3545 - val_loss: 2270.7114 - val_mse: 21359520.0000 - val_mae: 2271.0002 - lr: 7.8125e-06\n",
      "Epoch 124/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2254.3809 - mse: 20804052.0000 - mae: 2254.6790\n",
      "Epoch 124: val_loss improved from 2268.43652 to 2267.74341, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2263.4202 - mse: 20911104.0000 - mae: 2263.7190 - val_loss: 2267.7434 - val_mse: 21335114.0000 - val_mae: 2268.0327 - lr: 7.8125e-06\n",
      "Epoch 125/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2254.7151 - mse: 20823784.0000 - mae: 2255.0137\n",
      "Epoch 125: val_loss improved from 2267.74341 to 2264.12915, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2260.9695 - mse: 20888410.0000 - mae: 2261.2683 - val_loss: 2264.1292 - val_mse: 21300988.0000 - val_mae: 2264.4180 - lr: 7.8125e-06\n",
      "Epoch 126/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2257.7153 - mse: 20830600.0000 - mae: 2258.0144\n",
      "Epoch 126: val_loss did not improve from 2264.12915\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2260.7146 - mse: 20865108.0000 - mae: 2261.0139 - val_loss: 2264.8237 - val_mse: 21319858.0000 - val_mae: 2265.1121 - lr: 7.8125e-06\n",
      "Epoch 127/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2254.4641 - mse: 20792160.0000 - mae: 2254.7622\n",
      "Epoch 127: val_loss improved from 2264.12915 to 2262.75317, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2263.3376 - mse: 20899846.0000 - mae: 2263.6365 - val_loss: 2262.7532 - val_mse: 21259446.0000 - val_mae: 2263.0420 - lr: 7.8125e-06\n",
      "Epoch 128/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2254.1653 - mse: 20807566.0000 - mae: 2254.4619\n",
      "Epoch 128: val_loss did not improve from 2262.75317\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2263.1106 - mse: 20915244.0000 - mae: 2263.4077 - val_loss: 2265.5012 - val_mse: 21313028.0000 - val_mae: 2265.7883 - lr: 7.8125e-06\n",
      "Epoch 129/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2255.3857 - mse: 20831738.0000 - mae: 2255.6853\n",
      "Epoch 129: val_loss improved from 2262.75317 to 2259.13501, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2264.3694 - mse: 20940346.0000 - mae: 2264.6697 - val_loss: 2259.1350 - val_mse: 21234822.0000 - val_mae: 2259.4221 - lr: 7.8125e-06\n",
      "Epoch 130/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2261.1130 - mse: 20899140.0000 - mae: 2261.4128\n",
      "Epoch 130: val_loss improved from 2259.13501 to 2252.02637, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2267.7151 - mse: 20974850.0000 - mae: 2268.0154 - val_loss: 2252.0264 - val_mse: 21200214.0000 - val_mae: 2252.3140 - lr: 7.8125e-06\n",
      "Epoch 131/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2264.7603 - mse: 20969826.0000 - mae: 2265.0603\n",
      "Epoch 131: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2265.1643 - mse: 20965064.0000 - mae: 2265.4646 - val_loss: 2255.3774 - val_mse: 21220658.0000 - val_mae: 2255.6643 - lr: 7.8125e-06\n",
      "Epoch 132/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2263.7515 - mse: 20958286.0000 - mae: 2264.0503\n",
      "Epoch 132: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2264.1497 - mse: 20953250.0000 - mae: 2264.4485 - val_loss: 2261.3699 - val_mse: 21287766.0000 - val_mae: 2261.6580 - lr: 7.8125e-06\n",
      "Epoch 133/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2254.1379 - mse: 20815474.0000 - mae: 2254.4353\n",
      "Epoch 133: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2264.1548 - mse: 20952262.0000 - mae: 2264.4529 - val_loss: 2261.5259 - val_mse: 21208756.0000 - val_mae: 2261.8142 - lr: 7.8125e-06\n",
      "Epoch 134/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2260.8362 - mse: 20911572.0000 - mae: 2261.1348\n",
      "Epoch 134: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2261.2517 - mse: 20906876.0000 - mae: 2261.5503 - val_loss: 2265.2354 - val_mse: 21253398.0000 - val_mae: 2265.5229 - lr: 7.8125e-06\n",
      "Epoch 135/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2260.3721 - mse: 20915288.0000 - mae: 2260.6719\n",
      "Epoch 135: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2260.7981 - mse: 20910600.0000 - mae: 2261.0979 - val_loss: 2258.3618 - val_mse: 21188964.0000 - val_mae: 2258.6504 - lr: 7.8125e-06\n",
      "Epoch 136/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2262.1218 - mse: 20925800.0000 - mae: 2262.4194\n",
      "Epoch 136: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2262.1218 - mse: 20925800.0000 - mae: 2262.4194 - val_loss: 2263.1487 - val_mse: 21180496.0000 - val_mae: 2263.4363 - lr: 7.8125e-06\n",
      "Epoch 137/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2258.2102 - mse: 20892810.0000 - mae: 2258.5093\n",
      "Epoch 137: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2261.3071 - mse: 20927556.0000 - mae: 2261.6064 - val_loss: 2261.3765 - val_mse: 21170758.0000 - val_mae: 2261.6636 - lr: 7.8125e-06\n",
      "Epoch 138/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2253.5354 - mse: 20825948.0000 - mae: 2253.8350\n",
      "Epoch 138: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2262.4214 - mse: 20934570.0000 - mae: 2262.7214 - val_loss: 2261.9326 - val_mse: 21213136.0000 - val_mae: 2262.2197 - lr: 7.8125e-06\n",
      "Epoch 139/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2261.1521 - mse: 20936596.0000 - mae: 2261.4524\n",
      "Epoch 139: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2264.2263 - mse: 20971062.0000 - mae: 2264.5271 - val_loss: 2257.1680 - val_mse: 21139122.0000 - val_mae: 2257.4565 - lr: 7.8125e-06\n",
      "Epoch 140/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2250.3594 - mse: 20806732.0000 - mae: 2250.6589\n",
      "Epoch 140: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2259.0657 - mse: 20891858.0000 - mae: 2259.3660 - val_loss: 2252.6777 - val_mse: 21171540.0000 - val_mae: 2252.9670 - lr: 7.8125e-06\n",
      "Epoch 141/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2252.8560 - mse: 20840668.0000 - mae: 2253.1555\n",
      "Epoch 141: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2259.2683 - mse: 20907432.0000 - mae: 2259.5684 - val_loss: 2255.1904 - val_mse: 21162632.0000 - val_mae: 2255.4785 - lr: 3.9063e-06\n",
      "Epoch 142/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2253.8730 - mse: 20843136.0000 - mae: 2254.1721\n",
      "Epoch 142: val_loss did not improve from 2252.02637\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2257.0034 - mse: 20878338.0000 - mae: 2257.3027 - val_loss: 2252.8245 - val_mse: 21105246.0000 - val_mae: 2253.1113 - lr: 3.9063e-06\n",
      "Epoch 143/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2253.1489 - mse: 20833574.0000 - mae: 2253.4475\n",
      "Epoch 143: val_loss improved from 2252.02637 to 2251.78296, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2256.2947 - mse: 20869082.0000 - mae: 2256.5935 - val_loss: 2251.7830 - val_mse: 21082530.0000 - val_mae: 2252.0703 - lr: 3.9063e-06\n",
      "Epoch 144/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2247.0029 - mse: 20765856.0000 - mae: 2247.3008\n",
      "Epoch 144: val_loss improved from 2251.78296 to 2249.96069, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2253.4661 - mse: 20834254.0000 - mae: 2253.7639 - val_loss: 2249.9607 - val_mse: 21072650.0000 - val_mae: 2250.2483 - lr: 3.9063e-06\n",
      "Epoch 145/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2254.1157 - mse: 20853894.0000 - mae: 2254.4138\n",
      "Epoch 145: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2254.7070 - mse: 20848588.0000 - mae: 2255.0054 - val_loss: 2250.9592 - val_mse: 21071528.0000 - val_mae: 2251.2463 - lr: 3.9063e-06\n",
      "Epoch 146/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2251.1621 - mse: 20800266.0000 - mae: 2251.4595\n",
      "Epoch 146: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2254.2993 - mse: 20835708.0000 - mae: 2254.5969 - val_loss: 2250.0962 - val_mse: 21066360.0000 - val_mae: 2250.3848 - lr: 3.9063e-06\n",
      "Epoch 147/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2252.2100 - mse: 20833038.0000 - mae: 2252.5078\n",
      "Epoch 147: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2252.8079 - mse: 20827834.0000 - mae: 2253.1057 - val_loss: 2252.2471 - val_mse: 21072662.0000 - val_mae: 2252.5332 - lr: 3.9063e-06\n",
      "Epoch 148/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2253.3611 - mse: 20829732.0000 - mae: 2253.6587\n",
      "Epoch 148: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2253.9514 - mse: 20824494.0000 - mae: 2254.2493 - val_loss: 2252.1738 - val_mse: 21067218.0000 - val_mae: 2252.4609 - lr: 3.9063e-06\n",
      "Epoch 149/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2252.5464 - mse: 20821416.0000 - mae: 2252.8464\n",
      "Epoch 149: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2253.0530 - mse: 20818754.0000 - mae: 2253.3530 - val_loss: 2250.6641 - val_mse: 21070036.0000 - val_mae: 2250.9509 - lr: 3.9063e-06\n",
      "Epoch 150/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2244.3831 - mse: 20709698.0000 - mae: 2244.6819\n",
      "Epoch 150: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2253.5222 - mse: 20821686.0000 - mae: 2253.8218 - val_loss: 2251.0671 - val_mse: 21051102.0000 - val_mae: 2251.3547 - lr: 3.9063e-06\n",
      "Epoch 151/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2251.4395 - mse: 20816108.0000 - mae: 2251.7378\n",
      "Epoch 151: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2252.0393 - mse: 20810956.0000 - mae: 2252.3376 - val_loss: 2252.5242 - val_mse: 21094792.0000 - val_mae: 2252.8123 - lr: 3.9063e-06\n",
      "Epoch 152/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2246.8535 - mse: 20745926.0000 - mae: 2247.1528\n",
      "Epoch 152: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2250.1086 - mse: 20783346.0000 - mae: 2250.4082 - val_loss: 2251.6526 - val_mse: 21095716.0000 - val_mae: 2251.9399 - lr: 3.9063e-06\n",
      "Epoch 153/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2248.3313 - mse: 20760944.0000 - mae: 2248.6311\n",
      "Epoch 153: val_loss did not improve from 2249.96069\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2251.5815 - mse: 20798290.0000 - mae: 2251.8816 - val_loss: 2249.9980 - val_mse: 21103660.0000 - val_mae: 2250.2854 - lr: 3.9063e-06\n",
      "Epoch 154/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2250.1096 - mse: 20777664.0000 - mae: 2250.4111\n",
      "Epoch 154: val_loss improved from 2249.96069 to 2249.83545, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2250.1096 - mse: 20777664.0000 - mae: 2250.4111 - val_loss: 2249.8354 - val_mse: 21076670.0000 - val_mae: 2250.1235 - lr: 3.9063e-06\n",
      "Epoch 155/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2250.4116 - mse: 20772516.0000 - mae: 2250.7095\n",
      "Epoch 155: val_loss did not improve from 2249.83545\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2251.0222 - mse: 20767422.0000 - mae: 2251.3201 - val_loss: 2252.5117 - val_mse: 21103634.0000 - val_mae: 2252.7988 - lr: 3.9063e-06\n",
      "Epoch 156/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2240.0823 - mse: 20647508.0000 - mae: 2240.3823\n",
      "Epoch 156: val_loss did not improve from 2249.83545\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2249.2366 - mse: 20759904.0000 - mae: 2249.5374 - val_loss: 2252.3552 - val_mse: 21100268.0000 - val_mae: 2252.6423 - lr: 3.9063e-06\n",
      "Epoch 157/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2249.9968 - mse: 20753326.0000 - mae: 2250.2966\n",
      "Epoch 157: val_loss did not improve from 2249.83545\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2249.9968 - mse: 20753326.0000 - mae: 2250.2966 - val_loss: 2250.3562 - val_mse: 21079846.0000 - val_mae: 2250.6423 - lr: 3.9063e-06\n",
      "Epoch 158/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2243.2041 - mse: 20685670.0000 - mae: 2243.5027\n",
      "Epoch 158: val_loss improved from 2249.83545 to 2249.04761, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2249.7622 - mse: 20755862.0000 - mae: 2250.0610 - val_loss: 2249.0476 - val_mse: 21067794.0000 - val_mae: 2249.3350 - lr: 3.9063e-06\n",
      "Epoch 159/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2248.6541 - mse: 20759170.0000 - mae: 2248.9509\n",
      "Epoch 159: val_loss did not improve from 2249.04761\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2249.2729 - mse: 20754098.0000 - mae: 2249.5701 - val_loss: 2249.7471 - val_mse: 21068594.0000 - val_mae: 2250.0347 - lr: 3.9063e-06\n",
      "Epoch 160/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2242.3960 - mse: 20669326.0000 - mae: 2242.6953\n",
      "Epoch 160: val_loss improved from 2249.04761 to 2247.97607, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2248.9893 - mse: 20739678.0000 - mae: 2249.2888 - val_loss: 2247.9761 - val_mse: 21053658.0000 - val_mae: 2248.2637 - lr: 3.9063e-06\n",
      "Epoch 161/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2240.8684 - mse: 20684036.0000 - mae: 2241.1680\n",
      "Epoch 161: val_loss did not improve from 2247.97607\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2249.6360 - mse: 20771832.0000 - mae: 2249.9360 - val_loss: 2251.3394 - val_mse: 21076510.0000 - val_mae: 2251.6267 - lr: 3.9063e-06\n",
      "Epoch 162/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2248.9036 - mse: 20783388.0000 - mae: 2249.2046\n",
      "Epoch 162: val_loss did not improve from 2247.97607\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2249.5217 - mse: 20778278.0000 - mae: 2249.8230 - val_loss: 2248.4563 - val_mse: 21063838.0000 - val_mae: 2248.7427 - lr: 3.9063e-06\n",
      "Epoch 163/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2242.0425 - mse: 20693494.0000 - mae: 2242.3391\n",
      "Epoch 163: val_loss did not improve from 2247.97607\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2248.6191 - mse: 20763318.0000 - mae: 2248.9160 - val_loss: 2249.0425 - val_mse: 21052164.0000 - val_mae: 2249.3291 - lr: 3.9063e-06\n",
      "Epoch 164/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2239.6436 - mse: 20656820.0000 - mae: 2239.9426\n",
      "Epoch 164: val_loss improved from 2247.97607 to 2247.03687, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2248.7732 - mse: 20768666.0000 - mae: 2249.0730 - val_loss: 2247.0369 - val_mse: 21032336.0000 - val_mae: 2247.3235 - lr: 3.9063e-06\n",
      "Epoch 165/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2248.4814 - mse: 20766956.0000 - mae: 2248.7810\n",
      "Epoch 165: val_loss improved from 2247.03687 to 2246.09546, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2248.4814 - mse: 20766956.0000 - mae: 2248.7810 - val_loss: 2246.0955 - val_mse: 21034424.0000 - val_mae: 2246.3831 - lr: 3.9063e-06\n",
      "Epoch 166/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2242.4819 - mse: 20705732.0000 - mae: 2242.7791\n",
      "Epoch 166: val_loss did not improve from 2246.09546\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2249.1206 - mse: 20775354.0000 - mae: 2249.4185 - val_loss: 2246.9187 - val_mse: 21041950.0000 - val_mae: 2247.2056 - lr: 3.9063e-06\n",
      "Epoch 167/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2248.3748 - mse: 20778846.0000 - mae: 2248.6738\n",
      "Epoch 167: val_loss did not improve from 2246.09546\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2248.9985 - mse: 20773804.0000 - mae: 2249.2979 - val_loss: 2248.2417 - val_mse: 21047444.0000 - val_mae: 2248.5291 - lr: 3.9063e-06\n",
      "Epoch 168/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2239.1409 - mse: 20673806.0000 - mae: 2239.4399\n",
      "Epoch 168: val_loss improved from 2246.09546 to 2245.74707, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2247.9541 - mse: 20761868.0000 - mae: 2248.2537 - val_loss: 2245.7471 - val_mse: 21055230.0000 - val_mae: 2246.0344 - lr: 3.9063e-06\n",
      "Epoch 169/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2237.6855 - mse: 20642716.0000 - mae: 2237.9832\n",
      "Epoch 169: val_loss improved from 2245.74707 to 2245.72168, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2246.5554 - mse: 20731306.0000 - mae: 2246.8535 - val_loss: 2245.7217 - val_mse: 21047778.0000 - val_mae: 2246.0083 - lr: 3.9063e-06\n",
      "Epoch 170/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2236.7112 - mse: 20615506.0000 - mae: 2237.0100\n",
      "Epoch 170: val_loss did not improve from 2245.72168\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2245.9834 - mse: 20728304.0000 - mae: 2246.2827 - val_loss: 2247.5771 - val_mse: 21034918.0000 - val_mae: 2247.8638 - lr: 3.9063e-06\n",
      "Epoch 171/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2243.1643 - mse: 20688330.0000 - mae: 2243.4622\n",
      "Epoch 171: val_loss improved from 2245.72168 to 2243.23242, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2246.4485 - mse: 20725626.0000 - mae: 2246.7463 - val_loss: 2243.2324 - val_mse: 21011696.0000 - val_mae: 2243.5193 - lr: 3.9063e-06\n",
      "Epoch 172/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2236.1787 - mse: 20592440.0000 - mae: 2236.4766\n",
      "Epoch 172: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2246.3369 - mse: 20732378.0000 - mae: 2246.6355 - val_loss: 2245.3530 - val_mse: 21033632.0000 - val_mae: 2245.6399 - lr: 3.9063e-06\n",
      "Epoch 173/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2245.3208 - mse: 20731650.0000 - mae: 2245.6184\n",
      "Epoch 173: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2245.8965 - mse: 20729630.0000 - mae: 2246.1943 - val_loss: 2245.2285 - val_mse: 21010478.0000 - val_mae: 2245.5151 - lr: 3.9063e-06\n",
      "Epoch 174/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2239.7703 - mse: 20656030.0000 - mae: 2240.0691\n",
      "Epoch 174: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2246.6794 - mse: 20738720.0000 - mae: 2246.9788 - val_loss: 2245.2715 - val_mse: 21043510.0000 - val_mae: 2245.5581 - lr: 3.9063e-06\n",
      "Epoch 175/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2234.8618 - mse: 20587420.0000 - mae: 2235.1587\n",
      "Epoch 175: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2245.1270 - mse: 20727592.0000 - mae: 2245.4241 - val_loss: 2248.7280 - val_mse: 21049672.0000 - val_mae: 2249.0144 - lr: 3.9063e-06\n",
      "Epoch 176/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2244.9421 - mse: 20730040.0000 - mae: 2245.2415\n",
      "Epoch 176: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2245.5176 - mse: 20728006.0000 - mae: 2245.8171 - val_loss: 2245.5828 - val_mse: 21018854.0000 - val_mae: 2245.8691 - lr: 3.9063e-06\n",
      "Epoch 177/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2245.8833 - mse: 20746092.0000 - mae: 2246.1804\n",
      "Epoch 177: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2246.5059 - mse: 20741030.0000 - mae: 2246.8032 - val_loss: 2246.0190 - val_mse: 21040050.0000 - val_mae: 2246.3066 - lr: 3.9063e-06\n",
      "Epoch 178/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2245.5034 - mse: 20726520.0000 - mae: 2245.8025\n",
      "Epoch 178: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2245.5034 - mse: 20726520.0000 - mae: 2245.8025 - val_loss: 2247.7917 - val_mse: 21058980.0000 - val_mae: 2248.0793 - lr: 3.9063e-06\n",
      "Epoch 179/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2242.6018 - mse: 20689542.0000 - mae: 2242.8984\n",
      "Epoch 179: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2245.9131 - mse: 20727042.0000 - mae: 2246.2100 - val_loss: 2245.3401 - val_mse: 21031736.0000 - val_mae: 2245.6277 - lr: 3.9063e-06\n",
      "Epoch 180/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2246.0811 - mse: 20743302.0000 - mae: 2246.3796\n",
      "Epoch 180: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2246.6758 - mse: 20741386.0000 - mae: 2246.9746 - val_loss: 2246.0295 - val_mse: 21038994.0000 - val_mae: 2246.3171 - lr: 3.9063e-06\n",
      "Epoch 181/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2245.1943 - mse: 20730516.0000 - mae: 2245.4937\n",
      "Epoch 181: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2245.7800 - mse: 20728552.0000 - mae: 2246.0796 - val_loss: 2246.8401 - val_mse: 21034304.0000 - val_mae: 2247.1267 - lr: 3.9063e-06\n",
      "Epoch 182/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2235.3914 - mse: 20599886.0000 - mae: 2235.6909\n",
      "Epoch 182: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2244.7063 - mse: 20713300.0000 - mae: 2245.0063 - val_loss: 2246.4565 - val_mse: 21045644.0000 - val_mae: 2246.7439 - lr: 1.9531e-06\n",
      "Epoch 183/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2236.4666 - mse: 20618704.0000 - mae: 2236.7637\n",
      "Epoch 183: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2243.4233 - mse: 20701742.0000 - mae: 2243.7209 - val_loss: 2245.5752 - val_mse: 21042288.0000 - val_mae: 2245.8621 - lr: 1.9531e-06\n",
      "Epoch 184/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2242.3748 - mse: 20695600.0000 - mae: 2242.6724\n",
      "Epoch 184: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.9683 - mse: 20693844.0000 - mae: 2243.2659 - val_loss: 2245.1323 - val_mse: 21044156.0000 - val_mae: 2245.4180 - lr: 1.9531e-06\n",
      "Epoch 185/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2235.7083 - mse: 20604022.0000 - mae: 2236.0063\n",
      "Epoch 185: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.6687 - mse: 20687330.0000 - mae: 2242.9673 - val_loss: 2246.2566 - val_mse: 21050852.0000 - val_mae: 2246.5437 - lr: 1.9531e-06\n",
      "Epoch 186/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2239.9277 - mse: 20653422.0000 - mae: 2240.2271\n",
      "Epoch 186: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2243.2329 - mse: 20691114.0000 - mae: 2243.5325 - val_loss: 2246.3801 - val_mse: 21047394.0000 - val_mae: 2246.6665 - lr: 1.9531e-06\n",
      "Epoch 187/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2234.0935 - mse: 20604358.0000 - mae: 2234.3911\n",
      "Epoch 187: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.9438 - mse: 20693238.0000 - mae: 2243.2419 - val_loss: 2245.6709 - val_mse: 21031834.0000 - val_mae: 2245.9568 - lr: 1.9531e-06\n",
      "Epoch 188/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2241.9709 - mse: 20688818.0000 - mae: 2242.2693\n",
      "Epoch 188: val_loss did not improve from 2243.23242\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.5710 - mse: 20687164.0000 - mae: 2242.8694 - val_loss: 2245.7173 - val_mse: 21047784.0000 - val_mae: 2246.0037 - lr: 1.9531e-06\n",
      "Epoch 189/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2234.0762 - mse: 20575836.0000 - mae: 2234.3748\n",
      "Epoch 189: val_loss improved from 2243.23242 to 2242.91333, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2243.3474 - mse: 20689498.0000 - mae: 2243.6467 - val_loss: 2242.9133 - val_mse: 21022462.0000 - val_mae: 2243.1997 - lr: 1.9531e-06\n",
      "Epoch 190/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2242.3828 - mse: 20688228.0000 - mae: 2242.6809\n",
      "Epoch 190: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2243.0076 - mse: 20683260.0000 - mae: 2243.3059 - val_loss: 2244.3225 - val_mse: 21026448.0000 - val_mae: 2244.6079 - lr: 1.9531e-06\n",
      "Epoch 191/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2241.8826 - mse: 20682874.0000 - mae: 2242.1804\n",
      "Epoch 191: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.4846 - mse: 20681232.0000 - mae: 2242.7827 - val_loss: 2244.7170 - val_mse: 21030386.0000 - val_mae: 2245.0037 - lr: 1.9531e-06\n",
      "Epoch 192/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2239.3486 - mse: 20640974.0000 - mae: 2239.6472\n",
      "Epoch 192: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.6582 - mse: 20678774.0000 - mae: 2242.9570 - val_loss: 2245.0703 - val_mse: 21032282.0000 - val_mae: 2245.3564 - lr: 1.9531e-06\n",
      "Epoch 193/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2235.7014 - mse: 20594688.0000 - mae: 2235.9995\n",
      "Epoch 193: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.6624 - mse: 20678052.0000 - mae: 2242.9607 - val_loss: 2245.0737 - val_mse: 21028940.0000 - val_mae: 2245.3601 - lr: 1.9531e-06\n",
      "Epoch 194/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2233.8809 - mse: 20590538.0000 - mae: 2234.1790\n",
      "Epoch 194: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.7136 - mse: 20679498.0000 - mae: 2243.0122 - val_loss: 2245.1082 - val_mse: 21033786.0000 - val_mae: 2245.3943 - lr: 1.9531e-06\n",
      "Epoch 195/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2233.4382 - mse: 20563314.0000 - mae: 2233.7368\n",
      "Epoch 195: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.7246 - mse: 20677028.0000 - mae: 2243.0237 - val_loss: 2245.6035 - val_mse: 21041088.0000 - val_mae: 2245.8896 - lr: 1.9531e-06\n",
      "Epoch 196/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2239.0227 - mse: 20637632.0000 - mae: 2239.3225\n",
      "Epoch 196: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.3350 - mse: 20675442.0000 - mae: 2242.6353 - val_loss: 2244.5579 - val_mse: 21026480.0000 - val_mae: 2244.8440 - lr: 1.9531e-06\n",
      "Epoch 197/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2241.5232 - mse: 20676072.0000 - mae: 2241.8210\n",
      "Epoch 197: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.1509 - mse: 20671146.0000 - mae: 2242.4490 - val_loss: 2244.5906 - val_mse: 21025366.0000 - val_mae: 2244.8777 - lr: 1.9531e-06\n",
      "Epoch 198/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2235.3792 - mse: 20596638.0000 - mae: 2235.6777\n",
      "Epoch 198: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.0161 - mse: 20667790.0000 - mae: 2242.3149 - val_loss: 2244.9314 - val_mse: 21031364.0000 - val_mae: 2245.2173 - lr: 1.9531e-06\n",
      "Epoch 199/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2232.8074 - mse: 20555430.0000 - mae: 2233.1072\n",
      "Epoch 199: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2242.0845 - mse: 20669256.0000 - mae: 2242.3848 - val_loss: 2244.9614 - val_mse: 21031402.0000 - val_mae: 2245.2483 - lr: 1.9531e-06\n",
      "Epoch 200/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2241.1946 - mse: 20648460.0000 - mae: 2241.4932\n",
      "Epoch 200: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2241.7952 - mse: 20646940.0000 - mae: 2242.0938 - val_loss: 2244.2107 - val_mse: 21036826.0000 - val_mae: 2244.4961 - lr: 9.7656e-07\n",
      "Epoch 201/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2240.8943 - mse: 20649770.0000 - mae: 2241.1924\n",
      "Epoch 201: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2241.4963 - mse: 20648280.0000 - mae: 2241.7947 - val_loss: 2243.4607 - val_mse: 21033010.0000 - val_mae: 2243.7466 - lr: 9.7656e-07\n",
      "Epoch 202/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2232.5166 - mse: 20554810.0000 - mae: 2232.8137\n",
      "Epoch 202: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2241.3206 - mse: 20643864.0000 - mae: 2241.6182 - val_loss: 2243.3176 - val_mse: 21029092.0000 - val_mae: 2243.6033 - lr: 9.7656e-07\n",
      "Epoch 203/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2237.8130 - mse: 20603986.0000 - mae: 2238.1108\n",
      "Epoch 203: val_loss did not improve from 2242.91333\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2241.1182 - mse: 20641970.0000 - mae: 2241.4163 - val_loss: 2243.0466 - val_mse: 21026788.0000 - val_mae: 2243.3320 - lr: 9.7656e-07\n",
      "Epoch 204/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2232.2498 - mse: 20551656.0000 - mae: 2232.5471\n",
      "Epoch 204: val_loss improved from 2242.91333 to 2242.44019, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2241.0645 - mse: 20640746.0000 - mae: 2241.3621 - val_loss: 2242.4402 - val_mse: 21022438.0000 - val_mae: 2242.7251 - lr: 9.7656e-07\n",
      "Epoch 205/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2241.1719 - mse: 20640188.0000 - mae: 2241.4709\n",
      "Epoch 205: val_loss improved from 2242.44019 to 2242.43750, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2241.1719 - mse: 20640188.0000 - mae: 2241.4709 - val_loss: 2242.4375 - val_mse: 21019472.0000 - val_mae: 2242.7236 - lr: 9.7656e-07\n",
      "Epoch 206/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2233.9912 - mse: 20553200.0000 - mae: 2234.2908\n",
      "Epoch 206: val_loss improved from 2242.43750 to 2242.19263, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.9531 - mse: 20637040.0000 - mae: 2241.2527 - val_loss: 2242.1926 - val_mse: 21017374.0000 - val_mae: 2242.4785 - lr: 9.7656e-07\n",
      "Epoch 207/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2240.1541 - mse: 20636940.0000 - mae: 2240.4524\n",
      "Epoch 207: val_loss improved from 2242.19263 to 2241.69043, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.7820 - mse: 20632138.0000 - mae: 2241.0806 - val_loss: 2241.6904 - val_mse: 21012010.0000 - val_mae: 2241.9761 - lr: 9.7656e-07\n",
      "Epoch 208/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2233.8833 - mse: 20551396.0000 - mae: 2234.1819\n",
      "Epoch 208: val_loss did not improve from 2241.69043\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.8438 - mse: 20635256.0000 - mae: 2241.1431 - val_loss: 2241.9189 - val_mse: 21013600.0000 - val_mae: 2242.2051 - lr: 9.7656e-07\n",
      "Epoch 209/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2233.6938 - mse: 20544488.0000 - mae: 2233.9932\n",
      "Epoch 209: val_loss improved from 2241.69043 to 2241.56934, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.6501 - mse: 20628454.0000 - mae: 2240.9497 - val_loss: 2241.5693 - val_mse: 21009094.0000 - val_mae: 2241.8547 - lr: 9.7656e-07\n",
      "Epoch 210/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2231.9431 - mse: 20543736.0000 - mae: 2232.2415\n",
      "Epoch 210: val_loss improved from 2241.56934 to 2241.44580, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.7517 - mse: 20632938.0000 - mae: 2241.0503 - val_loss: 2241.4458 - val_mse: 21007814.0000 - val_mae: 2241.7312 - lr: 9.7656e-07\n",
      "Epoch 211/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2233.9221 - mse: 20556296.0000 - mae: 2234.2209\n",
      "Epoch 211: val_loss did not improve from 2241.44580\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.5447 - mse: 20627720.0000 - mae: 2240.8438 - val_loss: 2241.5425 - val_mse: 21008562.0000 - val_mae: 2241.8284 - lr: 9.7656e-07\n",
      "Epoch 212/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2239.9094 - mse: 20625904.0000 - mae: 2240.2092\n",
      "Epoch 212: val_loss improved from 2241.44580 to 2241.23169, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2240.5176 - mse: 20624624.0000 - mae: 2240.8174 - val_loss: 2241.2317 - val_mse: 21005712.0000 - val_mae: 2241.5178 - lr: 9.7656e-07\n",
      "Epoch 213/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2240.3210 - mse: 20624370.0000 - mae: 2240.6189\n",
      "Epoch 213: val_loss improved from 2241.23169 to 2241.20801, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.3210 - mse: 20624370.0000 - mae: 2240.6189 - val_loss: 2241.2080 - val_mse: 21002928.0000 - val_mae: 2241.4937 - lr: 9.7656e-07\n",
      "Epoch 214/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2240.0566 - mse: 20617838.0000 - mae: 2240.3562\n",
      "Epoch 214: val_loss improved from 2241.20801 to 2240.96631, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.0566 - mse: 20617838.0000 - mae: 2240.3562 - val_loss: 2240.9663 - val_mse: 21000766.0000 - val_mae: 2241.2522 - lr: 9.7656e-07\n",
      "Epoch 215/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2236.7900 - mse: 20581196.0000 - mae: 2237.0881\n",
      "Epoch 215: val_loss improved from 2240.96631 to 2240.47998, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2240.1023 - mse: 20619404.0000 - mae: 2240.4006 - val_loss: 2240.4800 - val_mse: 20996600.0000 - val_mae: 2240.7656 - lr: 9.7656e-07\n",
      "Epoch 216/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2239.5615 - mse: 20624446.0000 - mae: 2239.8594\n",
      "Epoch 216: val_loss improved from 2240.47998 to 2240.25049, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2240.1931 - mse: 20619720.0000 - mae: 2240.4912 - val_loss: 2240.2505 - val_mse: 20993392.0000 - val_mae: 2240.5361 - lr: 9.7656e-07\n",
      "Epoch 217/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2230.8850 - mse: 20505142.0000 - mae: 2231.1831\n",
      "Epoch 217: val_loss improved from 2240.25049 to 2240.10303, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.1499 - mse: 20619354.0000 - mae: 2240.4487 - val_loss: 2240.1030 - val_mse: 20992018.0000 - val_mae: 2240.3882 - lr: 9.7656e-07\n",
      "Epoch 218/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2231.2983 - mse: 20527808.0000 - mae: 2231.5964\n",
      "Epoch 218: val_loss improved from 2240.10303 to 2239.88818, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2240.1057 - mse: 20617202.0000 - mae: 2240.4043 - val_loss: 2239.8882 - val_mse: 20988580.0000 - val_mae: 2240.1748 - lr: 9.7656e-07\n",
      "Epoch 219/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2230.5383 - mse: 20497894.0000 - mae: 2230.8372\n",
      "Epoch 219: val_loss improved from 2239.88818 to 2239.58960, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2239.8032 - mse: 20612196.0000 - mae: 2240.1028 - val_loss: 2239.5896 - val_mse: 20987556.0000 - val_mae: 2239.8748 - lr: 9.7656e-07\n",
      "Epoch 220/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2236.6777 - mse: 20577094.0000 - mae: 2236.9749\n",
      "Epoch 220: val_loss improved from 2239.58960 to 2239.28833, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.9907 - mse: 20615334.0000 - mae: 2240.2886 - val_loss: 2239.2883 - val_mse: 20983530.0000 - val_mae: 2239.5742 - lr: 9.7656e-07\n",
      "Epoch 221/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2231.0354 - mse: 20524498.0000 - mae: 2231.3328\n",
      "Epoch 221: val_loss did not improve from 2239.28833\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.8447 - mse: 20613776.0000 - mae: 2240.1426 - val_loss: 2239.4722 - val_mse: 20985618.0000 - val_mae: 2239.7583 - lr: 9.7656e-07\n",
      "Epoch 222/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2230.9009 - mse: 20520248.0000 - mae: 2231.1985\n",
      "Epoch 222: val_loss improved from 2239.28833 to 2238.88989, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2239.7104 - mse: 20609712.0000 - mae: 2240.0085 - val_loss: 2238.8899 - val_mse: 20980230.0000 - val_mae: 2239.1753 - lr: 9.7656e-07\n",
      "Epoch 223/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2239.0920 - mse: 20612986.0000 - mae: 2239.3909\n",
      "Epoch 223: val_loss did not improve from 2238.88989\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.7053 - mse: 20611802.0000 - mae: 2240.0044 - val_loss: 2238.9575 - val_mse: 20980674.0000 - val_mae: 2239.2437 - lr: 9.7656e-07\n",
      "Epoch 224/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2229.3794 - mse: 20466632.0000 - mae: 2229.6775\n",
      "Epoch 224: val_loss did not improve from 2238.88989\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.5793 - mse: 20608664.0000 - mae: 2239.8782 - val_loss: 2239.0129 - val_mse: 20981486.0000 - val_mae: 2239.2986 - lr: 9.7656e-07\n",
      "Epoch 225/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2232.4292 - mse: 20521596.0000 - mae: 2232.7292\n",
      "Epoch 225: val_loss improved from 2238.88989 to 2238.43140, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.4045 - mse: 20605744.0000 - mae: 2239.7048 - val_loss: 2238.4314 - val_mse: 20977502.0000 - val_mae: 2238.7163 - lr: 9.7656e-07\n",
      "Epoch 226/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2239.3271 - mse: 20606324.0000 - mae: 2239.6257\n",
      "Epoch 226: val_loss did not improve from 2238.43140\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.3271 - mse: 20606324.0000 - mae: 2239.6257 - val_loss: 2238.7817 - val_mse: 20980588.0000 - val_mae: 2239.0679 - lr: 9.7656e-07\n",
      "Epoch 227/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2230.3523 - mse: 20514402.0000 - mae: 2230.6511\n",
      "Epoch 227: val_loss improved from 2238.43140 to 2238.17163, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.1709 - mse: 20603878.0000 - mae: 2239.4700 - val_loss: 2238.1716 - val_mse: 20976094.0000 - val_mae: 2238.4578 - lr: 9.7656e-07\n",
      "Epoch 228/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2230.4387 - mse: 20514150.0000 - mae: 2230.7375\n",
      "Epoch 228: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.2458 - mse: 20603530.0000 - mae: 2239.5452 - val_loss: 2238.2649 - val_mse: 20976700.0000 - val_mae: 2238.5503 - lr: 9.7656e-07\n",
      "Epoch 229/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2229.8442 - mse: 20488612.0000 - mae: 2230.1426\n",
      "Epoch 229: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.1228 - mse: 20603158.0000 - mae: 2239.4216 - val_loss: 2238.3665 - val_mse: 20976066.0000 - val_mae: 2238.6511 - lr: 9.7656e-07\n",
      "Epoch 230/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2231.9956 - mse: 20514794.0000 - mae: 2232.2942\n",
      "Epoch 230: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2238.9663 - mse: 20598994.0000 - mae: 2239.2654 - val_loss: 2238.3599 - val_mse: 20975314.0000 - val_mae: 2238.6455 - lr: 9.7656e-07\n",
      "Epoch 231/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2235.7698 - mse: 20564282.0000 - mae: 2236.0693\n",
      "Epoch 231: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.0884 - mse: 20602544.0000 - mae: 2239.3882 - val_loss: 2238.8025 - val_mse: 20979010.0000 - val_mae: 2239.0876 - lr: 9.7656e-07\n",
      "Epoch 232/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2229.7810 - mse: 20484772.0000 - mae: 2230.0806\n",
      "Epoch 232: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2239.0515 - mse: 20599326.0000 - mae: 2239.3518 - val_loss: 2238.2351 - val_mse: 20974540.0000 - val_mae: 2238.5205 - lr: 9.7656e-07\n",
      "Epoch 233/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2238.2964 - mse: 20599882.0000 - mae: 2238.5950\n",
      "Epoch 233: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2238.9150 - mse: 20598772.0000 - mae: 2239.2136 - val_loss: 2238.4797 - val_mse: 20976366.0000 - val_mae: 2238.7649 - lr: 9.7656e-07\n",
      "Epoch 234/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2238.7473 - mse: 20596756.0000 - mae: 2239.0464\n",
      "Epoch 234: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2238.7473 - mse: 20596756.0000 - mae: 2239.0464 - val_loss: 2238.6355 - val_mse: 20976140.0000 - val_mae: 2238.9211 - lr: 9.7656e-07\n",
      "Epoch 235/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2231.8486 - mse: 20512810.0000 - mae: 2232.1475\n",
      "Epoch 235: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2238.8210 - mse: 20596938.0000 - mae: 2239.1206 - val_loss: 2238.2915 - val_mse: 20975712.0000 - val_mae: 2238.5771 - lr: 9.7656e-07\n",
      "Epoch 236/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2235.1880 - mse: 20555348.0000 - mae: 2235.4871\n",
      "Epoch 236: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2238.5081 - mse: 20593620.0000 - mae: 2238.8074 - val_loss: 2238.7058 - val_mse: 20975860.0000 - val_mae: 2238.9915 - lr: 9.7656e-07\n",
      "Epoch 237/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2237.5952 - mse: 20591800.0000 - mae: 2237.8945\n",
      "Epoch 237: val_loss did not improve from 2238.17163\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2238.2166 - mse: 20590726.0000 - mae: 2238.5156 - val_loss: 2238.5408 - val_mse: 20974668.0000 - val_mae: 2238.8269 - lr: 9.7656e-07\n",
      "Epoch 238/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2227.5913 - mse: 20450898.0000 - mae: 2227.8901\n",
      "Epoch 238: val_loss improved from 2238.17163 to 2238.11353, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.8828 - mse: 20565798.0000 - mae: 2237.1824 - val_loss: 2238.1135 - val_mse: 20973614.0000 - val_mae: 2238.3994 - lr: 4.8828e-07\n",
      "Epoch 239/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2236.1514 - mse: 20567334.0000 - mae: 2236.4507\n",
      "Epoch 239: val_loss improved from 2238.11353 to 2237.89062, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.7742 - mse: 20566350.0000 - mae: 2237.0737 - val_loss: 2237.8906 - val_mse: 20973694.0000 - val_mae: 2238.1763 - lr: 4.8828e-07\n",
      "Epoch 240/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2236.7866 - mse: 20567186.0000 - mae: 2237.0869\n",
      "Epoch 240: val_loss improved from 2237.89062 to 2237.74438, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2236.7866 - mse: 20567186.0000 - mae: 2237.0869 - val_loss: 2237.7444 - val_mse: 20972232.0000 - val_mae: 2238.0308 - lr: 4.8828e-07\n",
      "Epoch 241/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2236.7761 - mse: 20567282.0000 - mae: 2237.0757\n",
      "Epoch 241: val_loss improved from 2237.74438 to 2237.58984, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2236.7761 - mse: 20567282.0000 - mae: 2237.0757 - val_loss: 2237.5898 - val_mse: 20971236.0000 - val_mae: 2237.8752 - lr: 4.8828e-07\n",
      "Epoch 242/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2227.5098 - mse: 20452652.0000 - mae: 2227.8091\n",
      "Epoch 242: val_loss improved from 2237.58984 to 2237.48901, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.7937 - mse: 20567430.0000 - mae: 2237.0935 - val_loss: 2237.4890 - val_mse: 20969502.0000 - val_mae: 2237.7747 - lr: 4.8828e-07\n",
      "Epoch 243/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2227.3816 - mse: 20451008.0000 - mae: 2227.6792\n",
      "Epoch 243: val_loss did not improve from 2237.48901\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.6697 - mse: 20565772.0000 - mae: 2236.9680 - val_loss: 2237.5259 - val_mse: 20969614.0000 - val_mae: 2237.8115 - lr: 4.8828e-07\n",
      "Epoch 244/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2236.6545 - mse: 20564832.0000 - mae: 2236.9543\n",
      "Epoch 244: val_loss improved from 2237.48901 to 2237.48267, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.6545 - mse: 20564832.0000 - mae: 2236.9543 - val_loss: 2237.4827 - val_mse: 20968478.0000 - val_mae: 2237.7683 - lr: 4.8828e-07\n",
      "Epoch 245/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2235.9397 - mse: 20564056.0000 - mae: 2236.2390\n",
      "Epoch 245: val_loss did not improve from 2237.48267\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.5649 - mse: 20563120.0000 - mae: 2236.8643 - val_loss: 2237.5144 - val_mse: 20968078.0000 - val_mae: 2237.8003 - lr: 4.8828e-07\n",
      "Epoch 246/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2227.6931 - mse: 20472828.0000 - mae: 2227.9907\n",
      "Epoch 246: val_loss did not improve from 2237.48267\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.5046 - mse: 20562338.0000 - mae: 2236.8027 - val_loss: 2237.5940 - val_mse: 20967868.0000 - val_mae: 2237.8801 - lr: 4.8828e-07\n",
      "Epoch 247/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2229.9348 - mse: 20491774.0000 - mae: 2230.2319\n",
      "Epoch 247: val_loss improved from 2237.48267 to 2237.23730, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2236.5881 - mse: 20563594.0000 - mae: 2236.8855 - val_loss: 2237.2373 - val_mse: 20964846.0000 - val_mae: 2237.5225 - lr: 4.8828e-07\n",
      "Epoch 248/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2227.1301 - mse: 20445836.0000 - mae: 2227.4277\n",
      "Epoch 248: val_loss did not improve from 2237.23730\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.4187 - mse: 20560686.0000 - mae: 2236.7168 - val_loss: 2237.2844 - val_mse: 20964294.0000 - val_mae: 2237.5703 - lr: 4.8828e-07\n",
      "Epoch 249/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2235.6724 - mse: 20561318.0000 - mae: 2235.9707\n",
      "Epoch 249: val_loss did not improve from 2237.23730\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.2993 - mse: 20560414.0000 - mae: 2236.5977 - val_loss: 2237.4033 - val_mse: 20964980.0000 - val_mae: 2237.6892 - lr: 4.8828e-07\n",
      "Epoch 250/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2227.2051 - mse: 20446088.0000 - mae: 2227.5042\n",
      "Epoch 250: val_loss improved from 2237.23730 to 2237.19629, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.4888 - mse: 20560880.0000 - mae: 2236.7888 - val_loss: 2237.1963 - val_mse: 20963184.0000 - val_mae: 2237.4817 - lr: 4.8828e-07\n",
      "Epoch 251/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2236.2688 - mse: 20559852.0000 - mae: 2236.5674\n",
      "Epoch 251: val_loss improved from 2237.19629 to 2236.99585, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2236.2688 - mse: 20559852.0000 - mae: 2236.5674 - val_loss: 2236.9958 - val_mse: 20962166.0000 - val_mae: 2237.2812 - lr: 4.8828e-07\n",
      "Epoch 252/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2232.9854 - mse: 20520846.0000 - mae: 2233.2822\n",
      "Epoch 252: val_loss improved from 2236.99585 to 2236.95898, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.3074 - mse: 20559128.0000 - mae: 2236.6045 - val_loss: 2236.9590 - val_mse: 20961178.0000 - val_mae: 2237.2451 - lr: 4.8828e-07\n",
      "Epoch 253/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2229.5981 - mse: 20486326.0000 - mae: 2229.8967\n",
      "Epoch 253: val_loss did not improve from 2236.95898\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.2490 - mse: 20558096.0000 - mae: 2236.5476 - val_loss: 2237.0061 - val_mse: 20961548.0000 - val_mae: 2237.2920 - lr: 4.8828e-07\n",
      "Epoch 254/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2232.8740 - mse: 20519668.0000 - mae: 2233.1738\n",
      "Epoch 254: val_loss did not improve from 2236.95898\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.1953 - mse: 20557916.0000 - mae: 2236.4954 - val_loss: 2237.1045 - val_mse: 20961898.0000 - val_mae: 2237.3906 - lr: 4.8828e-07\n",
      "Epoch 255/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2232.9695 - mse: 20520016.0000 - mae: 2233.2695\n",
      "Epoch 255: val_loss improved from 2236.95898 to 2236.84985, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2236.2913 - mse: 20558292.0000 - mae: 2236.5916 - val_loss: 2236.8499 - val_mse: 20960416.0000 - val_mae: 2237.1357 - lr: 4.8828e-07\n",
      "Epoch 256/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2235.5078 - mse: 20561412.0000 - mae: 2235.8079\n",
      "Epoch 256: val_loss did not improve from 2236.84985\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.1438 - mse: 20556838.0000 - mae: 2236.4438 - val_loss: 2236.8760 - val_mse: 20959858.0000 - val_mae: 2237.1616 - lr: 4.8828e-07\n",
      "Epoch 257/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2227.3096 - mse: 20467620.0000 - mae: 2227.6089\n",
      "Epoch 257: val_loss improved from 2236.84985 to 2236.59229, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.1252 - mse: 20557154.0000 - mae: 2236.4250 - val_loss: 2236.5923 - val_mse: 20958574.0000 - val_mae: 2236.8784 - lr: 4.8828e-07\n",
      "Epoch 258/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2226.8481 - mse: 20441286.0000 - mae: 2227.1445\n",
      "Epoch 258: val_loss improved from 2236.59229 to 2236.55103, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2236.1370 - mse: 20556112.0000 - mae: 2236.4338 - val_loss: 2236.5510 - val_mse: 20958354.0000 - val_mae: 2236.8369 - lr: 4.8828e-07\n",
      "Epoch 259/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2236.0552 - mse: 20555872.0000 - mae: 2236.3545\n",
      "Epoch 259: val_loss improved from 2236.55103 to 2236.54297, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2236.0552 - mse: 20555872.0000 - mae: 2236.3545 - val_loss: 2236.5430 - val_mse: 20958388.0000 - val_mae: 2236.8281 - lr: 4.8828e-07\n",
      "Epoch 260/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2235.4104 - mse: 20556850.0000 - mae: 2235.7104\n",
      "Epoch 260: val_loss improved from 2236.54297 to 2236.15527, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2236.0420 - mse: 20555998.0000 - mae: 2236.3425 - val_loss: 2236.1553 - val_mse: 20957962.0000 - val_mae: 2236.4407 - lr: 4.8828e-07\n",
      "Epoch 261/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2235.3796 - mse: 20556890.0000 - mae: 2235.6797\n",
      "Epoch 261: val_loss improved from 2236.15527 to 2235.99878, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2236.0115 - mse: 20556034.0000 - mae: 2236.3115 - val_loss: 2235.9988 - val_mse: 20958480.0000 - val_mae: 2236.2844 - lr: 4.8828e-07\n",
      "Epoch 262/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2229.2566 - mse: 20483984.0000 - mae: 2229.5566\n",
      "Epoch 262: val_loss did not improve from 2235.99878\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.9058 - mse: 20555688.0000 - mae: 2236.2058 - val_loss: 2236.2917 - val_mse: 20961396.0000 - val_mae: 2236.5771 - lr: 4.8828e-07\n",
      "Epoch 263/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2229.1826 - mse: 20482254.0000 - mae: 2229.4802\n",
      "Epoch 263: val_loss did not improve from 2235.99878\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.8433 - mse: 20554096.0000 - mae: 2236.1414 - val_loss: 2236.1868 - val_mse: 20954492.0000 - val_mae: 2236.4724 - lr: 4.8828e-07\n",
      "Epoch 264/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2235.9504 - mse: 20552820.0000 - mae: 2236.2476\n",
      "Epoch 264: val_loss did not improve from 2235.99878\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.9504 - mse: 20552820.0000 - mae: 2236.2476 - val_loss: 2236.1316 - val_mse: 20955962.0000 - val_mae: 2236.4172 - lr: 4.8828e-07\n",
      "Epoch 265/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2235.2632 - mse: 20553758.0000 - mae: 2235.5620\n",
      "Epoch 265: val_loss did not improve from 2235.99878\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2235.8970 - mse: 20552924.0000 - mae: 2236.1960 - val_loss: 2236.0596 - val_mse: 20956788.0000 - val_mae: 2236.3455 - lr: 4.8828e-07\n",
      "Epoch 266/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2226.5432 - mse: 20438114.0000 - mae: 2226.8423\n",
      "Epoch 266: val_loss improved from 2235.99878 to 2235.95996, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.8364 - mse: 20552932.0000 - mae: 2236.1360 - val_loss: 2235.9600 - val_mse: 20958834.0000 - val_mae: 2236.2458 - lr: 4.8828e-07\n",
      "Epoch 267/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2226.8721 - mse: 20461308.0000 - mae: 2227.1711\n",
      "Epoch 267: val_loss did not improve from 2235.95996\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.6943 - mse: 20550844.0000 - mae: 2235.9937 - val_loss: 2236.3354 - val_mse: 20954322.0000 - val_mae: 2236.6208 - lr: 4.8828e-07\n",
      "Epoch 268/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2235.1985 - mse: 20550366.0000 - mae: 2235.4988\n",
      "Epoch 268: val_loss did not improve from 2235.95996\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.8335 - mse: 20549546.0000 - mae: 2236.1338 - val_loss: 2236.3865 - val_mse: 20955654.0000 - val_mae: 2236.6726 - lr: 4.8828e-07\n",
      "Epoch 269/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2235.7468 - mse: 20549276.0000 - mae: 2236.0471\n",
      "Epoch 269: val_loss did not improve from 2235.95996\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.7468 - mse: 20549276.0000 - mae: 2236.0471 - val_loss: 2236.4255 - val_mse: 20955394.0000 - val_mae: 2236.7114 - lr: 4.8828e-07\n",
      "Epoch 270/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2232.4800 - mse: 20511312.0000 - mae: 2232.7788\n",
      "Epoch 270: val_loss did not improve from 2235.95996\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.8062 - mse: 20549592.0000 - mae: 2236.1050 - val_loss: 2236.1030 - val_mse: 20954940.0000 - val_mae: 2236.3889 - lr: 4.8828e-07\n",
      "Epoch 271/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2226.4651 - mse: 20434324.0000 - mae: 2226.7639\n",
      "Epoch 271: val_loss did not improve from 2235.95996\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.7646 - mse: 20549188.0000 - mae: 2236.0642 - val_loss: 2236.0608 - val_mse: 20954510.0000 - val_mae: 2236.3472 - lr: 4.8828e-07\n",
      "Epoch 272/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2228.6833 - mse: 20464622.0000 - mae: 2228.9805\n",
      "Epoch 272: val_loss did not improve from 2235.95996\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2235.6667 - mse: 20548842.0000 - mae: 2235.9644 - val_loss: 2235.9897 - val_mse: 20955506.0000 - val_mae: 2236.2766 - lr: 4.8828e-07\n",
      "Epoch 273/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2226.8933 - mse: 20460610.0000 - mae: 2227.1926\n",
      "Epoch 273: val_loss improved from 2235.95996 to 2235.59595, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.7131 - mse: 20550112.0000 - mae: 2236.0127 - val_loss: 2235.5959 - val_mse: 20955632.0000 - val_mae: 2235.8823 - lr: 4.8828e-07\n",
      "Epoch 274/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2234.8398 - mse: 20547336.0000 - mae: 2235.1389\n",
      "Epoch 274: val_loss did not improve from 2235.59595\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.4763 - mse: 20546526.0000 - mae: 2235.7754 - val_loss: 2236.2791 - val_mse: 20952924.0000 - val_mae: 2236.5642 - lr: 4.8828e-07\n",
      "Epoch 275/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2234.9524 - mse: 20551318.0000 - mae: 2235.2515\n",
      "Epoch 275: val_loss did not improve from 2235.59595\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.5938 - mse: 20546794.0000 - mae: 2235.8928 - val_loss: 2236.0625 - val_mse: 20952756.0000 - val_mae: 2236.3479 - lr: 4.8828e-07\n",
      "Epoch 276/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2235.6299 - mse: 20546344.0000 - mae: 2235.9260\n",
      "Epoch 276: val_loss did not improve from 2235.59595\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.6299 - mse: 20546344.0000 - mae: 2235.9260 - val_loss: 2236.0020 - val_mse: 20952210.0000 - val_mae: 2236.2876 - lr: 4.8828e-07\n",
      "Epoch 277/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2226.7168 - mse: 20456588.0000 - mae: 2227.0159\n",
      "Epoch 277: val_loss did not improve from 2235.59595\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.5359 - mse: 20546044.0000 - mae: 2235.8354 - val_loss: 2236.0337 - val_mse: 20952704.0000 - val_mae: 2236.3196 - lr: 4.8828e-07\n",
      "Epoch 278/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2234.9365 - mse: 20550520.0000 - mae: 2235.2358\n",
      "Epoch 278: val_loss did not improve from 2235.59595\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.5781 - mse: 20546002.0000 - mae: 2235.8774 - val_loss: 2235.6169 - val_mse: 20951824.0000 - val_mae: 2235.9016 - lr: 4.8828e-07\n",
      "Epoch 279/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2226.1914 - mse: 20431092.0000 - mae: 2226.4912\n",
      "Epoch 279: val_loss improved from 2235.59595 to 2235.51562, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2235.4934 - mse: 20545950.0000 - mae: 2235.7939 - val_loss: 2235.5156 - val_mse: 20952900.0000 - val_mae: 2235.8013 - lr: 4.8828e-07\n",
      "Epoch 280/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2234.7551 - mse: 20546998.0000 - mae: 2235.0540\n",
      "Epoch 280: val_loss did not improve from 2235.51562\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.3936 - mse: 20546208.0000 - mae: 2235.6924 - val_loss: 2235.6636 - val_mse: 20954918.0000 - val_mae: 2235.9492 - lr: 4.8828e-07\n",
      "Epoch 281/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2234.8018 - mse: 20545068.0000 - mae: 2235.1008\n",
      "Epoch 281: val_loss improved from 2235.51562 to 2235.37207, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.4395 - mse: 20544276.0000 - mae: 2235.7385 - val_loss: 2235.3721 - val_mse: 20946262.0000 - val_mae: 2235.6577 - lr: 4.8828e-07\n",
      "Epoch 282/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2235.2319 - mse: 20541468.0000 - mae: 2235.5308\n",
      "Epoch 282: val_loss improved from 2235.37207 to 2235.16699, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.2319 - mse: 20541468.0000 - mae: 2235.5308 - val_loss: 2235.1670 - val_mse: 20945634.0000 - val_mae: 2235.4526 - lr: 4.8828e-07\n",
      "Epoch 283/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2228.6763 - mse: 20470940.0000 - mae: 2228.9739\n",
      "Epoch 283: val_loss improved from 2235.16699 to 2234.72998, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2235.3347 - mse: 20542718.0000 - mae: 2235.6328 - val_loss: 2234.7300 - val_mse: 20944290.0000 - val_mae: 2235.0156 - lr: 4.8828e-07\n",
      "Epoch 284/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2234.4929 - mse: 20546244.0000 - mae: 2234.7920\n",
      "Epoch 284: val_loss improved from 2234.72998 to 2234.70264, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.1360 - mse: 20541746.0000 - mae: 2235.4353 - val_loss: 2234.7026 - val_mse: 20946210.0000 - val_mae: 2234.9885 - lr: 4.8828e-07\n",
      "Epoch 285/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2228.5403 - mse: 20471690.0000 - mae: 2228.8386\n",
      "Epoch 285: val_loss improved from 2234.70264 to 2234.67627, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.2026 - mse: 20543436.0000 - mae: 2235.5010 - val_loss: 2234.6763 - val_mse: 20947892.0000 - val_mae: 2234.9612 - lr: 4.8828e-07\n",
      "Epoch 286/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2226.2170 - mse: 20450594.0000 - mae: 2226.5154\n",
      "Epoch 286: val_loss did not improve from 2234.67627\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.0383 - mse: 20540064.0000 - mae: 2235.3374 - val_loss: 2235.0283 - val_mse: 20943208.0000 - val_mae: 2235.3137 - lr: 4.8828e-07\n",
      "Epoch 287/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2231.8430 - mse: 20501784.0000 - mae: 2232.1418\n",
      "Epoch 287: val_loss did not improve from 2234.67627\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.1724 - mse: 20540014.0000 - mae: 2235.4714 - val_loss: 2234.7559 - val_mse: 20943236.0000 - val_mae: 2235.0413 - lr: 4.8828e-07\n",
      "Epoch 288/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2234.4299 - mse: 20540036.0000 - mae: 2234.7278\n",
      "Epoch 288: val_loss did not improve from 2234.67627\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.0701 - mse: 20539276.0000 - mae: 2235.3679 - val_loss: 2234.7710 - val_mse: 20944310.0000 - val_mae: 2235.0569 - lr: 4.8828e-07\n",
      "Epoch 289/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2231.7202 - mse: 20502144.0000 - mae: 2232.0188\n",
      "Epoch 289: val_loss improved from 2234.67627 to 2234.43262, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.0518 - mse: 20540408.0000 - mae: 2235.3506 - val_loss: 2234.4326 - val_mse: 20945036.0000 - val_mae: 2234.7180 - lr: 4.8828e-07\n",
      "Epoch 290/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2225.7129 - mse: 20425448.0000 - mae: 2226.0103\n",
      "Epoch 290: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.0212 - mse: 20540308.0000 - mae: 2235.3193 - val_loss: 2234.6177 - val_mse: 20947052.0000 - val_mae: 2234.9038 - lr: 4.8828e-07\n",
      "Epoch 291/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2228.2615 - mse: 20466144.0000 - mae: 2228.5613\n",
      "Epoch 291: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2234.9268 - mse: 20537958.0000 - mae: 2235.2268 - val_loss: 2234.9421 - val_mse: 20942424.0000 - val_mae: 2235.2275 - lr: 4.8828e-07\n",
      "Epoch 292/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2231.6870 - mse: 20499422.0000 - mae: 2231.9854\n",
      "Epoch 292: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.0173 - mse: 20537660.0000 - mae: 2235.3157 - val_loss: 2234.6775 - val_mse: 20941902.0000 - val_mae: 2234.9636 - lr: 4.8828e-07\n",
      "Epoch 293/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2224.7075 - mse: 20394694.0000 - mae: 2225.0049\n",
      "Epoch 293: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2234.9639 - mse: 20537226.0000 - mae: 2235.2622 - val_loss: 2234.6235 - val_mse: 20943620.0000 - val_mae: 2234.9092 - lr: 4.8828e-07\n",
      "Epoch 294/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2234.2307 - mse: 20538508.0000 - mae: 2234.5286\n",
      "Epoch 294: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2234.8718 - mse: 20537754.0000 - mae: 2235.1699 - val_loss: 2234.5947 - val_mse: 20945134.0000 - val_mae: 2234.8804 - lr: 4.8828e-07\n",
      "Epoch 295/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2225.5667 - mse: 20421226.0000 - mae: 2225.8635\n",
      "Epoch 295: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2234.8752 - mse: 20536116.0000 - mae: 2235.1729 - val_loss: 2234.9858 - val_mse: 20942666.0000 - val_mae: 2235.2720 - lr: 4.8828e-07\n",
      "Epoch 296/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2225.9585 - mse: 20445098.0000 - mae: 2226.2583\n",
      "Epoch 296: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2234.7808 - mse: 20534526.0000 - mae: 2235.0811 - val_loss: 2235.1426 - val_mse: 20944306.0000 - val_mae: 2235.4285 - lr: 4.8828e-07\n",
      "Epoch 297/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2234.4097 - mse: 20536232.0000 - mae: 2234.7075\n",
      "Epoch 297: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2235.0508 - mse: 20535494.0000 - mae: 2235.3486 - val_loss: 2234.9229 - val_mse: 20943596.0000 - val_mae: 2235.2087 - lr: 4.8828e-07\n",
      "Epoch 298/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2225.9668 - mse: 20444976.0000 - mae: 2226.2659\n",
      "Epoch 298: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2234.7925 - mse: 20534436.0000 - mae: 2235.0920 - val_loss: 2235.0193 - val_mse: 20943630.0000 - val_mae: 2235.3057 - lr: 4.8828e-07\n",
      "Epoch 299/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2226.1873 - mse: 20445988.0000 - mae: 2226.4856\n",
      "Epoch 299: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2235.0051 - mse: 20535448.0000 - mae: 2235.3035 - val_loss: 2234.7388 - val_mse: 20942972.0000 - val_mae: 2235.0249 - lr: 4.8828e-07\n",
      "Epoch 300/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2224.8015 - mse: 20409368.0000 - mae: 2225.1016\n",
      "Epoch 300: val_loss did not improve from 2234.43262\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2234.1223 - mse: 20524402.0000 - mae: 2234.4226 - val_loss: 2234.6467 - val_mse: 20941906.0000 - val_mae: 2234.9326 - lr: 2.4414e-07\n",
      "Epoch 301/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2233.5063 - mse: 20525008.0000 - mae: 2233.8042\n",
      "Epoch 301: val_loss improved from 2234.43262 to 2234.34302, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2234.1489 - mse: 20524298.0000 - mae: 2234.4468 - val_loss: 2234.3430 - val_mse: 20940154.0000 - val_mae: 2234.6289 - lr: 2.4414e-07\n",
      "Epoch 302/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2233.3821 - mse: 20528554.0000 - mae: 2233.6819\n",
      "Epoch 302: val_loss improved from 2234.34302 to 2234.25537, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2234.0259 - mse: 20524074.0000 - mae: 2234.3259 - val_loss: 2234.2554 - val_mse: 20940112.0000 - val_mae: 2234.5410 - lr: 2.4414e-07\n",
      "Epoch 303/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.7043 - mse: 20486100.0000 - mae: 2231.0010\n",
      "Epoch 303: val_loss improved from 2234.25537 to 2234.02246, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2234.0393 - mse: 20524376.0000 - mae: 2234.3359 - val_loss: 2234.0225 - val_mse: 20939820.0000 - val_mae: 2234.3074 - lr: 2.4414e-07\n",
      "Epoch 304/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.5652 - mse: 20484688.0000 - mae: 2230.8625\n",
      "Epoch 304: val_loss did not improve from 2234.02246\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.8994 - mse: 20522934.0000 - mae: 2234.1970 - val_loss: 2234.3772 - val_mse: 20938168.0000 - val_mae: 2234.6624 - lr: 2.4414e-07\n",
      "Epoch 305/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2233.3945 - mse: 20527222.0000 - mae: 2233.6943\n",
      "Epoch 305: val_loss did not improve from 2234.02246\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2234.0393 - mse: 20522756.0000 - mae: 2234.3394 - val_loss: 2234.2324 - val_mse: 20937712.0000 - val_mae: 2234.5181 - lr: 2.4414e-07\n",
      "Epoch 306/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2233.3555 - mse: 20523426.0000 - mae: 2233.6543\n",
      "Epoch 306: val_loss did not improve from 2234.02246\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.9990 - mse: 20522732.0000 - mae: 2234.2981 - val_loss: 2234.1384 - val_mse: 20937536.0000 - val_mae: 2234.4236 - lr: 2.4414e-07\n",
      "Epoch 307/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2226.9458 - mse: 20438362.0000 - mae: 2227.2441\n",
      "Epoch 307: val_loss improved from 2234.02246 to 2233.95703, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.9456 - mse: 20522742.0000 - mae: 2234.2439 - val_loss: 2233.9570 - val_mse: 20937596.0000 - val_mae: 2234.2424 - lr: 2.4414e-07\n",
      "Epoch 308/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2226.9309 - mse: 20438546.0000 - mae: 2227.2307\n",
      "Epoch 308: val_loss improved from 2233.95703 to 2233.92285, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.9309 - mse: 20522888.0000 - mae: 2234.2312 - val_loss: 2233.9229 - val_mse: 20938896.0000 - val_mae: 2234.2087 - lr: 2.4414e-07\n",
      "Epoch 309/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2224.4875 - mse: 20406448.0000 - mae: 2224.7866\n",
      "Epoch 309: val_loss did not improve from 2233.92285\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.8079 - mse: 20521444.0000 - mae: 2234.1074 - val_loss: 2234.3281 - val_mse: 20937354.0000 - val_mae: 2234.6143 - lr: 2.4414e-07\n",
      "Epoch 310/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2233.3533 - mse: 20525718.0000 - mae: 2233.6521\n",
      "Epoch 310: val_loss did not improve from 2233.92285\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.9983 - mse: 20521256.0000 - mae: 2234.2974 - val_loss: 2234.1138 - val_mse: 20936866.0000 - val_mae: 2234.3994 - lr: 2.4414e-07\n",
      "Epoch 311/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.5457 - mse: 20482944.0000 - mae: 2230.8450\n",
      "Epoch 311: val_loss did not improve from 2233.92285\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.8813 - mse: 20521210.0000 - mae: 2234.1809 - val_loss: 2234.0598 - val_mse: 20937580.0000 - val_mae: 2234.3450 - lr: 2.4414e-07\n",
      "Epoch 312/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.5637 - mse: 20482788.0000 - mae: 2230.8616\n",
      "Epoch 312: val_loss improved from 2233.92285 to 2233.82373, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.8999 - mse: 20521082.0000 - mae: 2234.1980 - val_loss: 2233.8237 - val_mse: 20937148.0000 - val_mae: 2234.1086 - lr: 2.4414e-07\n",
      "Epoch 313/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2226.8394 - mse: 20436846.0000 - mae: 2227.1370\n",
      "Epoch 313: val_loss improved from 2233.82373 to 2233.81152, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.8408 - mse: 20521212.0000 - mae: 2234.1389 - val_loss: 2233.8115 - val_mse: 20938524.0000 - val_mae: 2234.0967 - lr: 2.4414e-07\n",
      "Epoch 314/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2233.1609 - mse: 20520530.0000 - mae: 2233.4602\n",
      "Epoch 314: val_loss did not improve from 2233.81152\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.8059 - mse: 20519856.0000 - mae: 2234.1055 - val_loss: 2233.9971 - val_mse: 20935354.0000 - val_mae: 2234.2830 - lr: 2.4414e-07\n",
      "Epoch 315/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.4988 - mse: 20481104.0000 - mae: 2230.7969\n",
      "Epoch 315: val_loss did not improve from 2233.81152\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.8350 - mse: 20519394.0000 - mae: 2234.1333 - val_loss: 2233.9751 - val_mse: 20936488.0000 - val_mae: 2234.2607 - lr: 2.4414e-07\n",
      "Epoch 316/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2227.1062 - mse: 20447362.0000 - mae: 2227.4045\n",
      "Epoch 316: val_loss did not improve from 2233.81152\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.7803 - mse: 20519338.0000 - mae: 2234.0789 - val_loss: 2233.8130 - val_mse: 20936592.0000 - val_mae: 2234.0984 - lr: 2.4414e-07\n",
      "Epoch 317/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2227.0935 - mse: 20447362.0000 - mae: 2227.3918\n",
      "Epoch 317: val_loss improved from 2233.81152 to 2233.77979, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.7676 - mse: 20519312.0000 - mae: 2234.0664 - val_loss: 2233.7798 - val_mse: 20938036.0000 - val_mae: 2234.0645 - lr: 2.4414e-07\n",
      "Epoch 318/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2224.8501 - mse: 20428248.0000 - mae: 2225.1477\n",
      "Epoch 318: val_loss did not improve from 2233.77979\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.6787 - mse: 20517838.0000 - mae: 2233.9768 - val_loss: 2234.1770 - val_mse: 20936472.0000 - val_mae: 2234.4624 - lr: 2.4414e-07\n",
      "Epoch 319/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.4570 - mse: 20479232.0000 - mae: 2230.7551\n",
      "Epoch 319: val_loss did not improve from 2233.77979\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.7952 - mse: 20517558.0000 - mae: 2234.0933 - val_loss: 2233.9688 - val_mse: 20936280.0000 - val_mae: 2234.2544 - lr: 2.4414e-07\n",
      "Epoch 320/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2233.7236 - mse: 20517426.0000 - mae: 2234.0222\n",
      "Epoch 320: val_loss did not improve from 2233.77979\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.7236 - mse: 20517426.0000 - mae: 2234.0222 - val_loss: 2233.9155 - val_mse: 20937138.0000 - val_mae: 2234.2014 - lr: 2.4414e-07\n",
      "Epoch 321/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2224.8457 - mse: 20427752.0000 - mae: 2225.1431\n",
      "Epoch 321: val_loss improved from 2233.77979 to 2233.72974, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.6765 - mse: 20517390.0000 - mae: 2233.9741 - val_loss: 2233.7297 - val_mse: 20937162.0000 - val_mae: 2234.0159 - lr: 2.4414e-07\n",
      "Epoch 322/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2224.7793 - mse: 20426266.0000 - mae: 2225.0781\n",
      "Epoch 322: val_loss did not improve from 2233.72974\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.6089 - mse: 20515882.0000 - mae: 2233.9082 - val_loss: 2234.0938 - val_mse: 20935666.0000 - val_mae: 2234.3796 - lr: 2.4414e-07\n",
      "Epoch 323/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2224.8325 - mse: 20425916.0000 - mae: 2225.1313\n",
      "Epoch 323: val_loss did not improve from 2233.72974\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.6624 - mse: 20515568.0000 - mae: 2233.9614 - val_loss: 2233.9473 - val_mse: 20935668.0000 - val_mae: 2234.2332 - lr: 2.4414e-07\n",
      "Epoch 324/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2226.6646 - mse: 20431150.0000 - mae: 2226.9634\n",
      "Epoch 324: val_loss did not improve from 2233.72974\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.6682 - mse: 20515602.0000 - mae: 2233.9673 - val_loss: 2233.9043 - val_mse: 20936394.0000 - val_mae: 2234.1899 - lr: 2.4414e-07\n",
      "Epoch 325/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2226.9036 - mse: 20443632.0000 - mae: 2227.2026\n",
      "Epoch 325: val_loss did not improve from 2233.72974\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.5820 - mse: 20515644.0000 - mae: 2233.8811 - val_loss: 2233.9197 - val_mse: 20937618.0000 - val_mae: 2234.2051 - lr: 2.4414e-07\n",
      "Epoch 326/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2232.9675 - mse: 20516738.0000 - mae: 2233.2673\n",
      "Epoch 326: val_loss improved from 2233.72974 to 2233.66968, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.6145 - mse: 20516088.0000 - mae: 2233.9146 - val_loss: 2233.6697 - val_mse: 20937276.0000 - val_mae: 2233.9548 - lr: 2.4414e-07\n",
      "Epoch 327/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2232.8701 - mse: 20518824.0000 - mae: 2233.1704\n",
      "Epoch 327: val_loss did not improve from 2233.66968\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.5164 - mse: 20514374.0000 - mae: 2233.8169 - val_loss: 2234.0706 - val_mse: 20935550.0000 - val_mae: 2234.3557 - lr: 2.4414e-07\n",
      "Epoch 328/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2226.5850 - mse: 20429722.0000 - mae: 2226.8845\n",
      "Epoch 328: val_loss did not improve from 2233.66968\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.5913 - mse: 20514210.0000 - mae: 2233.8911 - val_loss: 2233.7920 - val_mse: 20934672.0000 - val_mae: 2234.0771 - lr: 2.4414e-07\n",
      "Epoch 329/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2233.5647 - mse: 20514224.0000 - mae: 2233.8625\n",
      "Epoch 329: val_loss did not improve from 2233.66968\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.5647 - mse: 20514224.0000 - mae: 2233.8625 - val_loss: 2233.8442 - val_mse: 20936364.0000 - val_mae: 2234.1301 - lr: 2.4414e-07\n",
      "Epoch 330/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2224.1653 - mse: 20399686.0000 - mae: 2224.4639\n",
      "Epoch 330: val_loss improved from 2233.66968 to 2233.63696, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.4939 - mse: 20514836.0000 - mae: 2233.7930 - val_loss: 2233.6370 - val_mse: 20936710.0000 - val_mae: 2233.9221 - lr: 2.4414e-07\n",
      "Epoch 331/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2224.6396 - mse: 20423654.0000 - mae: 2224.9382\n",
      "Epoch 331: val_loss did not improve from 2233.63696\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.4724 - mse: 20513328.0000 - mae: 2233.7712 - val_loss: 2234.0669 - val_mse: 20935254.0000 - val_mae: 2234.3528 - lr: 2.4414e-07\n",
      "Epoch 332/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2226.8271 - mse: 20441206.0000 - mae: 2227.1240\n",
      "Epoch 332: val_loss did not improve from 2233.63696\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.5073 - mse: 20513290.0000 - mae: 2233.8047 - val_loss: 2233.8291 - val_mse: 20934728.0000 - val_mae: 2234.1150 - lr: 2.4414e-07\n",
      "Epoch 333/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2233.5527 - mse: 20513348.0000 - mae: 2233.8503\n",
      "Epoch 333: val_loss did not improve from 2233.63696\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.5527 - mse: 20513348.0000 - mae: 2233.8503 - val_loss: 2233.8704 - val_mse: 20936034.0000 - val_mae: 2234.1553 - lr: 2.4414e-07\n",
      "Epoch 334/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2233.4771 - mse: 20513488.0000 - mae: 2233.7756\n",
      "Epoch 334: val_loss did not improve from 2233.63696\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.4771 - mse: 20513488.0000 - mae: 2233.7756 - val_loss: 2233.7925 - val_mse: 20936556.0000 - val_mae: 2234.0786 - lr: 2.4414e-07\n",
      "Epoch 335/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.0891 - mse: 20475296.0000 - mae: 2230.3887\n",
      "Epoch 335: val_loss improved from 2233.63696 to 2233.59839, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.4314 - mse: 20513706.0000 - mae: 2233.7312 - val_loss: 2233.5984 - val_mse: 20936682.0000 - val_mae: 2233.8840 - lr: 2.4414e-07\n",
      "Epoch 336/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2224.1113 - mse: 20397290.0000 - mae: 2224.4104\n",
      "Epoch 336: val_loss did not improve from 2233.59839\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.4404 - mse: 20512456.0000 - mae: 2233.7397 - val_loss: 2233.9402 - val_mse: 20934866.0000 - val_mae: 2234.2258 - lr: 2.4414e-07\n",
      "Epoch 337/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2233.4006 - mse: 20512190.0000 - mae: 2233.6990\n",
      "Epoch 337: val_loss did not improve from 2233.59839\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.4006 - mse: 20512190.0000 - mae: 2233.6990 - val_loss: 2233.9819 - val_mse: 20935948.0000 - val_mae: 2234.2673 - lr: 2.4414e-07\n",
      "Epoch 338/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.1523 - mse: 20474396.0000 - mae: 2230.4507\n",
      "Epoch 338: val_loss did not improve from 2233.59839\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.4949 - mse: 20512800.0000 - mae: 2233.7932 - val_loss: 2233.7329 - val_mse: 20935884.0000 - val_mae: 2234.0188 - lr: 2.4414e-07\n",
      "Epoch 339/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2226.6904 - mse: 20440568.0000 - mae: 2226.9888\n",
      "Epoch 339: val_loss did not improve from 2233.59839\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.3713 - mse: 20512624.0000 - mae: 2233.6697 - val_loss: 2233.7859 - val_mse: 20937570.0000 - val_mae: 2234.0708 - lr: 2.4414e-07\n",
      "Epoch 340/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2233.3752 - mse: 20511722.0000 - mae: 2233.6753\n",
      "Epoch 340: val_loss did not improve from 2233.59839\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.3752 - mse: 20511722.0000 - mae: 2233.6753 - val_loss: 2233.9014 - val_mse: 20933974.0000 - val_mae: 2234.1868 - lr: 2.4414e-07\n",
      "Epoch 341/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2224.0559 - mse: 20395916.0000 - mae: 2224.3552\n",
      "Epoch 341: val_loss did not improve from 2233.59839\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.3850 - mse: 20511084.0000 - mae: 2233.6851 - val_loss: 2233.9753 - val_mse: 20935678.0000 - val_mae: 2234.2612 - lr: 2.4414e-07\n",
      "Epoch 342/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2226.3823 - mse: 20426932.0000 - mae: 2226.6812\n",
      "Epoch 342: val_loss did not improve from 2233.59839\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.3904 - mse: 20511456.0000 - mae: 2233.6895 - val_loss: 2233.6958 - val_mse: 20934600.0000 - val_mae: 2233.9819 - lr: 2.4414e-07\n",
      "Epoch 343/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2232.7209 - mse: 20515872.0000 - mae: 2233.0181\n",
      "Epoch 343: val_loss did not improve from 2233.59839\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.3684 - mse: 20511436.0000 - mae: 2233.6655 - val_loss: 2233.7271 - val_mse: 20936266.0000 - val_mae: 2234.0127 - lr: 2.4414e-07\n",
      "Epoch 344/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2224.4468 - mse: 20422322.0000 - mae: 2224.7451\n",
      "Epoch 344: val_loss improved from 2233.59839 to 2233.51953, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.2805 - mse: 20512034.0000 - mae: 2233.5796 - val_loss: 2233.5195 - val_mse: 20936206.0000 - val_mae: 2233.8049 - lr: 2.4414e-07\n",
      "Epoch 345/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2232.6646 - mse: 20511204.0000 - mae: 2232.9622\n",
      "Epoch 345: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.3132 - mse: 20510574.0000 - mae: 2233.6108 - val_loss: 2233.9377 - val_mse: 20934618.0000 - val_mae: 2234.2241 - lr: 2.4414e-07\n",
      "Epoch 346/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2224.4297 - mse: 20420604.0000 - mae: 2224.7275\n",
      "Epoch 346: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.2632 - mse: 20510280.0000 - mae: 2233.5613 - val_loss: 2233.9011 - val_mse: 20935548.0000 - val_mae: 2234.1868 - lr: 2.4414e-07\n",
      "Epoch 347/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2230.0259 - mse: 20472534.0000 - mae: 2230.3250\n",
      "Epoch 347: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.3694 - mse: 20510952.0000 - mae: 2233.6687 - val_loss: 2233.6707 - val_mse: 20935164.0000 - val_mae: 2233.9561 - lr: 2.4414e-07\n",
      "Epoch 348/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2224.3896 - mse: 20420970.0000 - mae: 2224.6873\n",
      "Epoch 348: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.2273 - mse: 20510684.0000 - mae: 2233.5254 - val_loss: 2233.7109 - val_mse: 20936522.0000 - val_mae: 2233.9966 - lr: 2.4414e-07\n",
      "Epoch 349/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2232.5796 - mse: 20510204.0000 - mae: 2232.8772\n",
      "Epoch 349: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.2285 - mse: 20509576.0000 - mae: 2233.5261 - val_loss: 2233.8557 - val_mse: 20933512.0000 - val_mae: 2234.1414 - lr: 2.4414e-07\n",
      "Epoch 350/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2223.9287 - mse: 20393852.0000 - mae: 2224.2271\n",
      "Epoch 350: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.2603 - mse: 20509052.0000 - mae: 2233.5596 - val_loss: 2233.9255 - val_mse: 20934904.0000 - val_mae: 2234.2107 - lr: 2.4414e-07\n",
      "Epoch 351/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2232.6101 - mse: 20510246.0000 - mae: 2232.9075\n",
      "Epoch 351: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.2590 - mse: 20509620.0000 - mae: 2233.5564 - val_loss: 2233.6968 - val_mse: 20934430.0000 - val_mae: 2233.9824 - lr: 2.4414e-07\n",
      "Epoch 352/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2222.9397 - mse: 20366774.0000 - mae: 2223.2375\n",
      "Epoch 352: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2233.2231 - mse: 20509610.0000 - mae: 2233.5212 - val_loss: 2233.6501 - val_mse: 20935412.0000 - val_mae: 2233.9358 - lr: 2.4414e-07\n",
      "Epoch 353/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2233.0884 - mse: 20508626.0000 - mae: 2233.3887\n",
      "Epoch 353: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2233.0884 - mse: 20508626.0000 - mae: 2233.3887 - val_loss: 2233.8801 - val_mse: 20933630.0000 - val_mae: 2234.1655 - lr: 2.4414e-07\n",
      "Epoch 354/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2229.9143 - mse: 20469722.0000 - mae: 2230.2126\n",
      "Epoch 354: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 4s 8ms/step - loss: 2233.2581 - mse: 20508142.0000 - mae: 2233.5564 - val_loss: 2233.9211 - val_mse: 20934420.0000 - val_mae: 2234.2070 - lr: 2.4414e-07\n",
      "Epoch 355/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2225.8503 - mse: 20419508.0000 - mae: 2226.1494\n",
      "Epoch 355: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 4s 8ms/step - loss: 2232.8630 - mse: 20504086.0000 - mae: 2233.1626 - val_loss: 2233.7412 - val_mse: 20933666.0000 - val_mae: 2234.0266 - lr: 1.2207e-07\n",
      "Epoch 356/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2223.5615 - mse: 20388960.0000 - mae: 2223.8611\n",
      "Epoch 356: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.8960 - mse: 20504220.0000 - mae: 2233.1958 - val_loss: 2233.7158 - val_mse: 20934220.0000 - val_mae: 2234.0012 - lr: 1.2207e-07\n",
      "Epoch 357/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2232.8308 - mse: 20504292.0000 - mae: 2233.1301\n",
      "Epoch 357: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.8308 - mse: 20504292.0000 - mae: 2233.1301 - val_loss: 2233.7122 - val_mse: 20934798.0000 - val_mae: 2233.9978 - lr: 1.2207e-07\n",
      "Epoch 358/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2232.2009 - mse: 20509008.0000 - mae: 2232.4993\n",
      "Epoch 358: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.8494 - mse: 20504584.0000 - mae: 2233.1479 - val_loss: 2233.5461 - val_mse: 20934334.0000 - val_mae: 2233.8311 - lr: 1.2207e-07\n",
      "Epoch 359/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2229.4597 - mse: 20466066.0000 - mae: 2229.7581\n",
      "Epoch 359: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.8047 - mse: 20504480.0000 - mae: 2233.1033 - val_loss: 2233.5479 - val_mse: 20934878.0000 - val_mae: 2233.8333 - lr: 1.2207e-07\n",
      "Epoch 360/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2232.1150 - mse: 20508276.0000 - mae: 2232.4138\n",
      "Epoch 360: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.7637 - mse: 20503852.0000 - mae: 2233.0625 - val_loss: 2233.5901 - val_mse: 20932872.0000 - val_mae: 2233.8755 - lr: 1.2207e-07\n",
      "Epoch 361/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2225.7957 - mse: 20418854.0000 - mae: 2226.0935\n",
      "Epoch 361: val_loss did not improve from 2233.51953\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.8071 - mse: 20503398.0000 - mae: 2233.1055 - val_loss: 2233.5547 - val_mse: 20933220.0000 - val_mae: 2233.8394 - lr: 1.2207e-07\n",
      "Epoch 362/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2229.4468 - mse: 20465382.0000 - mae: 2229.7456\n",
      "Epoch 362: val_loss improved from 2233.51953 to 2233.42822, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.7920 - mse: 20503810.0000 - mae: 2233.0913 - val_loss: 2233.4282 - val_mse: 20933014.0000 - val_mae: 2233.7144 - lr: 1.2207e-07\n",
      "Epoch 363/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2223.9438 - mse: 20413860.0000 - mae: 2224.2415\n",
      "Epoch 363: val_loss improved from 2233.42822 to 2233.38843, saving model to new_stne_lstm_weight_ns.h5\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.7786 - mse: 20503590.0000 - mae: 2233.0767 - val_loss: 2233.3884 - val_mse: 20933146.0000 - val_mae: 2233.6746 - lr: 1.2207e-07\n",
      "Epoch 364/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2232.6909 - mse: 20502840.0000 - mae: 2232.9890\n",
      "Epoch 364: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.6909 - mse: 20502840.0000 - mae: 2232.9890 - val_loss: 2233.4517 - val_mse: 20931804.0000 - val_mae: 2233.7363 - lr: 1.2207e-07\n",
      "Epoch 365/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2226.0981 - mse: 20430290.0000 - mae: 2226.3967\n",
      "Epoch 365: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.7822 - mse: 20502416.0000 - mae: 2233.0813 - val_loss: 2233.5103 - val_mse: 20932246.0000 - val_mae: 2233.7959 - lr: 1.2207e-07\n",
      "Epoch 366/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2232.0728 - mse: 20502946.0000 - mae: 2232.3701\n",
      "Epoch 366: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.7241 - mse: 20502360.0000 - mae: 2233.0215 - val_loss: 2233.4426 - val_mse: 20931978.0000 - val_mae: 2233.7283 - lr: 1.2207e-07\n",
      "Epoch 367/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2232.0286 - mse: 20502244.0000 - mae: 2232.3276\n",
      "Epoch 367: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.6799 - mse: 20501662.0000 - mae: 2232.9792 - val_loss: 2233.7864 - val_mse: 20933078.0000 - val_mae: 2234.0720 - lr: 1.2207e-07\n",
      "Epoch 368/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2225.8274 - mse: 20428582.0000 - mae: 2226.1282\n",
      "Epoch 368: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.5146 - mse: 20500702.0000 - mae: 2232.8157 - val_loss: 2233.7961 - val_mse: 20933020.0000 - val_mae: 2234.0818 - lr: 1.2207e-07\n",
      "Epoch 369/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2229.1128 - mse: 20461864.0000 - mae: 2229.4111\n",
      "Epoch 369: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.4575 - mse: 20500232.0000 - mae: 2232.7563 - val_loss: 2233.8418 - val_mse: 20933694.0000 - val_mae: 2234.1272 - lr: 1.2207e-07\n",
      "Epoch 370/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2225.3909 - mse: 20415082.0000 - mae: 2225.6897\n",
      "Epoch 370: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.4045 - mse: 20499608.0000 - mae: 2232.7039 - val_loss: 2233.9727 - val_mse: 20932762.0000 - val_mae: 2234.2588 - lr: 1.2207e-07\n",
      "Epoch 371/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2225.7690 - mse: 20427270.0000 - mae: 2226.0681\n",
      "Epoch 371: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.4556 - mse: 20499384.0000 - mae: 2232.7546 - val_loss: 2233.9773 - val_mse: 20933124.0000 - val_mae: 2234.2632 - lr: 1.2207e-07\n",
      "Epoch 372/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2231.7527 - mse: 20499938.0000 - mae: 2232.0508\n",
      "Epoch 372: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.4053 - mse: 20499364.0000 - mae: 2232.7034 - val_loss: 2234.0012 - val_mse: 20933734.0000 - val_mae: 2234.2861 - lr: 1.2207e-07\n",
      "Epoch 373/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2225.4180 - mse: 20415026.0000 - mae: 2225.7166\n",
      "Epoch 373: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.4319 - mse: 20499542.0000 - mae: 2232.7310 - val_loss: 2233.9058 - val_mse: 20933704.0000 - val_mae: 2234.1909 - lr: 1.2207e-07\n",
      "Epoch 374/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2225.5498 - mse: 20425214.0000 - mae: 2225.8469\n",
      "Epoch 374: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.2388 - mse: 20497338.0000 - mae: 2232.5359 - val_loss: 2233.9856 - val_mse: 20933204.0000 - val_mae: 2234.2715 - lr: 6.1035e-08\n",
      "Epoch 375/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2231.6030 - mse: 20501672.0000 - mae: 2231.9016\n",
      "Epoch 375: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.2524 - mse: 20497262.0000 - mae: 2232.5513 - val_loss: 2233.9126 - val_mse: 20933184.0000 - val_mae: 2234.1978 - lr: 6.1035e-08\n",
      "Epoch 376/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2231.5850 - mse: 20497918.0000 - mae: 2231.8838\n",
      "Epoch 376: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.2380 - mse: 20497350.0000 - mae: 2232.5369 - val_loss: 2233.9077 - val_mse: 20933560.0000 - val_mae: 2234.1929 - lr: 6.1035e-08\n",
      "Epoch 377/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2225.5320 - mse: 20425236.0000 - mae: 2225.8323\n",
      "Epoch 377: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.2209 - mse: 20497372.0000 - mae: 2232.5220 - val_loss: 2233.8452 - val_mse: 20933440.0000 - val_mae: 2234.1311 - lr: 6.1035e-08\n",
      "Epoch 378/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2228.8645 - mse: 20458686.0000 - mae: 2229.1648\n",
      "Epoch 378: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.2100 - mse: 20497066.0000 - mae: 2232.5105 - val_loss: 2233.9443 - val_mse: 20933094.0000 - val_mae: 2234.2300 - lr: 6.1035e-08\n",
      "Epoch 379/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2231.5791 - mse: 20501400.0000 - mae: 2231.8779\n",
      "Epoch 379: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.2288 - mse: 20496994.0000 - mae: 2232.5276 - val_loss: 2233.8716 - val_mse: 20932978.0000 - val_mae: 2234.1567 - lr: 6.1035e-08\n",
      "Epoch 380/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2223.3936 - mse: 20407332.0000 - mae: 2223.6914\n",
      "Epoch 380: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.2292 - mse: 20497044.0000 - mae: 2232.5276 - val_loss: 2233.8855 - val_mse: 20933494.0000 - val_mae: 2234.1711 - lr: 6.1035e-08\n",
      "Epoch 381/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2223.3630 - mse: 20407310.0000 - mae: 2223.6626\n",
      "Epoch 381: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 2232.2002 - mse: 20497038.0000 - mae: 2232.5002 - val_loss: 2233.8313 - val_mse: 20933250.0000 - val_mae: 2234.1167 - lr: 6.1035e-08\n",
      "Epoch 382/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2232.2178 - mse: 20497186.0000 - mae: 2232.5166\n",
      "Epoch 382: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.2178 - mse: 20497186.0000 - mae: 2232.5166 - val_loss: 2233.8364 - val_mse: 20933720.0000 - val_mae: 2234.1218 - lr: 6.1035e-08\n",
      "Epoch 383/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2222.8416 - mse: 20381584.0000 - mae: 2223.1382\n",
      "Epoch 383: val_loss did not improve from 2233.38843\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2232.1814 - mse: 20496834.0000 - mae: 2232.4785 - val_loss: 2233.9338 - val_mse: 20933314.0000 - val_mae: 2234.2197 - lr: 6.1035e-08\n"
     ]
    }
   ],
   "source": [
    "# loss : 모델이 훈련데이터에 대해 얼마나 잘 학습하고 있는지 평가하는 지표 (모델이 훈련데이터에 미치는 오차의 평균을 나타낸다)\n",
    "# val_loss : 검증 데이터(validation data)에 대한 손실 값으로 검증 데이터를 사용해 모델의 일반화 성능을 평가한다 (즉, 훈련 데이터 외의 데이터에 대해서도 얼마나 잘 예측하는지를 판단)\n",
    "# loss와 val_loss가 크지 않은 모델이 overfitting 되지 않고 이상적인 모델이다!\n",
    "\n",
    "lstm_history = lstm_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_lstm])\n",
    "lstm_history_ns = lstm_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_lstm_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    428/Unknown - 3s 2ms/step - loss: 7617.5039 - mse: 183790560.0000 - mae: 7617.8311\n",
      "Epoch 1: val_loss improved from inf to 7079.02686, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 4s 4ms/step - loss: 7609.1899 - mse: 183270432.0000 - mae: 7609.5181 - val_loss: 7079.0269 - val_mse: 170534576.0000 - val_mae: 7079.3257 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 7517.6055 - mse: 180120112.0000 - mae: 7517.9087\n",
      "Epoch 2: val_loss improved from 7079.02686 to 6955.62402, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 7515.5371 - mse: 179894016.0000 - mae: 7515.8408 - val_loss: 6955.6240 - val_mse: 166099504.0000 - val_mae: 6955.9238 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 7341.6401 - mse: 173835040.0000 - mae: 7341.9414\n",
      "Epoch 3: val_loss improved from 6955.62402 to 6763.56885, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 7345.1978 - mse: 173918096.0000 - mae: 7345.4990 - val_loss: 6763.5688 - val_mse: 159447376.0000 - val_mae: 6763.8857 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "418/436 [===========================>..] - ETA: 0s - loss: 7101.3042 - mse: 166238880.0000 - mae: 7101.6011\n",
      "Epoch 4: val_loss improved from 6763.56885 to 6546.51855, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 7117.3979 - mse: 166015216.0000 - mae: 7117.6973 - val_loss: 6546.5186 - val_mse: 151415216.0000 - val_mae: 6546.8140 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "418/436 [===========================>..] - ETA: 0s - loss: 6838.2417 - mse: 156609808.0000 - mae: 6838.5405\n",
      "Epoch 5: val_loss improved from 6546.51855 to 6283.20068, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 6853.1221 - mse: 156705360.0000 - mae: 6853.4224 - val_loss: 6283.2007 - val_mse: 142220096.0000 - val_mae: 6283.5176 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 6562.5645 - mse: 146611664.0000 - mae: 6562.8721\n",
      "Epoch 6: val_loss improved from 6283.20068 to 5990.80566, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 6557.4893 - mse: 146460080.0000 - mae: 6557.7979 - val_loss: 5990.8057 - val_mse: 132320504.0000 - val_mae: 5991.1040 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "416/436 [===========================>..] - ETA: 0s - loss: 6276.2661 - mse: 136339600.0000 - mae: 6276.5669\n",
      "Epoch 7: val_loss improved from 5990.80566 to 5751.72949, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 6269.9165 - mse: 136000832.0000 - mae: 6270.2178 - val_loss: 5751.7295 - val_mse: 122526672.0000 - val_mae: 5752.0117 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 5994.9146 - mse: 125874464.0000 - mae: 5995.2129\n",
      "Epoch 8: val_loss improved from 5751.72949 to 5466.45312, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 5991.3857 - mse: 125602520.0000 - mae: 5991.6865 - val_loss: 5466.4531 - val_mse: 112576664.0000 - val_mae: 5466.7603 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 5754.0874 - mse: 115819288.0000 - mae: 5754.3877\n",
      "Epoch 9: val_loss improved from 5466.45312 to 5220.73682, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 5741.0967 - mse: 115423568.0000 - mae: 5741.3965 - val_loss: 5220.7368 - val_mse: 103243520.0000 - val_mae: 5221.0176 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 5437.8345 - mse: 105325616.0000 - mae: 5438.1357\n",
      "Epoch 10: val_loss improved from 5220.73682 to 4955.72949, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 5437.8345 - mse: 105325616.0000 - mae: 5438.1357 - val_loss: 4955.7295 - val_mse: 94095480.0000 - val_mae: 4956.0063 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 5211.0757 - mse: 96276288.0000 - mae: 5211.3784\n",
      "Epoch 11: val_loss improved from 4955.72949 to 4743.39014, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 5205.1074 - mse: 96057792.0000 - mae: 5205.4106 - val_loss: 4743.3901 - val_mse: 85934552.0000 - val_mae: 4743.7935 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "420/436 [===========================>..] - ETA: 0s - loss: 4921.5454 - mse: 86819128.0000 - mae: 4921.8584\n",
      "Epoch 12: val_loss improved from 4743.39014 to 4516.46826, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 4927.1948 - mse: 86714296.0000 - mae: 4927.5103 - val_loss: 4516.4683 - val_mse: 77052344.0000 - val_mae: 4516.7905 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 4676.3604 - mse: 77753792.0000 - mae: 4676.6812\n",
      "Epoch 13: val_loss improved from 4516.46826 to 4317.49902, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 4692.5493 - mse: 78029280.0000 - mae: 4692.8696 - val_loss: 4317.4990 - val_mse: 69543152.0000 - val_mae: 4317.7505 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 4489.6777 - mse: 70837040.0000 - mae: 4489.9917\n",
      "Epoch 14: val_loss improved from 4317.49902 to 4178.48486, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 4493.0308 - mse: 70906968.0000 - mae: 4493.3452 - val_loss: 4178.4849 - val_mse: 64125712.0000 - val_mae: 4178.7876 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 4323.9463 - mse: 64361852.0000 - mae: 4324.2583\n",
      "Epoch 15: val_loss improved from 4178.48486 to 3980.74927, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 4316.4468 - mse: 64236752.0000 - mae: 4316.7583 - val_loss: 3980.7493 - val_mse: 56686140.0000 - val_mae: 3981.0510 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 4132.8950 - mse: 58189068.0000 - mae: 4133.2041\n",
      "Epoch 16: val_loss improved from 3980.74927 to 3794.21484, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 4133.2886 - mse: 58072484.0000 - mae: 4133.5991 - val_loss: 3794.2148 - val_mse: 51515468.0000 - val_mae: 3794.5183 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 3940.3286 - mse: 52659572.0000 - mae: 3940.6375\n",
      "Epoch 17: val_loss improved from 3794.21484 to 3665.27100, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3955.2390 - mse: 52791236.0000 - mae: 3955.5493 - val_loss: 3665.2710 - val_mse: 46298808.0000 - val_mae: 3665.5671 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 3856.3701 - mse: 48560736.0000 - mae: 3856.6750\n",
      "Epoch 18: val_loss did not improve from 3665.27100\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3849.5879 - mse: 48395624.0000 - mae: 3849.8926 - val_loss: 3677.0847 - val_mse: 43685208.0000 - val_mae: 3677.3840 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 3696.5430 - mse: 44590328.0000 - mae: 3696.8552\n",
      "Epoch 19: val_loss improved from 3665.27100 to 3440.32520, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3692.8147 - mse: 44387452.0000 - mae: 3693.1279 - val_loss: 3440.3252 - val_mse: 40094800.0000 - val_mae: 3440.6260 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "418/436 [===========================>..] - ETA: 0s - loss: 3555.0422 - mse: 42254164.0000 - mae: 3555.3489\n",
      "Epoch 20: val_loss did not improve from 3440.32520\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3571.2014 - mse: 42346812.0000 - mae: 3571.5105 - val_loss: 3451.0522 - val_mse: 39475092.0000 - val_mae: 3451.3572 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 3352.6570 - mse: 37702288.0000 - mae: 3352.9683\n",
      "Epoch 21: val_loss improved from 3440.32520 to 3297.77783, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3353.2126 - mse: 37631504.0000 - mae: 3353.5234 - val_loss: 3297.7778 - val_mse: 35937600.0000 - val_mae: 3298.0459 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "415/436 [===========================>..] - ETA: 0s - loss: 3330.3298 - mse: 36190816.0000 - mae: 3330.6382\n",
      "Epoch 22: val_loss improved from 3297.77783 to 3186.67773, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3328.5032 - mse: 36082792.0000 - mae: 3328.8149 - val_loss: 3186.6777 - val_mse: 33895440.0000 - val_mae: 3186.9932 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "418/436 [===========================>..] - ETA: 0s - loss: 3248.1338 - mse: 34906068.0000 - mae: 3248.4434\n",
      "Epoch 23: val_loss did not improve from 3186.67773\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3253.2209 - mse: 34882836.0000 - mae: 3253.5327 - val_loss: 3199.7197 - val_mse: 33025208.0000 - val_mae: 3200.0137 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3203.2183 - mse: 34032024.0000 - mae: 3203.5208\n",
      "Epoch 24: val_loss improved from 3186.67773 to 3057.73584, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3213.2778 - mse: 34194360.0000 - mae: 3213.5806 - val_loss: 3057.7358 - val_mse: 31005280.0000 - val_mae: 3058.0454 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "418/436 [===========================>..] - ETA: 0s - loss: 3259.5559 - mse: 34629220.0000 - mae: 3259.8718\n",
      "Epoch 25: val_loss improved from 3057.73584 to 2950.10815, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3263.3369 - mse: 34628016.0000 - mae: 3263.6541 - val_loss: 2950.1082 - val_mse: 29431534.0000 - val_mae: 2950.3975 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3288.4954 - mse: 37173256.0000 - mae: 3288.8169\n",
      "Epoch 26: val_loss improved from 2950.10815 to 2932.17554, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3286.4578 - mse: 37130716.0000 - mae: 3286.7800 - val_loss: 2932.1755 - val_mse: 29463504.0000 - val_mae: 2932.5051 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "416/436 [===========================>..] - ETA: 0s - loss: 3110.6565 - mse: 32185678.0000 - mae: 3110.9739\n",
      "Epoch 27: val_loss did not improve from 2932.17554\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3129.0862 - mse: 32375798.0000 - mae: 3129.4050 - val_loss: 2972.1895 - val_mse: 30185812.0000 - val_mae: 2972.4719 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 3028.5515 - mse: 31250892.0000 - mae: 3028.8550\n",
      "Epoch 28: val_loss did not improve from 2932.17554\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3035.8787 - mse: 31363922.0000 - mae: 3036.1831 - val_loss: 2978.9534 - val_mse: 30293132.0000 - val_mae: 2979.2573 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "420/436 [===========================>..] - ETA: 0s - loss: 3062.9915 - mse: 32115004.0000 - mae: 3063.3005\n",
      "Epoch 29: val_loss improved from 2932.17554 to 2928.17529, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3065.0845 - mse: 32067384.0000 - mae: 3065.3953 - val_loss: 2928.1753 - val_mse: 30652516.0000 - val_mae: 2928.4949 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "419/436 [===========================>..] - ETA: 0s - loss: 3087.0435 - mse: 32621002.0000 - mae: 3087.3535\n",
      "Epoch 30: val_loss improved from 2928.17529 to 2909.53955, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3089.9070 - mse: 32618762.0000 - mae: 3090.2183 - val_loss: 2909.5396 - val_mse: 29420542.0000 - val_mae: 2909.8604 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 3112.0923 - mse: 32781198.0000 - mae: 3112.4045\n",
      "Epoch 31: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3109.3550 - mse: 32752690.0000 - mae: 3109.6672 - val_loss: 2977.3894 - val_mse: 30774932.0000 - val_mae: 2977.7014 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "416/436 [===========================>..] - ETA: 0s - loss: 3041.9465 - mse: 32178684.0000 - mae: 3042.2573\n",
      "Epoch 32: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3071.2961 - mse: 32591570.0000 - mae: 3071.6079 - val_loss: 2976.7747 - val_mse: 31038114.0000 - val_mae: 2977.0586 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 3056.7959 - mse: 32064830.0000 - mae: 3057.1057\n",
      "Epoch 33: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3073.0781 - mse: 32294454.0000 - mae: 3073.3896 - val_loss: 2961.0857 - val_mse: 31033466.0000 - val_mae: 2961.4055 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "419/436 [===========================>..] - ETA: 0s - loss: 3046.3787 - mse: 32043600.0000 - mae: 3046.6934\n",
      "Epoch 34: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3063.6267 - mse: 32364442.0000 - mae: 3063.9434 - val_loss: 3087.1030 - val_mse: 35016104.0000 - val_mae: 3087.4534 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 3063.3640 - mse: 32817938.0000 - mae: 3063.6785\n",
      "Epoch 35: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3064.6602 - mse: 32851846.0000 - mae: 3064.9749 - val_loss: 3241.5378 - val_mse: 36551804.0000 - val_mae: 3241.8271 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3191.7375 - mse: 35133708.0000 - mae: 3192.0554\n",
      "Epoch 36: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3190.1152 - mse: 35111180.0000 - mae: 3190.4329 - val_loss: 2937.9434 - val_mse: 31054032.0000 - val_mae: 2938.2383 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 3229.6494 - mse: 34942180.0000 - mae: 3229.9578\n",
      "Epoch 37: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3226.2644 - mse: 34917624.0000 - mae: 3226.5725 - val_loss: 3023.7297 - val_mse: 31098020.0000 - val_mae: 3024.0256 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 3216.5813 - mse: 34754728.0000 - mae: 3216.8977\n",
      "Epoch 38: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3217.0647 - mse: 34711752.0000 - mae: 3217.3826 - val_loss: 3090.8848 - val_mse: 32784464.0000 - val_mae: 3091.2783 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3156.8982 - mse: 33440342.0000 - mae: 3157.2136\n",
      "Epoch 39: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3164.2317 - mse: 33532970.0000 - mae: 3164.5474 - val_loss: 2949.8228 - val_mse: 30683428.0000 - val_mae: 2950.1594 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 3074.6895 - mse: 32313448.0000 - mae: 3075.0024\n",
      "Epoch 40: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3091.5444 - mse: 32475588.0000 - mae: 3091.8591 - val_loss: 3012.9150 - val_mse: 30383744.0000 - val_mae: 3013.2756 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 3074.2310 - mse: 31880010.0000 - mae: 3074.5420\n",
      "Epoch 41: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3088.4958 - mse: 32072648.0000 - mae: 3088.8083 - val_loss: 2962.2698 - val_mse: 29874714.0000 - val_mae: 2962.5679 - lr: 5.0000e-04\n",
      "Epoch 42/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 3094.4307 - mse: 32486754.0000 - mae: 3094.7539\n",
      "Epoch 42: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3094.1011 - mse: 32514504.0000 - mae: 3094.4250 - val_loss: 3019.8857 - val_mse: 31697754.0000 - val_mae: 3020.2510 - lr: 5.0000e-04\n",
      "Epoch 43/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2999.6467 - mse: 30952858.0000 - mae: 2999.9626\n",
      "Epoch 43: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3007.3975 - mse: 31080772.0000 - mae: 3007.7139 - val_loss: 2916.6511 - val_mse: 30175778.0000 - val_mae: 2916.9321 - lr: 5.0000e-04\n",
      "Epoch 44/500\n",
      "419/436 [===========================>..] - ETA: 0s - loss: 2994.6802 - mse: 30908314.0000 - mae: 2994.9919\n",
      "Epoch 44: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3005.5449 - mse: 31048954.0000 - mae: 3005.8569 - val_loss: 3018.6606 - val_mse: 30926276.0000 - val_mae: 3018.9204 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2995.5554 - mse: 31044718.0000 - mae: 2995.8706\n",
      "Epoch 45: val_loss did not improve from 2909.53955\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3001.8901 - mse: 31147932.0000 - mae: 3002.2051 - val_loss: 2955.1772 - val_mse: 29983950.0000 - val_mae: 2955.4832 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2971.1521 - mse: 30729538.0000 - mae: 2971.4666\n",
      "Epoch 46: val_loss improved from 2909.53955 to 2893.37305, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 2s 4ms/step - loss: 2974.6714 - mse: 30810480.0000 - mae: 2974.9856 - val_loss: 2893.3730 - val_mse: 30023906.0000 - val_mae: 2893.6968 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3018.9631 - mse: 31793000.0000 - mae: 3019.2749\n",
      "Epoch 47: val_loss improved from 2893.37305 to 2863.36938, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 3018.0208 - mse: 31731488.0000 - mae: 3018.3320 - val_loss: 2863.3694 - val_mse: 29474266.0000 - val_mae: 2863.6633 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2970.3093 - mse: 30961026.0000 - mae: 2970.6140\n",
      "Epoch 48: val_loss did not improve from 2863.36938\n",
      "436/436 [==============================] - 2s 4ms/step - loss: 2979.6602 - mse: 31143974.0000 - mae: 2979.9661 - val_loss: 2901.5315 - val_mse: 30012722.0000 - val_mae: 2901.8245 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2914.8342 - mse: 30140048.0000 - mae: 2915.1379\n",
      "Epoch 49: val_loss did not improve from 2863.36938\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2936.1165 - mse: 30515704.0000 - mae: 2936.4209 - val_loss: 2869.2961 - val_mse: 29257056.0000 - val_mae: 2869.6047 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2899.5305 - mse: 30026854.0000 - mae: 2899.8386\n",
      "Epoch 50: val_loss improved from 2863.36938 to 2817.74731, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2915.3262 - mse: 30303562.0000 - mae: 2915.6360 - val_loss: 2817.7473 - val_mse: 28691956.0000 - val_mae: 2818.0688 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "417/436 [===========================>..] - ETA: 0s - loss: 2901.0837 - mse: 30529610.0000 - mae: 2901.3960\n",
      "Epoch 51: val_loss did not improve from 2817.74731\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2926.1213 - mse: 30890078.0000 - mae: 2926.4343 - val_loss: 2841.8047 - val_mse: 30394084.0000 - val_mae: 2842.0894 - lr: 5.0000e-04\n",
      "Epoch 52/500\n",
      "416/436 [===========================>..] - ETA: 0s - loss: 2933.7708 - mse: 31407144.0000 - mae: 2934.0737\n",
      "Epoch 52: val_loss did not improve from 2817.74731\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2934.8643 - mse: 31335178.0000 - mae: 2935.1687 - val_loss: 2836.6841 - val_mse: 30232058.0000 - val_mae: 2836.9771 - lr: 5.0000e-04\n",
      "Epoch 53/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2920.9565 - mse: 31078402.0000 - mae: 2921.2722\n",
      "Epoch 53: val_loss improved from 2817.74731 to 2727.55713, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2935.0508 - mse: 31357690.0000 - mae: 2935.3677 - val_loss: 2727.5571 - val_mse: 28973970.0000 - val_mae: 2727.8538 - lr: 5.0000e-04\n",
      "Epoch 54/500\n",
      "415/436 [===========================>..] - ETA: 0s - loss: 2822.1389 - mse: 30538722.0000 - mae: 2822.4521\n",
      "Epoch 54: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2838.3826 - mse: 30704430.0000 - mae: 2838.6975 - val_loss: 2727.9946 - val_mse: 29022914.0000 - val_mae: 2728.2947 - lr: 5.0000e-04\n",
      "Epoch 55/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2832.6111 - mse: 30396192.0000 - mae: 2832.9241\n",
      "Epoch 55: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2844.3311 - mse: 30571424.0000 - mae: 2844.6448 - val_loss: 2770.9629 - val_mse: 29000712.0000 - val_mae: 2771.2576 - lr: 5.0000e-04\n",
      "Epoch 56/500\n",
      "415/436 [===========================>..] - ETA: 0s - loss: 2811.4395 - mse: 29696832.0000 - mae: 2811.7515\n",
      "Epoch 56: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2844.9727 - mse: 30173924.0000 - mae: 2845.2861 - val_loss: 2887.3269 - val_mse: 29977892.0000 - val_mae: 2887.6006 - lr: 5.0000e-04\n",
      "Epoch 57/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2853.3606 - mse: 30260122.0000 - mae: 2853.6650\n",
      "Epoch 57: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2858.8665 - mse: 30295796.0000 - mae: 2859.1709 - val_loss: 2817.7148 - val_mse: 29650184.0000 - val_mae: 2818.0173 - lr: 5.0000e-04\n",
      "Epoch 58/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2820.6846 - mse: 29860504.0000 - mae: 2820.9917\n",
      "Epoch 58: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2839.4722 - mse: 30137346.0000 - mae: 2839.7808 - val_loss: 2846.3750 - val_mse: 29973070.0000 - val_mae: 2846.6765 - lr: 5.0000e-04\n",
      "Epoch 59/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2854.5027 - mse: 30879462.0000 - mae: 2854.8115\n",
      "Epoch 59: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2866.5864 - mse: 31081492.0000 - mae: 2866.8960 - val_loss: 2837.9775 - val_mse: 31044776.0000 - val_mae: 2838.2651 - lr: 5.0000e-04\n",
      "Epoch 60/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2883.5647 - mse: 30708858.0000 - mae: 2883.8694\n",
      "Epoch 60: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2897.0264 - mse: 30923338.0000 - mae: 2897.3318 - val_loss: 2826.4377 - val_mse: 29232608.0000 - val_mae: 2826.7400 - lr: 5.0000e-04\n",
      "Epoch 61/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2837.6033 - mse: 30104316.0000 - mae: 2837.9155\n",
      "Epoch 61: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2853.5676 - mse: 30272646.0000 - mae: 2853.8806 - val_loss: 2758.4402 - val_mse: 29089602.0000 - val_mae: 2758.7490 - lr: 5.0000e-04\n",
      "Epoch 62/500\n",
      "421/436 [===========================>..] - ETA: 0s - loss: 2780.2539 - mse: 29684638.0000 - mae: 2780.5581\n",
      "Epoch 62: val_loss did not improve from 2727.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2799.4985 - mse: 29890612.0000 - mae: 2799.8044 - val_loss: 2756.0586 - val_mse: 29196544.0000 - val_mae: 2756.4165 - lr: 5.0000e-04\n",
      "Epoch 63/500\n",
      "424/436 [============================>.] - ETA: 0s - loss: 2779.9136 - mse: 29792334.0000 - mae: 2780.2305\n",
      "Epoch 63: val_loss improved from 2727.55713 to 2708.67358, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2793.7034 - mse: 30019964.0000 - mae: 2794.0222 - val_loss: 2708.6736 - val_mse: 30185470.0000 - val_mae: 2708.9424 - lr: 5.0000e-04\n",
      "Epoch 64/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2811.2085 - mse: 30485924.0000 - mae: 2811.5327\n",
      "Epoch 64: val_loss did not improve from 2708.67358\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2818.8782 - mse: 30607770.0000 - mae: 2819.2021 - val_loss: 2930.6465 - val_mse: 32622656.0000 - val_mae: 2930.9021 - lr: 5.0000e-04\n",
      "Epoch 65/500\n",
      "415/436 [===========================>..] - ETA: 0s - loss: 2781.2405 - mse: 30018442.0000 - mae: 2781.5540\n",
      "Epoch 65: val_loss did not improve from 2708.67358\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2796.1191 - mse: 30161066.0000 - mae: 2796.4343 - val_loss: 2716.9087 - val_mse: 29833980.0000 - val_mae: 2717.2100 - lr: 5.0000e-04\n",
      "Epoch 66/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2761.0408 - mse: 29552512.0000 - mae: 2761.3457\n",
      "Epoch 66: val_loss did not improve from 2708.67358\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2762.4653 - mse: 29622620.0000 - mae: 2762.7703 - val_loss: 2739.5027 - val_mse: 30857368.0000 - val_mae: 2739.7986 - lr: 5.0000e-04\n",
      "Epoch 67/500\n",
      "420/436 [===========================>..] - ETA: 0s - loss: 2778.9773 - mse: 29745066.0000 - mae: 2779.2852\n",
      "Epoch 67: val_loss improved from 2708.67358 to 2682.94141, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2799.5989 - mse: 30104632.0000 - mae: 2799.9075 - val_loss: 2682.9414 - val_mse: 29121988.0000 - val_mae: 2683.2456 - lr: 5.0000e-04\n",
      "Epoch 68/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2755.3201 - mse: 29662026.0000 - mae: 2755.6233\n",
      "Epoch 68: val_loss improved from 2682.94141 to 2653.78125, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2764.4617 - mse: 29815472.0000 - mae: 2764.7656 - val_loss: 2653.7812 - val_mse: 28883614.0000 - val_mae: 2654.0850 - lr: 5.0000e-04\n",
      "Epoch 69/500\n",
      "417/436 [===========================>..] - ETA: 0s - loss: 2763.0852 - mse: 29583578.0000 - mae: 2763.3867\n",
      "Epoch 69: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2781.9170 - mse: 29872696.0000 - mae: 2782.2205 - val_loss: 2811.3699 - val_mse: 31099498.0000 - val_mae: 2811.6658 - lr: 5.0000e-04\n",
      "Epoch 70/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2800.0823 - mse: 29854342.0000 - mae: 2800.3872\n",
      "Epoch 70: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2808.7217 - mse: 29961650.0000 - mae: 2809.0276 - val_loss: 2804.1777 - val_mse: 29558094.0000 - val_mae: 2804.5083 - lr: 5.0000e-04\n",
      "Epoch 71/500\n",
      "415/436 [===========================>..] - ETA: 0s - loss: 2788.1355 - mse: 29981082.0000 - mae: 2788.4412\n",
      "Epoch 71: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2816.3333 - mse: 30364158.0000 - mae: 2816.6411 - val_loss: 2837.8040 - val_mse: 30002890.0000 - val_mae: 2838.1033 - lr: 5.0000e-04\n",
      "Epoch 72/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2860.1707 - mse: 30763368.0000 - mae: 2860.4790\n",
      "Epoch 72: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2858.7639 - mse: 30738714.0000 - mae: 2859.0725 - val_loss: 2707.8535 - val_mse: 30476646.0000 - val_mae: 2708.1477 - lr: 5.0000e-04\n",
      "Epoch 73/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2808.1401 - mse: 30009414.0000 - mae: 2808.4502\n",
      "Epoch 73: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2831.2886 - mse: 30512214.0000 - mae: 2831.5986 - val_loss: 2806.8193 - val_mse: 31225200.0000 - val_mae: 2807.1233 - lr: 5.0000e-04\n",
      "Epoch 74/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2967.8464 - mse: 32189370.0000 - mae: 2968.1584\n",
      "Epoch 74: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2967.8464 - mse: 32189370.0000 - mae: 2968.1584 - val_loss: 2760.8467 - val_mse: 30004918.0000 - val_mae: 2761.1487 - lr: 5.0000e-04\n",
      "Epoch 75/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 2899.4900 - mse: 31948144.0000 - mae: 2899.7974\n",
      "Epoch 75: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2906.1387 - mse: 31965420.0000 - mae: 2906.4470 - val_loss: 2657.5352 - val_mse: 28981650.0000 - val_mae: 2657.8386 - lr: 5.0000e-04\n",
      "Epoch 76/500\n",
      "420/436 [===========================>..] - ETA: 0s - loss: 2839.3125 - mse: 30435168.0000 - mae: 2839.6208\n",
      "Epoch 76: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2848.0115 - mse: 30515220.0000 - mae: 2848.3206 - val_loss: 2924.9204 - val_mse: 32825702.0000 - val_mae: 2925.2283 - lr: 5.0000e-04\n",
      "Epoch 77/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2815.5261 - mse: 29633146.0000 - mae: 2815.8362\n",
      "Epoch 77: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2832.7556 - mse: 29924192.0000 - mae: 2833.0667 - val_loss: 2740.3489 - val_mse: 29591274.0000 - val_mae: 2740.6550 - lr: 5.0000e-04\n",
      "Epoch 78/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2793.9443 - mse: 29305250.0000 - mae: 2794.2542\n",
      "Epoch 78: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2793.9443 - mse: 29305250.0000 - mae: 2794.2542 - val_loss: 2717.4934 - val_mse: 29947514.0000 - val_mae: 2717.7927 - lr: 5.0000e-04\n",
      "Epoch 79/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2762.6372 - mse: 29532784.0000 - mae: 2762.9441\n",
      "Epoch 79: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2762.6372 - mse: 29532784.0000 - mae: 2762.9441 - val_loss: 2673.8643 - val_mse: 29097334.0000 - val_mae: 2674.1548 - lr: 2.5000e-04\n",
      "Epoch 80/500\n",
      "421/436 [===========================>..] - ETA: 0s - loss: 2739.7419 - mse: 28958970.0000 - mae: 2740.0457\n",
      "Epoch 80: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2746.7573 - mse: 29105760.0000 - mae: 2747.0618 - val_loss: 2695.6077 - val_mse: 29939474.0000 - val_mae: 2695.9187 - lr: 2.5000e-04\n",
      "Epoch 81/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2723.3235 - mse: 28423594.0000 - mae: 2723.6338\n",
      "Epoch 81: val_loss did not improve from 2653.78125\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2733.1694 - mse: 28563276.0000 - mae: 2733.4802 - val_loss: 2683.8022 - val_mse: 29068676.0000 - val_mae: 2684.1111 - lr: 2.5000e-04\n",
      "Epoch 82/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2676.0537 - mse: 28143654.0000 - mae: 2676.3643\n",
      "Epoch 82: val_loss improved from 2653.78125 to 2515.82153, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2678.4617 - mse: 28195772.0000 - mae: 2678.7722 - val_loss: 2515.8215 - val_mse: 26937246.0000 - val_mae: 2516.1191 - lr: 2.5000e-04\n",
      "Epoch 83/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2645.9197 - mse: 27503488.0000 - mae: 2646.2336\n",
      "Epoch 83: val_loss did not improve from 2515.82153\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2646.3513 - mse: 27529012.0000 - mae: 2646.6655 - val_loss: 2572.0732 - val_mse: 27202656.0000 - val_mae: 2572.3799 - lr: 2.5000e-04\n",
      "Epoch 84/500\n",
      "418/436 [===========================>..] - ETA: 0s - loss: 2612.2805 - mse: 26956506.0000 - mae: 2612.5908\n",
      "Epoch 84: val_loss did not improve from 2515.82153\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2637.4639 - mse: 27259908.0000 - mae: 2637.7759 - val_loss: 2566.1504 - val_mse: 26533760.0000 - val_mae: 2566.4595 - lr: 2.5000e-04\n",
      "Epoch 85/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2671.9985 - mse: 28005006.0000 - mae: 2672.3147\n",
      "Epoch 85: val_loss did not improve from 2515.82153\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2676.9106 - mse: 28056000.0000 - mae: 2677.2268 - val_loss: 2545.2490 - val_mse: 27090696.0000 - val_mae: 2545.5535 - lr: 2.5000e-04\n",
      "Epoch 86/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 2627.4521 - mse: 27359360.0000 - mae: 2627.7612\n",
      "Epoch 86: val_loss did not improve from 2515.82153\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2646.3789 - mse: 27655622.0000 - mae: 2646.6880 - val_loss: 2626.3335 - val_mse: 28003592.0000 - val_mae: 2626.6304 - lr: 2.5000e-04\n",
      "Epoch 87/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2675.0872 - mse: 27571224.0000 - mae: 2675.3972\n",
      "Epoch 87: val_loss did not improve from 2515.82153\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2680.0696 - mse: 27695310.0000 - mae: 2680.3804 - val_loss: 2539.2688 - val_mse: 26826588.0000 - val_mae: 2539.5732 - lr: 2.5000e-04\n",
      "Epoch 88/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2611.9050 - mse: 26932844.0000 - mae: 2612.2107\n",
      "Epoch 88: val_loss did not improve from 2515.82153\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2615.0615 - mse: 26969912.0000 - mae: 2615.3679 - val_loss: 2524.4963 - val_mse: 26386164.0000 - val_mae: 2524.7966 - lr: 2.5000e-04\n",
      "Epoch 89/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2627.4141 - mse: 26887322.0000 - mae: 2627.7197\n",
      "Epoch 89: val_loss improved from 2515.82153 to 2504.59082, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2627.8491 - mse: 26876464.0000 - mae: 2628.1548 - val_loss: 2504.5908 - val_mse: 26340868.0000 - val_mae: 2504.8823 - lr: 2.5000e-04\n",
      "Epoch 90/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2588.3838 - mse: 26391372.0000 - mae: 2588.6895\n",
      "Epoch 90: val_loss did not improve from 2504.59082\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2594.4749 - mse: 26529856.0000 - mae: 2594.7810 - val_loss: 2505.1826 - val_mse: 25540566.0000 - val_mae: 2505.4893 - lr: 2.5000e-04\n",
      "Epoch 91/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2573.0129 - mse: 25933154.0000 - mae: 2573.3196\n",
      "Epoch 91: val_loss did not improve from 2504.59082\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2580.0632 - mse: 26038428.0000 - mae: 2580.3701 - val_loss: 2565.2847 - val_mse: 26171732.0000 - val_mae: 2565.5869 - lr: 2.5000e-04\n",
      "Epoch 92/500\n",
      "418/436 [===========================>..] - ETA: 0s - loss: 2597.5664 - mse: 26011896.0000 - mae: 2597.8713\n",
      "Epoch 92: val_loss improved from 2504.59082 to 2439.28442, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2637.6614 - mse: 26721624.0000 - mae: 2637.9688 - val_loss: 2439.2844 - val_mse: 24940306.0000 - val_mae: 2439.5894 - lr: 2.5000e-04\n",
      "Epoch 93/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2565.6890 - mse: 25968972.0000 - mae: 2565.9963\n",
      "Epoch 93: val_loss did not improve from 2439.28442\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2567.7830 - mse: 25940272.0000 - mae: 2568.0908 - val_loss: 2509.9404 - val_mse: 26062778.0000 - val_mae: 2510.2234 - lr: 2.5000e-04\n",
      "Epoch 94/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2552.7654 - mse: 25356084.0000 - mae: 2553.0706\n",
      "Epoch 94: val_loss did not improve from 2439.28442\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2565.7839 - mse: 25530922.0000 - mae: 2566.0898 - val_loss: 2555.2488 - val_mse: 25459766.0000 - val_mae: 2555.5479 - lr: 2.5000e-04\n",
      "Epoch 95/500\n",
      "420/436 [===========================>..] - ETA: 0s - loss: 2500.3745 - mse: 24127930.0000 - mae: 2500.6768\n",
      "Epoch 95: val_loss did not improve from 2439.28442\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2522.7820 - mse: 24448272.0000 - mae: 2523.0859 - val_loss: 2542.6699 - val_mse: 25928164.0000 - val_mae: 2542.9795 - lr: 2.5000e-04\n",
      "Epoch 96/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2552.0359 - mse: 24965200.0000 - mae: 2552.3386\n",
      "Epoch 96: val_loss did not improve from 2439.28442\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2571.1196 - mse: 25292060.0000 - mae: 2571.4231 - val_loss: 2464.5425 - val_mse: 24496476.0000 - val_mae: 2464.8516 - lr: 2.5000e-04\n",
      "Epoch 97/500\n",
      "418/436 [===========================>..] - ETA: 0s - loss: 2486.8857 - mse: 24006020.0000 - mae: 2487.1851\n",
      "Epoch 97: val_loss improved from 2439.28442 to 2417.05737, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2519.6418 - mse: 24612638.0000 - mae: 2519.9434 - val_loss: 2417.0574 - val_mse: 24691344.0000 - val_mae: 2417.3813 - lr: 2.5000e-04\n",
      "Epoch 98/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2534.6729 - mse: 25065830.0000 - mae: 2534.9797\n",
      "Epoch 98: val_loss improved from 2417.05737 to 2373.05933, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2538.9268 - mse: 25158200.0000 - mae: 2539.2336 - val_loss: 2373.0593 - val_mse: 23446772.0000 - val_mae: 2373.3386 - lr: 2.5000e-04\n",
      "Epoch 99/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 2545.0447 - mse: 24989086.0000 - mae: 2545.3501\n",
      "Epoch 99: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2555.1667 - mse: 25154220.0000 - mae: 2555.4736 - val_loss: 2482.2849 - val_mse: 25023310.0000 - val_mae: 2482.5884 - lr: 2.5000e-04\n",
      "Epoch 100/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2555.5808 - mse: 25343914.0000 - mae: 2555.8892\n",
      "Epoch 100: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2559.2766 - mse: 25333392.0000 - mae: 2559.5842 - val_loss: 2476.8779 - val_mse: 24963998.0000 - val_mae: 2477.1401 - lr: 2.5000e-04\n",
      "Epoch 101/500\n",
      "419/436 [===========================>..] - ETA: 0s - loss: 2524.1428 - mse: 24607744.0000 - mae: 2524.4458\n",
      "Epoch 101: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2538.4541 - mse: 24854578.0000 - mae: 2538.7585 - val_loss: 2471.3447 - val_mse: 24802588.0000 - val_mae: 2471.6509 - lr: 2.5000e-04\n",
      "Epoch 102/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 2524.1633 - mse: 24622496.0000 - mae: 2524.4709\n",
      "Epoch 102: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2547.4675 - mse: 24958766.0000 - mae: 2547.7759 - val_loss: 2455.4802 - val_mse: 24565444.0000 - val_mae: 2455.7732 - lr: 2.5000e-04\n",
      "Epoch 103/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2513.5181 - mse: 24779868.0000 - mae: 2513.8210\n",
      "Epoch 103: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2517.8994 - mse: 24816352.0000 - mae: 2518.2031 - val_loss: 2404.6665 - val_mse: 23615852.0000 - val_mae: 2404.9734 - lr: 2.5000e-04\n",
      "Epoch 104/500\n",
      "419/436 [===========================>..] - ETA: 0s - loss: 2501.3208 - mse: 24519968.0000 - mae: 2501.6287\n",
      "Epoch 104: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2522.8608 - mse: 24859300.0000 - mae: 2523.1699 - val_loss: 2493.3376 - val_mse: 24998542.0000 - val_mae: 2493.6396 - lr: 2.5000e-04\n",
      "Epoch 105/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2539.3164 - mse: 25408890.0000 - mae: 2539.6233\n",
      "Epoch 105: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2547.4731 - mse: 25488598.0000 - mae: 2547.7810 - val_loss: 2435.1531 - val_mse: 24282346.0000 - val_mae: 2435.4490 - lr: 2.5000e-04\n",
      "Epoch 106/500\n",
      "417/436 [===========================>..] - ETA: 0s - loss: 2477.4478 - mse: 24072944.0000 - mae: 2477.7542\n",
      "Epoch 106: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2495.7000 - mse: 24319382.0000 - mae: 2496.0071 - val_loss: 2436.4736 - val_mse: 24018672.0000 - val_mae: 2436.7590 - lr: 2.5000e-04\n",
      "Epoch 107/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2511.1033 - mse: 24706482.0000 - mae: 2511.4084\n",
      "Epoch 107: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2514.1025 - mse: 24751996.0000 - mae: 2514.4082 - val_loss: 2436.2339 - val_mse: 24135302.0000 - val_mae: 2436.5161 - lr: 2.5000e-04\n",
      "Epoch 108/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2528.1316 - mse: 24550656.0000 - mae: 2528.4419\n",
      "Epoch 108: val_loss did not improve from 2373.05933\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2528.1316 - mse: 24550656.0000 - mae: 2528.4419 - val_loss: 2509.7812 - val_mse: 26277846.0000 - val_mae: 2510.0737 - lr: 2.5000e-04\n",
      "Epoch 109/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2524.0654 - mse: 24934644.0000 - mae: 2524.3716\n",
      "Epoch 109: val_loss improved from 2373.05933 to 2346.00269, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2527.1040 - mse: 24977096.0000 - mae: 2527.4102 - val_loss: 2346.0027 - val_mse: 23382698.0000 - val_mae: 2346.3105 - lr: 1.2500e-04\n",
      "Epoch 110/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2501.9810 - mse: 24586836.0000 - mae: 2502.2847\n",
      "Epoch 110: val_loss did not improve from 2346.00269\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2505.1003 - mse: 24647344.0000 - mae: 2505.4043 - val_loss: 2407.4971 - val_mse: 24153824.0000 - val_mae: 2407.8083 - lr: 1.2500e-04\n",
      "Epoch 111/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2529.8435 - mse: 25134108.0000 - mae: 2530.1479\n",
      "Epoch 111: val_loss did not improve from 2346.00269\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2534.6013 - mse: 25221130.0000 - mae: 2534.9060 - val_loss: 2381.0730 - val_mse: 24147468.0000 - val_mae: 2381.3818 - lr: 1.2500e-04\n",
      "Epoch 112/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2515.9998 - mse: 25132638.0000 - mae: 2516.3015\n",
      "Epoch 112: val_loss did not improve from 2346.00269\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2530.1497 - mse: 25377654.0000 - mae: 2530.4524 - val_loss: 2361.1685 - val_mse: 23607694.0000 - val_mae: 2361.4841 - lr: 1.2500e-04\n",
      "Epoch 113/500\n",
      "424/436 [============================>.] - ETA: 0s - loss: 2478.5811 - mse: 24446308.0000 - mae: 2478.8840\n",
      "Epoch 113: val_loss did not improve from 2346.00269\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2494.8616 - mse: 24636964.0000 - mae: 2495.1660 - val_loss: 2398.8840 - val_mse: 23900252.0000 - val_mae: 2399.1895 - lr: 1.2500e-04\n",
      "Epoch 114/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2457.1294 - mse: 23693604.0000 - mae: 2457.4316\n",
      "Epoch 114: val_loss did not improve from 2346.00269\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2466.1731 - mse: 23886232.0000 - mae: 2466.4761 - val_loss: 2414.1550 - val_mse: 24269000.0000 - val_mae: 2414.4685 - lr: 1.2500e-04\n",
      "Epoch 115/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2462.2029 - mse: 24312272.0000 - mae: 2462.5061\n",
      "Epoch 115: val_loss did not improve from 2346.00269\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2468.5693 - mse: 24383354.0000 - mae: 2468.8728 - val_loss: 2346.0691 - val_mse: 23934492.0000 - val_mae: 2346.3689 - lr: 1.2500e-04\n",
      "Epoch 116/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 2420.6296 - mse: 23061466.0000 - mae: 2420.9285\n",
      "Epoch 116: val_loss improved from 2346.00269 to 2307.04663, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2440.6150 - mse: 23252160.0000 - mae: 2440.9160 - val_loss: 2307.0466 - val_mse: 22590138.0000 - val_mae: 2307.3484 - lr: 1.2500e-04\n",
      "Epoch 117/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2414.7302 - mse: 23258944.0000 - mae: 2415.0339\n",
      "Epoch 117: val_loss did not improve from 2307.04663\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2428.9639 - mse: 23434282.0000 - mae: 2429.2686 - val_loss: 2317.3252 - val_mse: 23057252.0000 - val_mae: 2317.6240 - lr: 1.2500e-04\n",
      "Epoch 118/500\n",
      "419/436 [===========================>..] - ETA: 0s - loss: 2397.6201 - mse: 22804852.0000 - mae: 2397.9238\n",
      "Epoch 118: val_loss did not improve from 2307.04663\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2421.1365 - mse: 23126208.0000 - mae: 2421.4419 - val_loss: 2323.3333 - val_mse: 23110510.0000 - val_mae: 2323.6331 - lr: 1.2500e-04\n",
      "Epoch 119/500\n",
      "419/436 [===========================>..] - ETA: 0s - loss: 2422.5332 - mse: 23081078.0000 - mae: 2422.8345\n",
      "Epoch 119: val_loss did not improve from 2307.04663\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2436.8533 - mse: 23262106.0000 - mae: 2437.1560 - val_loss: 2336.8723 - val_mse: 22898558.0000 - val_mae: 2337.1738 - lr: 1.2500e-04\n",
      "Epoch 120/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 2379.3406 - mse: 22302076.0000 - mae: 2379.6438\n",
      "Epoch 120: val_loss did not improve from 2307.04663\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2400.6570 - mse: 22616726.0000 - mae: 2400.9612 - val_loss: 2322.6924 - val_mse: 22693570.0000 - val_mae: 2323.0032 - lr: 1.2500e-04\n",
      "Epoch 121/500\n",
      "421/436 [===========================>..] - ETA: 0s - loss: 2414.2285 - mse: 22917212.0000 - mae: 2414.5300\n",
      "Epoch 121: val_loss improved from 2307.04663 to 2306.29858, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2434.2136 - mse: 23149062.0000 - mae: 2434.5166 - val_loss: 2306.2986 - val_mse: 22251856.0000 - val_mae: 2306.6025 - lr: 1.2500e-04\n",
      "Epoch 122/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2440.0234 - mse: 23214996.0000 - mae: 2440.3271\n",
      "Epoch 122: val_loss did not improve from 2306.29858\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2453.5459 - mse: 23434280.0000 - mae: 2453.8499 - val_loss: 2332.7952 - val_mse: 22792614.0000 - val_mae: 2333.0999 - lr: 1.2500e-04\n",
      "Epoch 123/500\n",
      "416/436 [===========================>..] - ETA: 0s - loss: 2414.8052 - mse: 23185874.0000 - mae: 2415.1074\n",
      "Epoch 123: val_loss did not improve from 2306.29858\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2452.3184 - mse: 23750968.0000 - mae: 2452.6235 - val_loss: 2324.0330 - val_mse: 22842330.0000 - val_mae: 2324.3474 - lr: 1.2500e-04\n",
      "Epoch 124/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2430.4026 - mse: 23538752.0000 - mae: 2430.7031\n",
      "Epoch 124: val_loss improved from 2306.29858 to 2296.55713, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2446.5605 - mse: 23824064.0000 - mae: 2446.8621 - val_loss: 2296.5571 - val_mse: 21874366.0000 - val_mae: 2296.8623 - lr: 1.2500e-04\n",
      "Epoch 125/500\n",
      "414/436 [===========================>..] - ETA: 0s - loss: 2407.3357 - mse: 23065446.0000 - mae: 2407.6357\n",
      "Epoch 125: val_loss did not improve from 2296.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2425.3188 - mse: 23361108.0000 - mae: 2425.6201 - val_loss: 2297.4961 - val_mse: 22642804.0000 - val_mae: 2297.7966 - lr: 1.2500e-04\n",
      "Epoch 126/500\n",
      "415/436 [===========================>..] - ETA: 0s - loss: 2358.7678 - mse: 22411546.0000 - mae: 2359.0698\n",
      "Epoch 126: val_loss did not improve from 2296.55713\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2394.5457 - mse: 23040924.0000 - mae: 2394.8503 - val_loss: 2347.2747 - val_mse: 23220422.0000 - val_mae: 2347.6030 - lr: 1.2500e-04\n",
      "Epoch 127/500\n",
      "417/436 [===========================>..] - ETA: 0s - loss: 2356.6465 - mse: 22207962.0000 - mae: 2356.9495\n",
      "Epoch 127: val_loss improved from 2296.55713 to 2271.56152, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2372.0642 - mse: 22498686.0000 - mae: 2372.3687 - val_loss: 2271.5615 - val_mse: 22460754.0000 - val_mae: 2271.8757 - lr: 1.2500e-04\n",
      "Epoch 128/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2344.2087 - mse: 22147306.0000 - mae: 2344.5125\n",
      "Epoch 128: val_loss did not improve from 2271.56152\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2350.8342 - mse: 22288784.0000 - mae: 2351.1389 - val_loss: 2291.8071 - val_mse: 22926128.0000 - val_mae: 2292.1150 - lr: 1.2500e-04\n",
      "Epoch 129/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2410.2437 - mse: 23215876.0000 - mae: 2410.5491\n",
      "Epoch 129: val_loss did not improve from 2271.56152\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2415.0222 - mse: 23246520.0000 - mae: 2415.3284 - val_loss: 2332.6223 - val_mse: 23256848.0000 - val_mae: 2332.9165 - lr: 1.2500e-04\n",
      "Epoch 130/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2387.3433 - mse: 22523658.0000 - mae: 2387.6465\n",
      "Epoch 130: val_loss improved from 2271.56152 to 2253.91260, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2386.5442 - mse: 22485148.0000 - mae: 2386.8477 - val_loss: 2253.9126 - val_mse: 21973768.0000 - val_mae: 2254.2131 - lr: 1.2500e-04\n",
      "Epoch 131/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2348.7078 - mse: 21902046.0000 - mae: 2349.0098\n",
      "Epoch 131: val_loss did not improve from 2253.91260\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2361.3186 - mse: 22056364.0000 - mae: 2361.6216 - val_loss: 2276.4827 - val_mse: 22362186.0000 - val_mae: 2276.7886 - lr: 1.2500e-04\n",
      "Epoch 132/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2358.2070 - mse: 22105288.0000 - mae: 2358.5120\n",
      "Epoch 132: val_loss improved from 2253.91260 to 2253.24927, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2380.1746 - mse: 22415894.0000 - mae: 2380.4810 - val_loss: 2253.2493 - val_mse: 21725282.0000 - val_mae: 2253.5461 - lr: 1.2500e-04\n",
      "Epoch 133/500\n",
      "421/436 [===========================>..] - ETA: 0s - loss: 2358.0945 - mse: 22255850.0000 - mae: 2358.3987\n",
      "Epoch 133: val_loss did not improve from 2253.24927\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2373.2141 - mse: 22379612.0000 - mae: 2373.5198 - val_loss: 2320.6990 - val_mse: 22640824.0000 - val_mae: 2321.0063 - lr: 1.2500e-04\n",
      "Epoch 134/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2321.5286 - mse: 21329142.0000 - mae: 2321.8325\n",
      "Epoch 134: val_loss improved from 2253.24927 to 2253.15698, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2339.7156 - mse: 21651758.0000 - mae: 2340.0200 - val_loss: 2253.1570 - val_mse: 21630348.0000 - val_mae: 2253.4563 - lr: 1.2500e-04\n",
      "Epoch 135/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2387.2437 - mse: 22701284.0000 - mae: 2387.5442\n",
      "Epoch 135: val_loss did not improve from 2253.15698\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2388.8086 - mse: 22730952.0000 - mae: 2389.1094 - val_loss: 2318.9705 - val_mse: 22228876.0000 - val_mae: 2319.2837 - lr: 1.2500e-04\n",
      "Epoch 136/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2383.9021 - mse: 22337130.0000 - mae: 2384.2009\n",
      "Epoch 136: val_loss did not improve from 2253.15698\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2408.5300 - mse: 22736934.0000 - mae: 2408.8298 - val_loss: 2312.7212 - val_mse: 22159462.0000 - val_mae: 2313.0242 - lr: 1.2500e-04\n",
      "Epoch 137/500\n",
      "423/436 [============================>.] - ETA: 0s - loss: 2357.0244 - mse: 22285994.0000 - mae: 2357.3242\n",
      "Epoch 137: val_loss did not improve from 2253.15698\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2377.1089 - mse: 22596344.0000 - mae: 2377.4102 - val_loss: 2284.5942 - val_mse: 22070712.0000 - val_mae: 2284.9136 - lr: 1.2500e-04\n",
      "Epoch 138/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2369.4810 - mse: 22469848.0000 - mae: 2369.7830\n",
      "Epoch 138: val_loss did not improve from 2253.15698\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2372.9226 - mse: 22580594.0000 - mae: 2373.2244 - val_loss: 2258.0703 - val_mse: 21318858.0000 - val_mae: 2258.3765 - lr: 1.2500e-04\n",
      "Epoch 139/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2377.8271 - mse: 22330356.0000 - mae: 2378.1277\n",
      "Epoch 139: val_loss improved from 2253.15698 to 2252.90259, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2391.2456 - mse: 22532894.0000 - mae: 2391.5474 - val_loss: 2252.9026 - val_mse: 20895472.0000 - val_mae: 2253.2170 - lr: 1.2500e-04\n",
      "Epoch 140/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2358.1206 - mse: 22249776.0000 - mae: 2358.4238\n",
      "Epoch 140: val_loss did not improve from 2252.90259\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2365.2607 - mse: 22337134.0000 - mae: 2365.5642 - val_loss: 2293.1616 - val_mse: 22652970.0000 - val_mae: 2293.4663 - lr: 1.2500e-04\n",
      "Epoch 141/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2373.4805 - mse: 22318952.0000 - mae: 2373.7825\n",
      "Epoch 141: val_loss improved from 2252.90259 to 2225.17993, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2380.0918 - mse: 22482098.0000 - mae: 2380.3948 - val_loss: 2225.1799 - val_mse: 21172370.0000 - val_mae: 2225.4795 - lr: 1.2500e-04\n",
      "Epoch 142/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2397.5669 - mse: 22673404.0000 - mae: 2397.8704\n",
      "Epoch 142: val_loss did not improve from 2225.17993\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2397.5669 - mse: 22673404.0000 - mae: 2397.8704 - val_loss: 2255.7209 - val_mse: 21217936.0000 - val_mae: 2256.0308 - lr: 1.2500e-04\n",
      "Epoch 143/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2365.8298 - mse: 22508032.0000 - mae: 2366.1328\n",
      "Epoch 143: val_loss improved from 2225.17993 to 2208.84253, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2362.8892 - mse: 22465162.0000 - mae: 2363.1921 - val_loss: 2208.8425 - val_mse: 21151612.0000 - val_mae: 2209.1570 - lr: 1.2500e-04\n",
      "Epoch 144/500\n",
      "417/436 [===========================>..] - ETA: 0s - loss: 2346.6353 - mse: 22144736.0000 - mae: 2346.9365\n",
      "Epoch 144: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2372.1787 - mse: 22551634.0000 - mae: 2372.4824 - val_loss: 2324.9717 - val_mse: 22812496.0000 - val_mae: 2325.2732 - lr: 1.2500e-04\n",
      "Epoch 145/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2362.4048 - mse: 22612832.0000 - mae: 2362.7095\n",
      "Epoch 145: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2373.0796 - mse: 22858746.0000 - mae: 2373.3850 - val_loss: 2253.7266 - val_mse: 22493572.0000 - val_mae: 2254.0298 - lr: 1.2500e-04\n",
      "Epoch 146/500\n",
      "421/436 [===========================>..] - ETA: 0s - loss: 2343.7524 - mse: 22158186.0000 - mae: 2344.0596\n",
      "Epoch 146: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2366.1997 - mse: 22489168.0000 - mae: 2366.5083 - val_loss: 2272.5718 - val_mse: 22207964.0000 - val_mae: 2272.8767 - lr: 1.2500e-04\n",
      "Epoch 147/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2343.9890 - mse: 22280706.0000 - mae: 2344.2976\n",
      "Epoch 147: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2339.3442 - mse: 22216584.0000 - mae: 2339.6536 - val_loss: 2295.5151 - val_mse: 22118296.0000 - val_mae: 2295.8345 - lr: 1.2500e-04\n",
      "Epoch 148/500\n",
      "419/436 [===========================>..] - ETA: 0s - loss: 2321.3298 - mse: 21634830.0000 - mae: 2321.6367\n",
      "Epoch 148: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2333.4563 - mse: 21772646.0000 - mae: 2333.7644 - val_loss: 2229.6794 - val_mse: 21859880.0000 - val_mae: 2229.9929 - lr: 1.2500e-04\n",
      "Epoch 149/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2321.6401 - mse: 21379006.0000 - mae: 2321.9438\n",
      "Epoch 149: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2336.6704 - mse: 21680140.0000 - mae: 2336.9753 - val_loss: 2223.1567 - val_mse: 21003690.0000 - val_mae: 2223.4751 - lr: 1.2500e-04\n",
      "Epoch 150/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2378.4797 - mse: 22368732.0000 - mae: 2378.7856\n",
      "Epoch 150: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2379.5349 - mse: 22391944.0000 - mae: 2379.8408 - val_loss: 2291.4626 - val_mse: 22377726.0000 - val_mae: 2291.7632 - lr: 1.2500e-04\n",
      "Epoch 151/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2364.9500 - mse: 22333348.0000 - mae: 2365.2537\n",
      "Epoch 151: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2370.1455 - mse: 22387670.0000 - mae: 2370.4492 - val_loss: 2270.3438 - val_mse: 21069566.0000 - val_mae: 2270.6536 - lr: 1.2500e-04\n",
      "Epoch 152/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2346.5273 - mse: 21763242.0000 - mae: 2346.8330\n",
      "Epoch 152: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2360.3000 - mse: 21943822.0000 - mae: 2360.6069 - val_loss: 2273.2292 - val_mse: 21489996.0000 - val_mae: 2273.5344 - lr: 1.2500e-04\n",
      "Epoch 153/500\n",
      "417/436 [===========================>..] - ETA: 0s - loss: 2336.3469 - mse: 21457584.0000 - mae: 2336.6504\n",
      "Epoch 153: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2358.9487 - mse: 21714988.0000 - mae: 2359.2549 - val_loss: 2241.6760 - val_mse: 21236816.0000 - val_mae: 2241.9863 - lr: 1.2500e-04\n",
      "Epoch 154/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2294.6597 - mse: 21052622.0000 - mae: 2294.9658\n",
      "Epoch 154: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2311.8557 - mse: 21330434.0000 - mae: 2312.1628 - val_loss: 2243.2986 - val_mse: 21453098.0000 - val_mae: 2243.5903 - lr: 6.2500e-05\n",
      "Epoch 155/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2314.6296 - mse: 21575612.0000 - mae: 2314.9338\n",
      "Epoch 155: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2327.5181 - mse: 21722722.0000 - mae: 2327.8232 - val_loss: 2247.7012 - val_mse: 21439894.0000 - val_mae: 2248.0000 - lr: 6.2500e-05\n",
      "Epoch 156/500\n",
      "424/436 [============================>.] - ETA: 0s - loss: 2272.0273 - mse: 20914350.0000 - mae: 2272.3325\n",
      "Epoch 156: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2295.6729 - mse: 21299598.0000 - mae: 2295.9790 - val_loss: 2235.1028 - val_mse: 21487540.0000 - val_mae: 2235.4019 - lr: 6.2500e-05\n",
      "Epoch 157/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2299.8689 - mse: 21178864.0000 - mae: 2300.1760\n",
      "Epoch 157: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2307.2952 - mse: 21329208.0000 - mae: 2307.6023 - val_loss: 2233.2961 - val_mse: 20994110.0000 - val_mae: 2233.5955 - lr: 6.2500e-05\n",
      "Epoch 158/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2280.9485 - mse: 20925122.0000 - mae: 2281.2527\n",
      "Epoch 158: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2302.9124 - mse: 21279500.0000 - mae: 2303.2178 - val_loss: 2228.4895 - val_mse: 21529684.0000 - val_mae: 2228.7930 - lr: 6.2500e-05\n",
      "Epoch 159/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2277.7900 - mse: 21125578.0000 - mae: 2278.0947\n",
      "Epoch 159: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2297.4814 - mse: 21470354.0000 - mae: 2297.7876 - val_loss: 2257.4570 - val_mse: 21405054.0000 - val_mae: 2257.7620 - lr: 6.2500e-05\n",
      "Epoch 160/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2289.3066 - mse: 21405476.0000 - mae: 2289.6130\n",
      "Epoch 160: val_loss did not improve from 2208.84253\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2299.2212 - mse: 21532738.0000 - mae: 2299.5283 - val_loss: 2210.8569 - val_mse: 21155804.0000 - val_mae: 2211.1567 - lr: 6.2500e-05\n",
      "Epoch 161/500\n",
      "424/436 [============================>.] - ETA: 0s - loss: 2252.7893 - mse: 20896138.0000 - mae: 2253.0942\n",
      "Epoch 161: val_loss improved from 2208.84253 to 2202.00098, saving model to new_stne_rnn_weight.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2270.2976 - mse: 21143620.0000 - mae: 2270.6033 - val_loss: 2202.0010 - val_mse: 20914374.0000 - val_mae: 2202.3013 - lr: 6.2500e-05\n",
      "Epoch 162/500\n",
      "424/436 [============================>.] - ETA: 0s - loss: 2259.9451 - mse: 20790926.0000 - mae: 2260.2512\n",
      "Epoch 162: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2286.8428 - mse: 21154648.0000 - mae: 2287.1504 - val_loss: 2206.6687 - val_mse: 20804716.0000 - val_mae: 2206.9634 - lr: 6.2500e-05\n",
      "Epoch 163/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2276.6299 - mse: 21000550.0000 - mae: 2276.9387\n",
      "Epoch 163: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2280.1729 - mse: 21059494.0000 - mae: 2280.4822 - val_loss: 2231.4172 - val_mse: 21342316.0000 - val_mae: 2231.7134 - lr: 6.2500e-05\n",
      "Epoch 164/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2269.5027 - mse: 20856070.0000 - mae: 2269.8086\n",
      "Epoch 164: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2285.8518 - mse: 21108254.0000 - mae: 2286.1584 - val_loss: 2221.0117 - val_mse: 21389404.0000 - val_mae: 2221.3052 - lr: 6.2500e-05\n",
      "Epoch 165/500\n",
      "420/436 [===========================>..] - ETA: 0s - loss: 2278.7561 - mse: 20908242.0000 - mae: 2279.0596\n",
      "Epoch 165: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2298.8699 - mse: 21211702.0000 - mae: 2299.1743 - val_loss: 2205.3657 - val_mse: 21073218.0000 - val_mae: 2205.6589 - lr: 6.2500e-05\n",
      "Epoch 166/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2301.0745 - mse: 21124860.0000 - mae: 2301.3828\n",
      "Epoch 166: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2311.0410 - mse: 21283118.0000 - mae: 2311.3503 - val_loss: 2210.5525 - val_mse: 21107128.0000 - val_mae: 2210.8528 - lr: 6.2500e-05\n",
      "Epoch 167/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2313.1636 - mse: 21405862.0000 - mae: 2313.4705\n",
      "Epoch 167: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2315.2996 - mse: 21442548.0000 - mae: 2315.6062 - val_loss: 2245.8545 - val_mse: 22168642.0000 - val_mae: 2246.1443 - lr: 6.2500e-05\n",
      "Epoch 168/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2307.7622 - mse: 21292320.0000 - mae: 2308.0696\n",
      "Epoch 168: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2316.4653 - mse: 21422108.0000 - mae: 2316.7737 - val_loss: 2203.5181 - val_mse: 21160518.0000 - val_mae: 2203.8198 - lr: 6.2500e-05\n",
      "Epoch 169/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2322.2236 - mse: 21448440.0000 - mae: 2322.5327\n",
      "Epoch 169: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2323.8201 - mse: 21466466.0000 - mae: 2324.1292 - val_loss: 2231.4148 - val_mse: 22228450.0000 - val_mae: 2231.7039 - lr: 6.2500e-05\n",
      "Epoch 170/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2280.3831 - mse: 21097202.0000 - mae: 2280.6914\n",
      "Epoch 170: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2281.7549 - mse: 21094870.0000 - mae: 2282.0635 - val_loss: 2207.3760 - val_mse: 21906784.0000 - val_mae: 2207.6707 - lr: 6.2500e-05\n",
      "Epoch 171/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2280.8025 - mse: 20999700.0000 - mae: 2281.1108\n",
      "Epoch 171: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2285.2180 - mse: 21037528.0000 - mae: 2285.5266 - val_loss: 2213.6450 - val_mse: 21592386.0000 - val_mae: 2213.9375 - lr: 6.2500e-05\n",
      "Epoch 172/500\n",
      "413/436 [===========================>..] - ETA: 0s - loss: 2298.5483 - mse: 21448746.0000 - mae: 2298.8557\n",
      "Epoch 172: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2319.4014 - mse: 21663970.0000 - mae: 2319.7097 - val_loss: 2202.4790 - val_mse: 21238146.0000 - val_mae: 2202.7673 - lr: 3.1250e-05\n",
      "Epoch 173/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2313.2661 - mse: 21451856.0000 - mae: 2313.5742\n",
      "Epoch 173: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2312.7800 - mse: 21440368.0000 - mae: 2313.0884 - val_loss: 2209.9243 - val_mse: 21318014.0000 - val_mae: 2210.2107 - lr: 3.1250e-05\n",
      "Epoch 174/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2289.2190 - mse: 20974064.0000 - mae: 2289.5256\n",
      "Epoch 174: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2313.6394 - mse: 21339528.0000 - mae: 2313.9470 - val_loss: 2205.2100 - val_mse: 21289244.0000 - val_mae: 2205.4951 - lr: 3.1250e-05\n",
      "Epoch 175/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2299.6479 - mse: 21304130.0000 - mae: 2299.9531\n",
      "Epoch 175: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2308.9358 - mse: 21427648.0000 - mae: 2309.2412 - val_loss: 2214.7544 - val_mse: 21657756.0000 - val_mae: 2215.0403 - lr: 3.1250e-05\n",
      "Epoch 176/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2306.1565 - mse: 21270280.0000 - mae: 2306.4622\n",
      "Epoch 176: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2311.1548 - mse: 21338740.0000 - mae: 2311.4607 - val_loss: 2226.9231 - val_mse: 21581726.0000 - val_mae: 2227.2090 - lr: 3.1250e-05\n",
      "Epoch 177/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2279.3345 - mse: 21087206.0000 - mae: 2279.6389\n",
      "Epoch 177: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2300.0334 - mse: 21327230.0000 - mae: 2300.3386 - val_loss: 2230.0808 - val_mse: 21690146.0000 - val_mae: 2230.3694 - lr: 3.1250e-05\n",
      "Epoch 178/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2317.6404 - mse: 21528718.0000 - mae: 2317.9453\n",
      "Epoch 178: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2322.2732 - mse: 21610226.0000 - mae: 2322.5786 - val_loss: 2226.2097 - val_mse: 21873008.0000 - val_mae: 2226.4929 - lr: 3.1250e-05\n",
      "Epoch 179/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2302.7949 - mse: 21345358.0000 - mae: 2303.0984\n",
      "Epoch 179: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2302.7949 - mse: 21345358.0000 - mae: 2303.0984 - val_loss: 2204.8220 - val_mse: 21324018.0000 - val_mae: 2205.1074 - lr: 3.1250e-05\n",
      "Epoch 180/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2285.6626 - mse: 21116864.0000 - mae: 2285.9663\n",
      "Epoch 180: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2296.5471 - mse: 21232638.0000 - mae: 2296.8511 - val_loss: 2213.1609 - val_mse: 21418012.0000 - val_mae: 2213.4468 - lr: 3.1250e-05\n",
      "Epoch 181/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2293.8787 - mse: 21112246.0000 - mae: 2294.1831\n",
      "Epoch 181: val_loss did not improve from 2202.00098\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2296.1750 - mse: 21183164.0000 - mae: 2296.4795 - val_loss: 2215.9080 - val_mse: 21464788.0000 - val_mae: 2216.1941 - lr: 3.1250e-05\n",
      "Epoch 1/500\n",
      "    427/Unknown - 1s 2ms/step - loss: 2286.9390 - mse: 21115980.0000 - mae: 2287.2439\n",
      "Epoch 1: val_loss improved from inf to 2197.67603, saving model to new_stne_rnn_weight_ns.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2305.4519 - mse: 21398612.0000 - mae: 2305.7576 - val_loss: 2197.6760 - val_mse: 20984406.0000 - val_mae: 2197.9612 - lr: 1.5625e-05\n",
      "Epoch 2/500\n",
      "420/436 [===========================>..] - ETA: 0s - loss: 2273.9944 - mse: 21033498.0000 - mae: 2274.2993\n",
      "Epoch 2: val_loss improved from 2197.67603 to 2190.68530, saving model to new_stne_rnn_weight_ns.h5\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2300.9456 - mse: 21411690.0000 - mae: 2301.2520 - val_loss: 2190.6853 - val_mse: 20931004.0000 - val_mae: 2190.9717 - lr: 1.5625e-05\n",
      "Epoch 3/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2274.8086 - mse: 21018644.0000 - mae: 2275.1125\n",
      "Epoch 3: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2296.0332 - mse: 21329262.0000 - mae: 2296.3381 - val_loss: 2194.2888 - val_mse: 20889404.0000 - val_mae: 2194.5740 - lr: 1.5625e-05\n",
      "Epoch 4/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2277.1997 - mse: 21048120.0000 - mae: 2277.5049\n",
      "Epoch 4: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2298.0291 - mse: 21346892.0000 - mae: 2298.3350 - val_loss: 2204.1809 - val_mse: 21219390.0000 - val_mae: 2204.4663 - lr: 1.5625e-05\n",
      "Epoch 5/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2279.8914 - mse: 21115974.0000 - mae: 2280.1965\n",
      "Epoch 5: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2300.8411 - mse: 21422192.0000 - mae: 2301.1465 - val_loss: 2204.5159 - val_mse: 21177228.0000 - val_mae: 2204.8005 - lr: 1.5625e-05\n",
      "Epoch 6/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2276.9470 - mse: 21067848.0000 - mae: 2277.2517\n",
      "Epoch 6: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2297.9526 - mse: 21374656.0000 - mae: 2298.2583 - val_loss: 2204.2554 - val_mse: 21202054.0000 - val_mae: 2204.5405 - lr: 1.5625e-05\n",
      "Epoch 7/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2276.8188 - mse: 21004998.0000 - mae: 2277.1240\n",
      "Epoch 7: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2292.8877 - mse: 21259486.0000 - mae: 2293.1936 - val_loss: 2210.8525 - val_mse: 21260668.0000 - val_mae: 2211.1389 - lr: 1.5625e-05\n",
      "Epoch 8/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2279.0588 - mse: 21033144.0000 - mae: 2279.3623\n",
      "Epoch 8: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2297.2239 - mse: 21291368.0000 - mae: 2297.5278 - val_loss: 2224.0483 - val_mse: 21608520.0000 - val_mae: 2224.3340 - lr: 1.5625e-05\n",
      "Epoch 9/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2293.9172 - mse: 21278496.0000 - mae: 2294.2219\n",
      "Epoch 9: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2293.9512 - mse: 21266034.0000 - mae: 2294.2559 - val_loss: 2224.2090 - val_mse: 21582590.0000 - val_mae: 2224.4932 - lr: 1.5625e-05\n",
      "Epoch 10/500\n",
      "424/436 [============================>.] - ETA: 0s - loss: 2259.2329 - mse: 20832432.0000 - mae: 2259.5386\n",
      "Epoch 10: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2280.8394 - mse: 21148920.0000 - mae: 2281.1458 - val_loss: 2213.5239 - val_mse: 21420828.0000 - val_mae: 2213.8088 - lr: 1.5625e-05\n",
      "Epoch 11/500\n",
      "417/436 [===========================>..] - ETA: 0s - loss: 2240.6826 - mse: 20707358.0000 - mae: 2240.9871\n",
      "Epoch 11: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2269.4241 - mse: 21106976.0000 - mae: 2269.7295 - val_loss: 2207.9678 - val_mse: 21172582.0000 - val_mae: 2208.2551 - lr: 1.5625e-05\n",
      "Epoch 12/500\n",
      "425/436 [============================>.] - ETA: 0s - loss: 2249.2786 - mse: 20742752.0000 - mae: 2249.5845\n",
      "Epoch 12: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2269.0994 - mse: 21030886.0000 - mae: 2269.4060 - val_loss: 2199.7361 - val_mse: 21053588.0000 - val_mae: 2200.0222 - lr: 1.5625e-05\n",
      "Epoch 13/500\n",
      "420/436 [===========================>..] - ETA: 0s - loss: 2249.7422 - mse: 20823938.0000 - mae: 2250.0466\n",
      "Epoch 13: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2274.1699 - mse: 21154516.0000 - mae: 2274.4753 - val_loss: 2205.1655 - val_mse: 21271952.0000 - val_mae: 2205.4529 - lr: 7.8125e-06\n",
      "Epoch 14/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2252.0593 - mse: 20781980.0000 - mae: 2252.3645\n",
      "Epoch 14: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2272.6714 - mse: 21085518.0000 - mae: 2272.9775 - val_loss: 2204.7720 - val_mse: 21224242.0000 - val_mae: 2205.0596 - lr: 7.8125e-06\n",
      "Epoch 15/500\n",
      "424/436 [============================>.] - ETA: 0s - loss: 2252.6509 - mse: 20756624.0000 - mae: 2252.9563\n",
      "Epoch 15: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2273.3494 - mse: 21054042.0000 - mae: 2273.6555 - val_loss: 2207.8215 - val_mse: 21306498.0000 - val_mae: 2208.1089 - lr: 7.8125e-06\n",
      "Epoch 16/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2259.8345 - mse: 20872372.0000 - mae: 2260.1394\n",
      "Epoch 16: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2268.7200 - mse: 20995488.0000 - mae: 2269.0251 - val_loss: 2209.8589 - val_mse: 21337568.0000 - val_mae: 2210.1479 - lr: 7.8125e-06\n",
      "Epoch 17/500\n",
      "424/436 [============================>.] - ETA: 0s - loss: 2252.9268 - mse: 20737834.0000 - mae: 2253.2310\n",
      "Epoch 17: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2273.8916 - mse: 21036712.0000 - mae: 2274.1968 - val_loss: 2205.4675 - val_mse: 21250328.0000 - val_mae: 2205.7566 - lr: 7.8125e-06\n",
      "Epoch 18/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2260.8774 - mse: 20850848.0000 - mae: 2261.1841\n",
      "Epoch 18: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2269.9402 - mse: 20978876.0000 - mae: 2270.2471 - val_loss: 2214.9800 - val_mse: 21417708.0000 - val_mae: 2215.2686 - lr: 7.8125e-06\n",
      "Epoch 19/500\n",
      "417/436 [===========================>..] - ETA: 0s - loss: 2244.6504 - mse: 20617042.0000 - mae: 2244.9551\n",
      "Epoch 19: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2271.1235 - mse: 20981626.0000 - mae: 2271.4297 - val_loss: 2216.2681 - val_mse: 21409544.0000 - val_mae: 2216.5564 - lr: 7.8125e-06\n",
      "Epoch 20/500\n",
      "422/436 [============================>.] - ETA: 0s - loss: 2254.0759 - mse: 20739638.0000 - mae: 2254.3823\n",
      "Epoch 20: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2271.3862 - mse: 20986208.0000 - mae: 2271.6936 - val_loss: 2215.3562 - val_mse: 21392204.0000 - val_mae: 2215.6440 - lr: 7.8125e-06\n",
      "Epoch 21/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2257.3694 - mse: 20777838.0000 - mae: 2257.6741\n",
      "Epoch 21: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2270.3005 - mse: 20978620.0000 - mae: 2270.6055 - val_loss: 2215.8799 - val_mse: 21477224.0000 - val_mae: 2216.1680 - lr: 7.8125e-06\n",
      "Epoch 22/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2247.5317 - mse: 20655704.0000 - mae: 2247.8362\n",
      "Epoch 22: val_loss did not improve from 2190.68530\n",
      "436/436 [==============================] - 1s 3ms/step - loss: 2267.5659 - mse: 20954256.0000 - mae: 2267.8713 - val_loss: 2216.0452 - val_mse: 21452378.0000 - val_mae: 2216.3333 - lr: 7.8125e-06\n"
     ]
    }
   ],
   "source": [
    "rnn_history = rnn_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_rnn])\n",
    "rnn_history_ns = rnn_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_rnn_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "    430/Unknown - 8s 6ms/step - loss: 7631.7407 - mse: 184187936.0000 - mae: 7632.0610\n",
      "Epoch 1: val_loss improved from inf to 7113.06445, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 9s 9ms/step - loss: 7622.6895 - mse: 183759776.0000 - mae: 7623.0093 - val_loss: 7113.0645 - val_mse: 171764960.0000 - val_mae: 7113.3652 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 7592.9219 - mse: 182782560.0000 - mae: 7593.2344\n",
      "Epoch 2: val_loss improved from 7113.06445 to 7078.98096, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 7595.4126 - mse: 182771616.0000 - mae: 7595.7246 - val_loss: 7078.9810 - val_mse: 170478352.0000 - val_mae: 7079.2886 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 7558.0161 - mse: 181367360.0000 - mae: 7558.3511\n",
      "Epoch 3: val_loss improved from 7078.98096 to 7023.45947, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 7546.7100 - mse: 181019600.0000 - mae: 7547.0439 - val_loss: 7023.4595 - val_mse: 168519024.0000 - val_mae: 7023.7676 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 7486.9351 - mse: 178769424.0000 - mae: 7487.2725\n",
      "Epoch 4: val_loss improved from 7023.45947 to 6951.26025, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 7481.4727 - mse: 178627088.0000 - mae: 7481.8096 - val_loss: 6951.2603 - val_mse: 166014432.0000 - val_mae: 6951.5918 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 7380.2036 - mse: 175256448.0000 - mae: 7380.5420\n",
      "Epoch 5: val_loss improved from 6951.26025 to 6869.49609, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 7391.8608 - mse: 175614256.0000 - mae: 7392.1992 - val_loss: 6869.4961 - val_mse: 162958320.0000 - val_mae: 6869.8032 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 7260.6953 - mse: 171542480.0000 - mae: 7261.0054\n",
      "Epoch 6: val_loss improved from 6869.49609 to 6771.86523, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 7285.8809 - mse: 172082688.0000 - mae: 7286.1919 - val_loss: 6771.8652 - val_mse: 159464224.0000 - val_mae: 6772.1558 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 7188.0981 - mse: 168344576.0000 - mae: 7188.4204\n",
      "Epoch 7: val_loss improved from 6771.86523 to 6686.02881, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 7186.8027 - mse: 168319536.0000 - mae: 7187.1255 - val_loss: 6686.0288 - val_mse: 155868048.0000 - val_mae: 6686.3779 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 7075.4966 - mse: 164246512.0000 - mae: 7075.8345\n",
      "Epoch 8: val_loss improved from 6686.02881 to 6560.69775, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 7080.2910 - mse: 164388352.0000 - mae: 7080.6294 - val_loss: 6560.6978 - val_mse: 151929696.0000 - val_mae: 6561.0225 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 6941.3564 - mse: 159950192.0000 - mae: 6941.6860\n",
      "Epoch 9: val_loss improved from 6560.69775 to 6427.74219, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 6940.3105 - mse: 159859712.0000 - mae: 6940.6401 - val_loss: 6427.7422 - val_mse: 147605392.0000 - val_mae: 6428.1675 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 6808.2739 - mse: 155132720.0000 - mae: 6808.6211\n",
      "Epoch 10: val_loss improved from 6427.74219 to 6302.76465, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 6814.9263 - mse: 155219296.0000 - mae: 6815.2744 - val_loss: 6302.7646 - val_mse: 143329984.0000 - val_mae: 6303.1260 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 6732.8916 - mse: 151989648.0000 - mae: 6733.2217\n",
      "Epoch 11: val_loss improved from 6302.76465 to 6190.40283, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 6735.4287 - mse: 152010512.0000 - mae: 6735.7603 - val_loss: 6190.4028 - val_mse: 139115440.0000 - val_mae: 6190.7202 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 6549.0195 - mse: 145633216.0000 - mae: 6549.3545\n",
      "Epoch 12: val_loss improved from 6190.40283 to 6121.98340, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 6557.4902 - mse: 145828704.0000 - mae: 6557.8257 - val_loss: 6121.9834 - val_mse: 134429888.0000 - val_mae: 6122.3433 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 6430.8955 - mse: 140930832.0000 - mae: 6431.2329\n",
      "Epoch 13: val_loss improved from 6121.98340 to 5925.80420, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 6430.8955 - mse: 140930832.0000 - mae: 6431.2329 - val_loss: 5925.8042 - val_mse: 129649360.0000 - val_mae: 5926.1216 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 6301.1807 - mse: 136426992.0000 - mae: 6301.5215\n",
      "Epoch 14: val_loss improved from 5925.80420 to 5780.13477, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 6286.3604 - mse: 135856368.0000 - mae: 6286.7007 - val_loss: 5780.1348 - val_mse: 124811144.0000 - val_mae: 5780.4551 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 6110.5532 - mse: 130576672.0000 - mae: 6110.8994\n",
      "Epoch 15: val_loss improved from 5780.13477 to 5646.85059, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 6108.1743 - mse: 130518952.0000 - mae: 6108.5200 - val_loss: 5646.8506 - val_mse: 119888944.0000 - val_mae: 5647.1499 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 5978.1899 - mse: 125554000.0000 - mae: 5978.5391\n",
      "Epoch 16: val_loss improved from 5646.85059 to 5567.63184, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 5969.7505 - mse: 125319400.0000 - mae: 5970.0996 - val_loss: 5567.6318 - val_mse: 115132824.0000 - val_mae: 5567.9883 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 5866.4805 - mse: 120282352.0000 - mae: 5866.8315\n",
      "Epoch 17: val_loss improved from 5567.63184 to 5442.10156, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 5864.8384 - mse: 120218960.0000 - mae: 5865.1890 - val_loss: 5442.1016 - val_mse: 110523584.0000 - val_mae: 5442.4624 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 5752.7188 - mse: 115548600.0000 - mae: 5753.0645\n",
      "Epoch 18: val_loss improved from 5442.10156 to 5326.05762, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 5751.9858 - mse: 115511288.0000 - mae: 5752.3311 - val_loss: 5326.0576 - val_mse: 106103752.0000 - val_mae: 5326.4331 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 5630.6460 - mse: 111541920.0000 - mae: 5630.9985\n",
      "Epoch 19: val_loss improved from 5326.05762 to 5135.33447, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 5630.6460 - mse: 111541920.0000 - mae: 5630.9985 - val_loss: 5135.3345 - val_mse: 101238184.0000 - val_mae: 5135.6816 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 5486.6904 - mse: 106521600.0000 - mae: 5487.0483\n",
      "Epoch 20: val_loss improved from 5135.33447 to 5068.00732, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 5476.0918 - mse: 106114568.0000 - mae: 5476.4487 - val_loss: 5068.0073 - val_mse: 96428120.0000 - val_mae: 5068.3682 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 5303.3267 - mse: 100494976.0000 - mae: 5303.6929\n",
      "Epoch 21: val_loss improved from 5068.00732 to 4891.79150, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 5313.1265 - mse: 100577496.0000 - mae: 5313.4917 - val_loss: 4891.7915 - val_mse: 91924384.0000 - val_mae: 4892.0713 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 5191.7173 - mse: 95811016.0000 - mae: 5192.0552\n",
      "Epoch 22: val_loss improved from 4891.79150 to 4792.64307, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 5191.5322 - mse: 95774792.0000 - mae: 5191.8701 - val_loss: 4792.6431 - val_mse: 87253912.0000 - val_mae: 4792.9253 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 5052.1528 - mse: 90927376.0000 - mae: 5052.4673\n",
      "Epoch 23: val_loss improved from 4792.64307 to 4699.31885, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 5044.3486 - mse: 90769704.0000 - mae: 5044.6641 - val_loss: 4699.3188 - val_mse: 83027984.0000 - val_mae: 4699.6021 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 4940.5244 - mse: 86534448.0000 - mae: 4940.8452\n",
      "Epoch 24: val_loss did not improve from 4699.31885\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 4942.4111 - mse: 86460240.0000 - mae: 4942.7329 - val_loss: 4749.2617 - val_mse: 80135376.0000 - val_mae: 4749.5205 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 4995.3994 - mse: 84267424.0000 - mae: 4995.7109\n",
      "Epoch 25: val_loss improved from 4699.31885 to 4512.57764, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 4994.8906 - mse: 84251408.0000 - mae: 4995.2031 - val_loss: 4512.5776 - val_mse: 75939968.0000 - val_mae: 4512.8652 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 4894.1401 - mse: 80388664.0000 - mae: 4894.4624\n",
      "Epoch 26: val_loss did not improve from 4512.57764\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 4898.7505 - mse: 80342912.0000 - mae: 4899.0732 - val_loss: 4532.3735 - val_mse: 73086344.0000 - val_mae: 4532.6958 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 4711.0859 - mse: 76182856.0000 - mae: 4711.4136\n",
      "Epoch 27: val_loss improved from 4512.57764 to 4304.64404, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 4710.2319 - mse: 76147408.0000 - mae: 4710.5596 - val_loss: 4304.6440 - val_mse: 68271312.0000 - val_mae: 4304.9111 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 4608.4639 - mse: 71649680.0000 - mae: 4608.7759\n",
      "Epoch 28: val_loss did not improve from 4304.64404\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 4615.8906 - mse: 71762040.0000 - mae: 4616.2026 - val_loss: 4767.7617 - val_mse: 70555776.0000 - val_mae: 4768.0215 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 4659.4487 - mse: 70166912.0000 - mae: 4659.7744\n",
      "Epoch 29: val_loss improved from 4304.64404 to 4109.50391, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 4662.4746 - mse: 70185616.0000 - mae: 4662.8008 - val_loss: 4109.5039 - val_mse: 61003068.0000 - val_mae: 4109.9717 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 4353.0991 - mse: 63977516.0000 - mae: 4353.4346\n",
      "Epoch 30: val_loss improved from 4109.50391 to 4028.43921, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 4350.8652 - mse: 63870052.0000 - mae: 4351.2007 - val_loss: 4028.4392 - val_mse: 57617824.0000 - val_mae: 4028.7329 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 4580.1387 - mse: 64919584.0000 - mae: 4580.4712\n",
      "Epoch 31: val_loss did not improve from 4028.43921\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 4579.1807 - mse: 64924568.0000 - mae: 4579.5127 - val_loss: 4520.4653 - val_mse: 62423780.0000 - val_mae: 4520.7559 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 4252.6460 - mse: 59084600.0000 - mae: 4252.9858\n",
      "Epoch 32: val_loss did not improve from 4028.43921\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 4253.5986 - mse: 59068560.0000 - mae: 4253.9390 - val_loss: 4102.0464 - val_mse: 56633708.0000 - val_mae: 4102.3442 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 4062.6108 - mse: 54983232.0000 - mae: 4062.9998\n",
      "Epoch 33: val_loss improved from 4028.43921 to 3892.89526, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 4068.0999 - mse: 55081224.0000 - mae: 4068.4883 - val_loss: 3892.8953 - val_mse: 52886492.0000 - val_mae: 3893.1445 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3956.5620 - mse: 52176292.0000 - mae: 3956.8918\n",
      "Epoch 34: val_loss improved from 3892.89526 to 3796.09155, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3959.0134 - mse: 52163076.0000 - mae: 3959.3433 - val_loss: 3796.0916 - val_mse: 48227236.0000 - val_mae: 3796.4290 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 3837.6694 - mse: 48734812.0000 - mae: 3838.0247\n",
      "Epoch 35: val_loss improved from 3796.09155 to 3607.22974, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3838.4214 - mse: 48704224.0000 - mae: 3838.7756 - val_loss: 3607.2297 - val_mse: 44426128.0000 - val_mae: 3607.5518 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 3701.5071 - mse: 45525772.0000 - mae: 3701.8516\n",
      "Epoch 36: val_loss did not improve from 3607.22974\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3706.8083 - mse: 45552152.0000 - mae: 3707.1538 - val_loss: 3692.2014 - val_mse: 44302796.0000 - val_mae: 3692.5083 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3715.5139 - mse: 44939292.0000 - mae: 3715.8708\n",
      "Epoch 37: val_loss improved from 3607.22974 to 3567.58789, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3720.3618 - mse: 44938648.0000 - mae: 3720.7180 - val_loss: 3567.5879 - val_mse: 42264820.0000 - val_mae: 3567.9204 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 3621.3218 - mse: 43385336.0000 - mae: 3621.6616\n",
      "Epoch 38: val_loss improved from 3567.58789 to 3543.37280, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3621.3218 - mse: 43385336.0000 - mae: 3621.6616 - val_loss: 3543.3728 - val_mse: 42724036.0000 - val_mae: 3543.7544 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3632.5623 - mse: 42693032.0000 - mae: 3632.9075\n",
      "Epoch 39: val_loss improved from 3543.37280 to 3425.02344, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3636.9602 - mse: 42800980.0000 - mae: 3637.3064 - val_loss: 3425.0234 - val_mse: 38719164.0000 - val_mae: 3425.3665 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 3471.8101 - mse: 38738812.0000 - mae: 3472.1677\n",
      "Epoch 40: val_loss did not improve from 3425.02344\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3468.3733 - mse: 38634604.0000 - mae: 3468.7310 - val_loss: 3463.6338 - val_mse: 37130068.0000 - val_mae: 3463.9192 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3502.6677 - mse: 39567156.0000 - mae: 3502.9971\n",
      "Epoch 41: val_loss did not improve from 3425.02344\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3502.8689 - mse: 39607076.0000 - mae: 3503.1987 - val_loss: 3530.2039 - val_mse: 39877384.0000 - val_mae: 3530.5649 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 3584.6616 - mse: 40632080.0000 - mae: 3584.9861\n",
      "Epoch 42: val_loss improved from 3425.02344 to 3371.07129, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3587.5122 - mse: 40661136.0000 - mae: 3587.8369 - val_loss: 3371.0713 - val_mse: 36727764.0000 - val_mae: 3371.4150 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 3458.3184 - mse: 37773040.0000 - mae: 3458.6453\n",
      "Epoch 43: val_loss did not improve from 3371.07129\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 3467.8162 - mse: 37810480.0000 - mae: 3468.1443 - val_loss: 3398.7368 - val_mse: 37657412.0000 - val_mae: 3399.0784 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 3398.2019 - mse: 36805088.0000 - mae: 3398.5347\n",
      "Epoch 44: val_loss did not improve from 3371.07129\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3403.3718 - mse: 36881924.0000 - mae: 3403.7048 - val_loss: 3669.3638 - val_mse: 46462152.0000 - val_mae: 3669.6597 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3466.3281 - mse: 38774572.0000 - mae: 3466.6616\n",
      "Epoch 45: val_loss improved from 3371.07129 to 3308.62036, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3469.0417 - mse: 38773136.0000 - mae: 3469.3757 - val_loss: 3308.6204 - val_mse: 34991924.0000 - val_mae: 3308.9363 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 3327.6438 - mse: 35788740.0000 - mae: 3327.9651\n",
      "Epoch 46: val_loss improved from 3308.62036 to 3218.89795, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3332.1274 - mse: 35917104.0000 - mae: 3332.4487 - val_loss: 3218.8979 - val_mse: 34127688.0000 - val_mae: 3219.1887 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3211.9790 - mse: 33738684.0000 - mae: 3212.2944\n",
      "Epoch 47: val_loss improved from 3218.89795 to 3129.83350, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3213.0090 - mse: 33750436.0000 - mae: 3213.3252 - val_loss: 3129.8335 - val_mse: 32073464.0000 - val_mae: 3130.2114 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 3153.8752 - mse: 33102230.0000 - mae: 3154.2341\n",
      "Epoch 48: val_loss improved from 3129.83350 to 3113.79663, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3157.6721 - mse: 33144594.0000 - mae: 3158.0308 - val_loss: 3113.7966 - val_mse: 32354036.0000 - val_mae: 3114.1284 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 3115.3970 - mse: 32464834.0000 - mae: 3115.7437\n",
      "Epoch 49: val_loss improved from 3113.79663 to 3062.89966, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3114.2942 - mse: 32449750.0000 - mae: 3114.6399 - val_loss: 3062.8997 - val_mse: 31822002.0000 - val_mae: 3063.1812 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 3200.3140 - mse: 33701056.0000 - mae: 3200.6562\n",
      "Epoch 50: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3197.5200 - mse: 33686264.0000 - mae: 3197.8613 - val_loss: 3525.5728 - val_mse: 45070132.0000 - val_mae: 3525.8550 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 3264.4626 - mse: 35351564.0000 - mae: 3264.8145\n",
      "Epoch 51: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3264.3872 - mse: 35391276.0000 - mae: 3264.7385 - val_loss: 3122.9824 - val_mse: 33271458.0000 - val_mae: 3123.2649 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 3457.9150 - mse: 39138704.0000 - mae: 3458.2693\n",
      "Epoch 52: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3463.3489 - mse: 39195644.0000 - mae: 3463.7034 - val_loss: 3328.1985 - val_mse: 37666520.0000 - val_mae: 3328.5103 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 3256.1970 - mse: 35405228.0000 - mae: 3256.5325\n",
      "Epoch 53: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3256.1970 - mse: 35405228.0000 - mae: 3256.5325 - val_loss: 3079.7083 - val_mse: 31968228.0000 - val_mae: 3079.9924 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3394.5020 - mse: 38687840.0000 - mae: 3394.8223\n",
      "Epoch 54: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3397.7737 - mse: 38732924.0000 - mae: 3398.0947 - val_loss: 3373.1150 - val_mse: 37216584.0000 - val_mae: 3373.4231 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3165.4871 - mse: 33480842.0000 - mae: 3165.8210\n",
      "Epoch 55: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3165.8777 - mse: 33490282.0000 - mae: 3166.2117 - val_loss: 3171.1357 - val_mse: 34953832.0000 - val_mae: 3171.4380 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 3370.9426 - mse: 37678416.0000 - mae: 3371.2808\n",
      "Epoch 56: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3383.7563 - mse: 37924768.0000 - mae: 3384.0945 - val_loss: 3307.6401 - val_mse: 37196444.0000 - val_mae: 3307.9690 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3527.4460 - mse: 40665432.0000 - mae: 3527.8076\n",
      "Epoch 57: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3518.8313 - mse: 40509524.0000 - mae: 3519.1934 - val_loss: 3075.0271 - val_mse: 32107698.0000 - val_mae: 3075.3711 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3191.2734 - mse: 34066568.0000 - mae: 3191.6216\n",
      "Epoch 58: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3195.3438 - mse: 34113284.0000 - mae: 3195.6919 - val_loss: 3537.1650 - val_mse: 40471888.0000 - val_mae: 3537.4978 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 3459.7222 - mse: 39267316.0000 - mae: 3460.0588\n",
      "Epoch 59: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3459.7222 - mse: 39267316.0000 - mae: 3460.0588 - val_loss: 4045.2146 - val_mse: 53102120.0000 - val_mae: 4045.5166 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3284.7327 - mse: 36150024.0000 - mae: 3285.0417\n",
      "Epoch 60: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3282.9194 - mse: 36121724.0000 - mae: 3283.2283 - val_loss: 3110.9016 - val_mse: 33733700.0000 - val_mae: 3111.1709 - lr: 5.0000e-04\n",
      "Epoch 61/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3276.0022 - mse: 36517012.0000 - mae: 3276.3105\n",
      "Epoch 61: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3287.1467 - mse: 36852968.0000 - mae: 3287.4553 - val_loss: 3887.8955 - val_mse: 55862180.0000 - val_mae: 3888.1721 - lr: 5.0000e-04\n",
      "Epoch 62/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 3774.4749 - mse: 46390764.0000 - mae: 3774.7822\n",
      "Epoch 62: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3774.4749 - mse: 46390764.0000 - mae: 3774.7822 - val_loss: 3222.7993 - val_mse: 37338848.0000 - val_mae: 3223.0747 - lr: 5.0000e-04\n",
      "Epoch 63/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3277.6516 - mse: 36284236.0000 - mae: 3277.9666\n",
      "Epoch 63: val_loss did not improve from 3062.89966\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3278.1763 - mse: 36290756.0000 - mae: 3278.4915 - val_loss: 3253.1448 - val_mse: 36029320.0000 - val_mae: 3253.4434 - lr: 5.0000e-04\n",
      "Epoch 64/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 3153.1333 - mse: 34034324.0000 - mae: 3153.4500\n",
      "Epoch 64: val_loss improved from 3062.89966 to 2911.10400, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3150.0234 - mse: 34013160.0000 - mae: 3150.3398 - val_loss: 2911.1040 - val_mse: 31042290.0000 - val_mae: 2911.3762 - lr: 5.0000e-04\n",
      "Epoch 65/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3079.1902 - mse: 33529982.0000 - mae: 3079.5195\n",
      "Epoch 65: val_loss did not improve from 2911.10400\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3080.2249 - mse: 33569856.0000 - mae: 3080.5544 - val_loss: 3098.7063 - val_mse: 33353232.0000 - val_mae: 3099.0476 - lr: 5.0000e-04\n",
      "Epoch 66/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2997.4666 - mse: 31565456.0000 - mae: 2997.8215\n",
      "Epoch 66: val_loss did not improve from 2911.10400\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2997.4666 - mse: 31565456.0000 - mae: 2997.8215 - val_loss: 3056.6235 - val_mse: 32274906.0000 - val_mae: 3056.9336 - lr: 5.0000e-04\n",
      "Epoch 67/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 3040.9934 - mse: 32150684.0000 - mae: 3041.3599\n",
      "Epoch 67: val_loss did not improve from 2911.10400\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3041.7947 - mse: 32165168.0000 - mae: 3042.1614 - val_loss: 2911.8833 - val_mse: 30778068.0000 - val_mae: 2912.2017 - lr: 5.0000e-04\n",
      "Epoch 68/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2951.0283 - mse: 30816008.0000 - mae: 2951.3931\n",
      "Epoch 68: val_loss improved from 2911.10400 to 2878.41528, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2952.3765 - mse: 30807080.0000 - mae: 2952.7410 - val_loss: 2878.4153 - val_mse: 30345858.0000 - val_mae: 2878.7490 - lr: 5.0000e-04\n",
      "Epoch 69/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 3034.2417 - mse: 32716122.0000 - mae: 3034.5583\n",
      "Epoch 69: val_loss did not improve from 2878.41528\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3034.2417 - mse: 32716122.0000 - mae: 3034.5583 - val_loss: 2913.3621 - val_mse: 31072124.0000 - val_mae: 2913.7266 - lr: 5.0000e-04\n",
      "Epoch 70/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3015.7156 - mse: 32666318.0000 - mae: 3016.0364\n",
      "Epoch 70: val_loss did not improve from 2878.41528\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3022.0422 - mse: 32762306.0000 - mae: 3022.3630 - val_loss: 3131.0684 - val_mse: 36292872.0000 - val_mae: 3131.3882 - lr: 5.0000e-04\n",
      "Epoch 71/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 3063.1440 - mse: 33088964.0000 - mae: 3063.4749\n",
      "Epoch 71: val_loss did not improve from 2878.41528\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3069.1960 - mse: 33242670.0000 - mae: 3069.5266 - val_loss: 3182.1221 - val_mse: 37886968.0000 - val_mae: 3182.3940 - lr: 5.0000e-04\n",
      "Epoch 72/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3157.4973 - mse: 34382696.0000 - mae: 3157.8306\n",
      "Epoch 72: val_loss did not improve from 2878.41528\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3156.4541 - mse: 34371728.0000 - mae: 3156.7874 - val_loss: 2902.0247 - val_mse: 30868686.0000 - val_mae: 2902.3474 - lr: 5.0000e-04\n",
      "Epoch 73/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3031.1021 - mse: 32509748.0000 - mae: 3031.4265\n",
      "Epoch 73: val_loss did not improve from 2878.41528\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3034.4468 - mse: 32542782.0000 - mae: 3034.7717 - val_loss: 2947.2097 - val_mse: 31471598.0000 - val_mae: 2947.5327 - lr: 5.0000e-04\n",
      "Epoch 74/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 3069.8369 - mse: 33064166.0000 - mae: 3070.1538\n",
      "Epoch 74: val_loss did not improve from 2878.41528\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3071.0681 - mse: 33086628.0000 - mae: 3071.3855 - val_loss: 3078.9802 - val_mse: 33105500.0000 - val_mae: 3079.3401 - lr: 5.0000e-04\n",
      "Epoch 75/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 3053.4399 - mse: 32844596.0000 - mae: 3053.7532\n",
      "Epoch 75: val_loss did not improve from 2878.41528\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3057.0654 - mse: 32881566.0000 - mae: 3057.3789 - val_loss: 3053.1086 - val_mse: 32561334.0000 - val_mae: 3053.4197 - lr: 5.0000e-04\n",
      "Epoch 76/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 3008.1726 - mse: 32358378.0000 - mae: 3008.4897\n",
      "Epoch 76: val_loss did not improve from 2878.41528\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 3012.3010 - mse: 32424632.0000 - mae: 3012.6184 - val_loss: 2940.0583 - val_mse: 32530140.0000 - val_mae: 2940.3257 - lr: 5.0000e-04\n",
      "Epoch 77/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2912.2681 - mse: 30617610.0000 - mae: 2912.5828\n",
      "Epoch 77: val_loss improved from 2878.41528 to 2843.91016, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2910.9683 - mse: 30562334.0000 - mae: 2911.2837 - val_loss: 2843.9102 - val_mse: 29946562.0000 - val_mae: 2844.2515 - lr: 5.0000e-04\n",
      "Epoch 78/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2909.0825 - mse: 30496126.0000 - mae: 2909.3911\n",
      "Epoch 78: val_loss did not improve from 2843.91016\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2918.6318 - mse: 30719102.0000 - mae: 2918.9409 - val_loss: 2925.5032 - val_mse: 30765604.0000 - val_mae: 2925.8628 - lr: 5.0000e-04\n",
      "Epoch 79/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2938.0474 - mse: 30512296.0000 - mae: 2938.3721\n",
      "Epoch 79: val_loss did not improve from 2843.91016\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2941.8508 - mse: 30522028.0000 - mae: 2942.1763 - val_loss: 2863.0837 - val_mse: 29356030.0000 - val_mae: 2863.4414 - lr: 5.0000e-04\n",
      "Epoch 80/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2949.1997 - mse: 30487256.0000 - mae: 2949.5149\n",
      "Epoch 80: val_loss did not improve from 2843.91016\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2960.3679 - mse: 30571582.0000 - mae: 2960.6841 - val_loss: 3002.8594 - val_mse: 32046942.0000 - val_mae: 3003.1860 - lr: 5.0000e-04\n",
      "Epoch 81/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2905.6853 - mse: 30559200.0000 - mae: 2906.0151\n",
      "Epoch 81: val_loss improved from 2843.91016 to 2822.60400, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2903.9041 - mse: 30528760.0000 - mae: 2904.2334 - val_loss: 2822.6040 - val_mse: 29089418.0000 - val_mae: 2822.8911 - lr: 5.0000e-04\n",
      "Epoch 82/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2875.2759 - mse: 30527726.0000 - mae: 2875.6216\n",
      "Epoch 82: val_loss improved from 2822.60400 to 2735.36377, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2877.4280 - mse: 30518434.0000 - mae: 2877.7729 - val_loss: 2735.3638 - val_mse: 28725852.0000 - val_mae: 2735.6514 - lr: 5.0000e-04\n",
      "Epoch 83/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2907.0042 - mse: 30802756.0000 - mae: 2907.3372\n",
      "Epoch 83: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2920.3594 - mse: 31055368.0000 - mae: 2920.6929 - val_loss: 2837.6614 - val_mse: 30738660.0000 - val_mae: 2837.9775 - lr: 5.0000e-04\n",
      "Epoch 84/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2857.0559 - mse: 29829230.0000 - mae: 2857.3972\n",
      "Epoch 84: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2860.7017 - mse: 29893280.0000 - mae: 2861.0430 - val_loss: 2840.6082 - val_mse: 31415504.0000 - val_mae: 2840.9358 - lr: 5.0000e-04\n",
      "Epoch 85/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2836.1511 - mse: 29861264.0000 - mae: 2836.4954\n",
      "Epoch 85: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2848.7266 - mse: 30035300.0000 - mae: 2849.0710 - val_loss: 2792.4670 - val_mse: 29912116.0000 - val_mae: 2792.7871 - lr: 5.0000e-04\n",
      "Epoch 86/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2885.3748 - mse: 30633222.0000 - mae: 2885.7454\n",
      "Epoch 86: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2887.5999 - mse: 30650226.0000 - mae: 2887.9705 - val_loss: 2778.2339 - val_mse: 29751436.0000 - val_mae: 2778.5156 - lr: 5.0000e-04\n",
      "Epoch 87/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2887.6606 - mse: 30701674.0000 - mae: 2887.9885\n",
      "Epoch 87: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2899.1514 - mse: 30886162.0000 - mae: 2899.4802 - val_loss: 2870.7588 - val_mse: 30023990.0000 - val_mae: 2871.1172 - lr: 5.0000e-04\n",
      "Epoch 88/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2912.9583 - mse: 30775370.0000 - mae: 2913.3032\n",
      "Epoch 88: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2916.2639 - mse: 30832792.0000 - mae: 2916.6091 - val_loss: 2960.1033 - val_mse: 32425286.0000 - val_mae: 2960.4500 - lr: 5.0000e-04\n",
      "Epoch 89/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2873.9819 - mse: 30381246.0000 - mae: 2874.3018\n",
      "Epoch 89: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2882.4866 - mse: 30472406.0000 - mae: 2882.8069 - val_loss: 2846.9966 - val_mse: 30317338.0000 - val_mae: 2847.3352 - lr: 5.0000e-04\n",
      "Epoch 90/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2873.8462 - mse: 29981856.0000 - mae: 2874.1697\n",
      "Epoch 90: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2886.2959 - mse: 30156258.0000 - mae: 2886.6194 - val_loss: 2805.1880 - val_mse: 29765750.0000 - val_mae: 2805.4856 - lr: 5.0000e-04\n",
      "Epoch 91/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2870.7017 - mse: 30096688.0000 - mae: 2871.0149\n",
      "Epoch 91: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2868.3984 - mse: 30054912.0000 - mae: 2868.7119 - val_loss: 2880.8647 - val_mse: 29933712.0000 - val_mae: 2881.1592 - lr: 5.0000e-04\n",
      "Epoch 92/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2927.6440 - mse: 30658274.0000 - mae: 2927.9878\n",
      "Epoch 92: val_loss did not improve from 2735.36377\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2935.7236 - mse: 30780734.0000 - mae: 2936.0671 - val_loss: 2891.6106 - val_mse: 29444370.0000 - val_mae: 2891.8813 - lr: 5.0000e-04\n",
      "Epoch 93/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2886.8159 - mse: 29830248.0000 - mae: 2887.1931\n",
      "Epoch 93: val_loss improved from 2735.36377 to 2716.00488, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2889.8936 - mse: 29887464.0000 - mae: 2890.2708 - val_loss: 2716.0049 - val_mse: 28683638.0000 - val_mae: 2716.4036 - lr: 2.5000e-04\n",
      "Epoch 94/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2805.3601 - mse: 28640282.0000 - mae: 2805.7322\n",
      "Epoch 94: val_loss improved from 2716.00488 to 2629.38647, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2807.5994 - mse: 28679874.0000 - mae: 2807.9712 - val_loss: 2629.3865 - val_mse: 27705200.0000 - val_mae: 2629.6592 - lr: 2.5000e-04\n",
      "Epoch 95/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2809.6521 - mse: 29362204.0000 - mae: 2810.0188\n",
      "Epoch 95: val_loss did not improve from 2629.38647\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2809.2507 - mse: 29395820.0000 - mae: 2809.6167 - val_loss: 2808.3376 - val_mse: 29504584.0000 - val_mae: 2808.6143 - lr: 2.5000e-04\n",
      "Epoch 96/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2801.9380 - mse: 28908690.0000 - mae: 2802.2798\n",
      "Epoch 96: val_loss did not improve from 2629.38647\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2805.5383 - mse: 28914132.0000 - mae: 2805.8804 - val_loss: 2672.2896 - val_mse: 27636868.0000 - val_mae: 2672.5525 - lr: 2.5000e-04\n",
      "Epoch 97/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2754.5378 - mse: 28815754.0000 - mae: 2754.8525\n",
      "Epoch 97: val_loss did not improve from 2629.38647\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2758.9292 - mse: 28829642.0000 - mae: 2759.2451 - val_loss: 2634.7244 - val_mse: 28143874.0000 - val_mae: 2635.1208 - lr: 2.5000e-04\n",
      "Epoch 98/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2732.6736 - mse: 28326518.0000 - mae: 2732.9915\n",
      "Epoch 98: val_loss did not improve from 2629.38647\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2729.1318 - mse: 28259188.0000 - mae: 2729.4497 - val_loss: 2650.6201 - val_mse: 27611526.0000 - val_mae: 2650.9600 - lr: 2.5000e-04\n",
      "Epoch 99/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2765.4836 - mse: 28199038.0000 - mae: 2765.7937\n",
      "Epoch 99: val_loss improved from 2629.38647 to 2619.23682, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2778.5774 - mse: 28405426.0000 - mae: 2778.8882 - val_loss: 2619.2368 - val_mse: 25747684.0000 - val_mae: 2619.5525 - lr: 2.5000e-04\n",
      "Epoch 100/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2758.4282 - mse: 27772898.0000 - mae: 2758.7439\n",
      "Epoch 100: val_loss improved from 2619.23682 to 2569.02271, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2758.4282 - mse: 27772898.0000 - mae: 2758.7439 - val_loss: 2569.0227 - val_mse: 26570494.0000 - val_mae: 2569.3015 - lr: 2.5000e-04\n",
      "Epoch 101/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2674.1338 - mse: 27039592.0000 - mae: 2674.4463\n",
      "Epoch 101: val_loss did not improve from 2569.02271\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2682.7395 - mse: 27148366.0000 - mae: 2683.0527 - val_loss: 2618.3032 - val_mse: 25250102.0000 - val_mae: 2618.6504 - lr: 2.5000e-04\n",
      "Epoch 102/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2640.2207 - mse: 26142838.0000 - mae: 2640.5315\n",
      "Epoch 102: val_loss did not improve from 2569.02271\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2640.5872 - mse: 26170652.0000 - mae: 2640.8984 - val_loss: 2653.9917 - val_mse: 27158766.0000 - val_mae: 2654.3691 - lr: 2.5000e-04\n",
      "Epoch 103/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2710.4224 - mse: 28266284.0000 - mae: 2710.7373\n",
      "Epoch 103: val_loss did not improve from 2569.02271\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2719.2976 - mse: 28438202.0000 - mae: 2719.6140 - val_loss: 2616.3423 - val_mse: 27693008.0000 - val_mae: 2616.7507 - lr: 2.5000e-04\n",
      "Epoch 104/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2657.1426 - mse: 26909830.0000 - mae: 2657.4521\n",
      "Epoch 104: val_loss improved from 2569.02271 to 2520.56836, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2667.7944 - mse: 27100404.0000 - mae: 2668.1057 - val_loss: 2520.5684 - val_mse: 26053106.0000 - val_mae: 2520.9663 - lr: 2.5000e-04\n",
      "Epoch 105/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2623.7310 - mse: 25777102.0000 - mae: 2624.0549\n",
      "Epoch 105: val_loss improved from 2520.56836 to 2453.49194, saving model to new_stne_gru_weight.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2624.3757 - mse: 25773044.0000 - mae: 2624.7000 - val_loss: 2453.4919 - val_mse: 23758404.0000 - val_mae: 2453.7952 - lr: 2.5000e-04\n",
      "Epoch 106/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2634.7380 - mse: 25859198.0000 - mae: 2635.0608\n",
      "Epoch 106: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2634.5642 - mse: 25855690.0000 - mae: 2634.8867 - val_loss: 2553.6973 - val_mse: 24948762.0000 - val_mae: 2553.9949 - lr: 2.5000e-04\n",
      "Epoch 107/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2663.4646 - mse: 25632206.0000 - mae: 2663.7830\n",
      "Epoch 107: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2664.0474 - mse: 25617968.0000 - mae: 2664.3660 - val_loss: 2524.3838 - val_mse: 25202278.0000 - val_mae: 2524.7302 - lr: 2.5000e-04\n",
      "Epoch 108/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2566.5027 - mse: 24975704.0000 - mae: 2566.8242\n",
      "Epoch 108: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2566.5027 - mse: 24975704.0000 - mae: 2566.8242 - val_loss: 2606.2595 - val_mse: 26739110.0000 - val_mae: 2606.6072 - lr: 2.5000e-04\n",
      "Epoch 109/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2613.1213 - mse: 25618548.0000 - mae: 2613.4509\n",
      "Epoch 109: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2615.1980 - mse: 25656192.0000 - mae: 2615.5283 - val_loss: 2518.6104 - val_mse: 24175414.0000 - val_mae: 2518.9729 - lr: 2.5000e-04\n",
      "Epoch 110/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2638.3213 - mse: 25805466.0000 - mae: 2638.6428\n",
      "Epoch 110: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2636.3831 - mse: 25781530.0000 - mae: 2636.7041 - val_loss: 2603.6240 - val_mse: 25654306.0000 - val_mae: 2603.8984 - lr: 2.5000e-04\n",
      "Epoch 111/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2677.9863 - mse: 26495746.0000 - mae: 2678.3323\n",
      "Epoch 111: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2683.6992 - mse: 26542112.0000 - mae: 2684.0447 - val_loss: 2637.1494 - val_mse: 26935718.0000 - val_mae: 2637.4949 - lr: 2.5000e-04\n",
      "Epoch 112/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2723.2937 - mse: 27008758.0000 - mae: 2723.6362\n",
      "Epoch 112: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2727.9167 - mse: 27079020.0000 - mae: 2728.2590 - val_loss: 2647.7515 - val_mse: 27838316.0000 - val_mae: 2648.0190 - lr: 2.5000e-04\n",
      "Epoch 113/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2722.0752 - mse: 27304562.0000 - mae: 2722.3989\n",
      "Epoch 113: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2724.1628 - mse: 27346088.0000 - mae: 2724.4866 - val_loss: 2719.9180 - val_mse: 27721410.0000 - val_mae: 2720.2300 - lr: 2.5000e-04\n",
      "Epoch 114/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2727.3667 - mse: 27515780.0000 - mae: 2727.6958\n",
      "Epoch 114: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2727.3667 - mse: 27515780.0000 - mae: 2727.6958 - val_loss: 2653.0522 - val_mse: 27527278.0000 - val_mae: 2653.3586 - lr: 2.5000e-04\n",
      "Epoch 115/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2761.3506 - mse: 28314570.0000 - mae: 2761.6838\n",
      "Epoch 115: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2770.1079 - mse: 28462634.0000 - mae: 2770.4414 - val_loss: 2664.9810 - val_mse: 26917262.0000 - val_mae: 2665.3350 - lr: 2.5000e-04\n",
      "Epoch 116/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2761.9067 - mse: 28593390.0000 - mae: 2762.2224\n",
      "Epoch 116: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2764.5671 - mse: 28634444.0000 - mae: 2764.8831 - val_loss: 2657.2512 - val_mse: 27491056.0000 - val_mae: 2657.5374 - lr: 1.2500e-04\n",
      "Epoch 117/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2720.9189 - mse: 28084252.0000 - mae: 2721.2280\n",
      "Epoch 117: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2738.7441 - mse: 28364880.0000 - mae: 2739.0542 - val_loss: 2648.4690 - val_mse: 27525176.0000 - val_mae: 2648.7661 - lr: 1.2500e-04\n",
      "Epoch 118/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2709.8694 - mse: 27763024.0000 - mae: 2710.1917\n",
      "Epoch 118: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2716.6599 - mse: 27878150.0000 - mae: 2716.9822 - val_loss: 2615.9607 - val_mse: 26981618.0000 - val_mae: 2616.2292 - lr: 1.2500e-04\n",
      "Epoch 119/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2698.4167 - mse: 27644302.0000 - mae: 2698.7390\n",
      "Epoch 119: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2698.4167 - mse: 27644302.0000 - mae: 2698.7390 - val_loss: 2577.4250 - val_mse: 26892490.0000 - val_mae: 2577.7737 - lr: 1.2500e-04\n",
      "Epoch 120/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2710.9980 - mse: 27989476.0000 - mae: 2711.3079\n",
      "Epoch 120: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2710.1877 - mse: 27950270.0000 - mae: 2710.4980 - val_loss: 2607.4419 - val_mse: 27606122.0000 - val_mae: 2607.7356 - lr: 1.2500e-04\n",
      "Epoch 121/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2697.3545 - mse: 27318224.0000 - mae: 2697.6672\n",
      "Epoch 121: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2702.3215 - mse: 27396686.0000 - mae: 2702.6357 - val_loss: 2660.9290 - val_mse: 27844648.0000 - val_mae: 2661.2878 - lr: 1.2500e-04\n",
      "Epoch 122/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2670.8452 - mse: 26771264.0000 - mae: 2671.1545\n",
      "Epoch 122: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2673.5720 - mse: 26761088.0000 - mae: 2673.8821 - val_loss: 2584.0325 - val_mse: 27047774.0000 - val_mae: 2584.3892 - lr: 1.2500e-04\n",
      "Epoch 123/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2671.0469 - mse: 26925548.0000 - mae: 2671.3557\n",
      "Epoch 123: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2671.7083 - mse: 26932740.0000 - mae: 2672.0173 - val_loss: 2560.5759 - val_mse: 26323670.0000 - val_mae: 2560.9038 - lr: 1.2500e-04\n",
      "Epoch 124/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2666.1230 - mse: 26746724.0000 - mae: 2666.4326\n",
      "Epoch 124: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2662.0342 - mse: 26678666.0000 - mae: 2662.3445 - val_loss: 2570.0117 - val_mse: 26351434.0000 - val_mae: 2570.3335 - lr: 1.2500e-04\n",
      "Epoch 125/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2665.3206 - mse: 26593496.0000 - mae: 2665.6238\n",
      "Epoch 125: val_loss did not improve from 2453.49194\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2673.6697 - mse: 26781696.0000 - mae: 2673.9744 - val_loss: 2604.0679 - val_mse: 26962338.0000 - val_mae: 2604.4224 - lr: 1.2500e-04\n",
      "Epoch 1/500\n",
      "    432/Unknown - 2s 6ms/step - loss: 2683.8645 - mse: 27447378.0000 - mae: 2684.1672\n",
      "Epoch 1: val_loss improved from inf to 2620.99854, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2690.3799 - mse: 27544888.0000 - mae: 2690.6824 - val_loss: 2620.9985 - val_mse: 27531150.0000 - val_mae: 2621.2986 - lr: 6.2500e-05\n",
      "Epoch 2/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2650.7749 - mse: 26790706.0000 - mae: 2651.0779\n",
      "Epoch 2: val_loss improved from 2620.99854 to 2578.12183, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2667.4021 - mse: 27099640.0000 - mae: 2667.7063 - val_loss: 2578.1218 - val_mse: 26935362.0000 - val_mae: 2578.4626 - lr: 6.2500e-05\n",
      "Epoch 3/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2674.9678 - mse: 27308940.0000 - mae: 2675.2761\n",
      "Epoch 3: val_loss did not improve from 2578.12183\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2674.9678 - mse: 27308940.0000 - mae: 2675.2761 - val_loss: 2580.0891 - val_mse: 27244668.0000 - val_mae: 2580.3997 - lr: 6.2500e-05\n",
      "Epoch 4/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2681.8193 - mse: 27361522.0000 - mae: 2682.1289\n",
      "Epoch 4: val_loss did not improve from 2578.12183\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2682.3931 - mse: 27350048.0000 - mae: 2682.7026 - val_loss: 2581.7444 - val_mse: 27320772.0000 - val_mae: 2582.0583 - lr: 6.2500e-05\n",
      "Epoch 5/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2674.4180 - mse: 27266108.0000 - mae: 2674.7209\n",
      "Epoch 5: val_loss improved from 2578.12183 to 2563.38379, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2674.4180 - mse: 27266108.0000 - mae: 2674.7209 - val_loss: 2563.3838 - val_mse: 26757128.0000 - val_mae: 2563.6519 - lr: 6.2500e-05\n",
      "Epoch 6/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2665.4265 - mse: 27061620.0000 - mae: 2665.7285\n",
      "Epoch 6: val_loss did not improve from 2563.38379\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2665.4265 - mse: 27061620.0000 - mae: 2665.7285 - val_loss: 2569.0430 - val_mse: 26702094.0000 - val_mae: 2569.3359 - lr: 6.2500e-05\n",
      "Epoch 7/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2675.7598 - mse: 27153882.0000 - mae: 2676.0669\n",
      "Epoch 7: val_loss improved from 2563.38379 to 2561.50537, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2675.7598 - mse: 27153882.0000 - mae: 2676.0669 - val_loss: 2561.5054 - val_mse: 26506282.0000 - val_mae: 2561.7527 - lr: 6.2500e-05\n",
      "Epoch 8/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2684.8748 - mse: 27736756.0000 - mae: 2685.1721\n",
      "Epoch 8: val_loss improved from 2561.50537 to 2560.18701, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2687.7979 - mse: 27767070.0000 - mae: 2688.0959 - val_loss: 2560.1870 - val_mse: 26528230.0000 - val_mae: 2560.4583 - lr: 6.2500e-05\n",
      "Epoch 9/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2646.8726 - mse: 27084560.0000 - mae: 2647.1741\n",
      "Epoch 9: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2663.6067 - mse: 27372166.0000 - mae: 2663.9087 - val_loss: 2568.0283 - val_mse: 26740668.0000 - val_mae: 2568.2925 - lr: 6.2500e-05\n",
      "Epoch 10/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2651.4712 - mse: 27070202.0000 - mae: 2651.7710\n",
      "Epoch 10: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2669.9946 - mse: 27393972.0000 - mae: 2670.2952 - val_loss: 2561.2856 - val_mse: 26083156.0000 - val_mae: 2561.5898 - lr: 6.2500e-05\n",
      "Epoch 11/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2643.3760 - mse: 27108966.0000 - mae: 2643.6785\n",
      "Epoch 11: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2660.6313 - mse: 27392334.0000 - mae: 2660.9351 - val_loss: 2590.0046 - val_mse: 27583598.0000 - val_mae: 2590.3208 - lr: 6.2500e-05\n",
      "Epoch 12/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2636.1980 - mse: 27001976.0000 - mae: 2636.5044\n",
      "Epoch 12: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2654.3589 - mse: 27320672.0000 - mae: 2654.6650 - val_loss: 2572.5601 - val_mse: 26734122.0000 - val_mae: 2572.8247 - lr: 6.2500e-05\n",
      "Epoch 13/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2625.5024 - mse: 26572326.0000 - mae: 2625.8081\n",
      "Epoch 13: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2643.4153 - mse: 26899738.0000 - mae: 2643.7212 - val_loss: 2577.4614 - val_mse: 27237848.0000 - val_mae: 2577.7432 - lr: 6.2500e-05\n",
      "Epoch 14/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2638.9822 - mse: 26696140.0000 - mae: 2639.2883\n",
      "Epoch 14: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2654.0820 - mse: 26941972.0000 - mae: 2654.3879 - val_loss: 2577.6780 - val_mse: 26728614.0000 - val_mae: 2577.9351 - lr: 6.2500e-05\n",
      "Epoch 15/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2646.2166 - mse: 26864100.0000 - mae: 2646.5176\n",
      "Epoch 15: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2654.9211 - mse: 26994990.0000 - mae: 2655.2231 - val_loss: 2573.1731 - val_mse: 26641168.0000 - val_mae: 2573.4753 - lr: 6.2500e-05\n",
      "Epoch 16/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2640.1812 - mse: 26799458.0000 - mae: 2640.4802\n",
      "Epoch 16: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2655.1799 - mse: 27040458.0000 - mae: 2655.4790 - val_loss: 2565.1345 - val_mse: 26132636.0000 - val_mae: 2565.4270 - lr: 6.2500e-05\n",
      "Epoch 17/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2653.5291 - mse: 27228318.0000 - mae: 2653.8293\n",
      "Epoch 17: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2654.0979 - mse: 27216860.0000 - mae: 2654.3984 - val_loss: 2579.9800 - val_mse: 26542156.0000 - val_mae: 2580.2808 - lr: 6.2500e-05\n",
      "Epoch 18/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2657.6284 - mse: 27031236.0000 - mae: 2657.9290\n",
      "Epoch 18: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2657.7361 - mse: 27016110.0000 - mae: 2658.0371 - val_loss: 2630.8535 - val_mse: 27402184.0000 - val_mae: 2631.1089 - lr: 6.2500e-05\n",
      "Epoch 19/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2681.4539 - mse: 27503654.0000 - mae: 2681.7563\n",
      "Epoch 19: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2689.0161 - mse: 27609418.0000 - mae: 2689.3186 - val_loss: 2613.4844 - val_mse: 27203548.0000 - val_mae: 2613.7339 - lr: 3.1250e-05\n",
      "Epoch 20/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2670.8999 - mse: 27225708.0000 - mae: 2671.2070\n",
      "Epoch 20: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2688.3220 - mse: 27532852.0000 - mae: 2688.6299 - val_loss: 2632.1909 - val_mse: 27521212.0000 - val_mae: 2632.4702 - lr: 3.1250e-05\n",
      "Epoch 21/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2660.9541 - mse: 27097088.0000 - mae: 2661.2600\n",
      "Epoch 21: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2677.1982 - mse: 27375794.0000 - mae: 2677.5046 - val_loss: 2608.7546 - val_mse: 27328604.0000 - val_mae: 2609.0244 - lr: 3.1250e-05\n",
      "Epoch 22/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2661.8206 - mse: 27210012.0000 - mae: 2662.1250\n",
      "Epoch 22: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2668.5671 - mse: 27296338.0000 - mae: 2668.8721 - val_loss: 2593.4277 - val_mse: 26982836.0000 - val_mae: 2593.6904 - lr: 3.1250e-05\n",
      "Epoch 23/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2652.3569 - mse: 26996048.0000 - mae: 2652.6658\n",
      "Epoch 23: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2669.2517 - mse: 27283440.0000 - mae: 2669.5605 - val_loss: 2584.5950 - val_mse: 26971170.0000 - val_mae: 2584.8606 - lr: 3.1250e-05\n",
      "Epoch 24/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2671.4229 - mse: 27336762.0000 - mae: 2671.7310\n",
      "Epoch 24: val_loss did not improve from 2560.18701\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2671.2788 - mse: 27309686.0000 - mae: 2671.5872 - val_loss: 2590.4185 - val_mse: 27105424.0000 - val_mae: 2590.6938 - lr: 3.1250e-05\n",
      "Epoch 25/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2664.5510 - mse: 27392412.0000 - mae: 2664.8604\n",
      "Epoch 25: val_loss improved from 2560.18701 to 2552.85840, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2671.4221 - mse: 27485700.0000 - mae: 2671.7322 - val_loss: 2552.8584 - val_mse: 26346606.0000 - val_mae: 2553.1682 - lr: 3.1250e-05\n",
      "Epoch 26/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2661.3948 - mse: 27408144.0000 - mae: 2661.6985\n",
      "Epoch 26: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2661.7039 - mse: 27394318.0000 - mae: 2662.0076 - val_loss: 2566.6536 - val_mse: 26529734.0000 - val_mae: 2566.9480 - lr: 3.1250e-05\n",
      "Epoch 27/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2643.5417 - mse: 27154960.0000 - mae: 2643.8469\n",
      "Epoch 27: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2660.3840 - mse: 27428208.0000 - mae: 2660.6902 - val_loss: 2554.1619 - val_mse: 26270418.0000 - val_mae: 2554.4541 - lr: 3.1250e-05\n",
      "Epoch 28/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2654.8701 - mse: 27402238.0000 - mae: 2655.1780\n",
      "Epoch 28: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2655.3779 - mse: 27389488.0000 - mae: 2655.6855 - val_loss: 2579.6060 - val_mse: 26972024.0000 - val_mae: 2579.9141 - lr: 3.1250e-05\n",
      "Epoch 29/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2656.7239 - mse: 27198268.0000 - mae: 2657.0303\n",
      "Epoch 29: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2664.0054 - mse: 27293418.0000 - mae: 2664.3120 - val_loss: 2575.1106 - val_mse: 26587646.0000 - val_mae: 2575.3992 - lr: 3.1250e-05\n",
      "Epoch 30/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2649.9258 - mse: 26698314.0000 - mae: 2650.2319\n",
      "Epoch 30: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2650.1648 - mse: 26676734.0000 - mae: 2650.4712 - val_loss: 2580.6111 - val_mse: 26582728.0000 - val_mae: 2580.9124 - lr: 3.1250e-05\n",
      "Epoch 31/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2652.2732 - mse: 26848268.0000 - mae: 2652.5759\n",
      "Epoch 31: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2666.9922 - mse: 27072064.0000 - mae: 2667.2944 - val_loss: 2569.8601 - val_mse: 26317702.0000 - val_mae: 2570.1389 - lr: 3.1250e-05\n",
      "Epoch 32/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2627.8679 - mse: 26609166.0000 - mae: 2628.1755\n",
      "Epoch 32: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2641.1851 - mse: 26796208.0000 - mae: 2641.4937 - val_loss: 2580.4207 - val_mse: 25841406.0000 - val_mae: 2580.7271 - lr: 3.1250e-05\n",
      "Epoch 33/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2648.2605 - mse: 26593426.0000 - mae: 2648.5662\n",
      "Epoch 33: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2659.5784 - mse: 26745534.0000 - mae: 2659.8843 - val_loss: 2579.9082 - val_mse: 26001118.0000 - val_mae: 2580.1958 - lr: 3.1250e-05\n",
      "Epoch 34/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2667.5029 - mse: 26851632.0000 - mae: 2667.8083\n",
      "Epoch 34: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2667.8711 - mse: 26829320.0000 - mae: 2668.1768 - val_loss: 2579.9695 - val_mse: 26001912.0000 - val_mae: 2580.2783 - lr: 3.1250e-05\n",
      "Epoch 35/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2637.7131 - mse: 26294816.0000 - mae: 2638.0161\n",
      "Epoch 35: val_loss did not improve from 2552.85840\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2653.3567 - mse: 26554136.0000 - mae: 2653.6604 - val_loss: 2570.2961 - val_mse: 25925062.0000 - val_mae: 2570.6130 - lr: 3.1250e-05\n",
      "Epoch 36/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2635.7271 - mse: 26488322.0000 - mae: 2636.0281\n",
      "Epoch 36: val_loss improved from 2552.85840 to 2546.78467, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2642.6008 - mse: 26581052.0000 - mae: 2642.9023 - val_loss: 2546.7847 - val_mse: 25384376.0000 - val_mae: 2547.1008 - lr: 1.5625e-05\n",
      "Epoch 37/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2619.6370 - mse: 26105478.0000 - mae: 2619.9404\n",
      "Epoch 37: val_loss did not improve from 2546.78467\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2631.5522 - mse: 26280024.0000 - mae: 2631.8562 - val_loss: 2552.9011 - val_mse: 25801162.0000 - val_mae: 2553.2173 - lr: 1.5625e-05\n",
      "Epoch 38/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2624.6497 - mse: 26274446.0000 - mae: 2624.9519\n",
      "Epoch 38: val_loss did not improve from 2546.78467\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2631.2900 - mse: 26365966.0000 - mae: 2631.5930 - val_loss: 2557.6492 - val_mse: 25640844.0000 - val_mae: 2557.9673 - lr: 1.5625e-05\n",
      "Epoch 39/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2612.6211 - mse: 26199692.0000 - mae: 2612.9238\n",
      "Epoch 39: val_loss improved from 2546.78467 to 2541.29785, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2626.1497 - mse: 26410326.0000 - mae: 2626.4534 - val_loss: 2541.2979 - val_mse: 25554006.0000 - val_mae: 2541.6138 - lr: 1.5625e-05\n",
      "Epoch 40/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2622.8218 - mse: 26447758.0000 - mae: 2623.1270\n",
      "Epoch 40: val_loss did not improve from 2541.29785\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2624.3750 - mse: 26444524.0000 - mae: 2624.6802 - val_loss: 2541.5479 - val_mse: 25586566.0000 - val_mae: 2541.8660 - lr: 1.5625e-05\n",
      "Epoch 41/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2611.2378 - mse: 26263034.0000 - mae: 2611.5408\n",
      "Epoch 41: val_loss did not improve from 2541.29785\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2622.5415 - mse: 26431448.0000 - mae: 2622.8455 - val_loss: 2551.8503 - val_mse: 25630236.0000 - val_mae: 2552.1655 - lr: 1.5625e-05\n",
      "Epoch 42/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2625.6870 - mse: 26641358.0000 - mae: 2625.9902\n",
      "Epoch 42: val_loss did not improve from 2541.29785\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2625.6870 - mse: 26641358.0000 - mae: 2625.9902 - val_loss: 2552.9102 - val_mse: 26094794.0000 - val_mae: 2553.2307 - lr: 1.5625e-05\n",
      "Epoch 43/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2613.0322 - mse: 26464912.0000 - mae: 2613.3362\n",
      "Epoch 43: val_loss improved from 2541.29785 to 2538.43384, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2619.8579 - mse: 26560292.0000 - mae: 2620.1621 - val_loss: 2538.4338 - val_mse: 25665248.0000 - val_mae: 2538.7527 - lr: 1.5625e-05\n",
      "Epoch 44/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2610.9470 - mse: 26492624.0000 - mae: 2611.2520\n",
      "Epoch 44: val_loss did not improve from 2538.43384\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2611.6047 - mse: 26482538.0000 - mae: 2611.9099 - val_loss: 2543.9272 - val_mse: 25936642.0000 - val_mae: 2544.2437 - lr: 1.5625e-05\n",
      "Epoch 45/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2600.2361 - mse: 26246978.0000 - mae: 2600.5391\n",
      "Epoch 45: val_loss did not improve from 2538.43384\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2613.2744 - mse: 26439554.0000 - mae: 2613.5776 - val_loss: 2541.0337 - val_mse: 25943970.0000 - val_mae: 2541.3479 - lr: 1.5625e-05\n",
      "Epoch 46/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2610.3013 - mse: 26506422.0000 - mae: 2610.6035\n",
      "Epoch 46: val_loss did not improve from 2538.43384\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2610.5850 - mse: 26493086.0000 - mae: 2610.8877 - val_loss: 2542.9597 - val_mse: 25966668.0000 - val_mae: 2543.2734 - lr: 1.5625e-05\n",
      "Epoch 47/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2608.7510 - mse: 26491882.0000 - mae: 2609.0542\n",
      "Epoch 47: val_loss did not improve from 2538.43384\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2616.5867 - mse: 26603900.0000 - mae: 2616.8901 - val_loss: 2540.1252 - val_mse: 25803958.0000 - val_mae: 2540.4446 - lr: 1.5625e-05\n",
      "Epoch 48/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2609.6338 - mse: 26339750.0000 - mae: 2609.9380\n",
      "Epoch 48: val_loss did not improve from 2538.43384\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2609.6338 - mse: 26339750.0000 - mae: 2609.9380 - val_loss: 2543.3582 - val_mse: 25746720.0000 - val_mae: 2543.6760 - lr: 1.5625e-05\n",
      "Epoch 49/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2597.5940 - mse: 26094668.0000 - mae: 2597.8982\n",
      "Epoch 49: val_loss did not improve from 2538.43384\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2605.0808 - mse: 26192402.0000 - mae: 2605.3850 - val_loss: 2538.8748 - val_mse: 25488382.0000 - val_mae: 2539.1936 - lr: 1.5625e-05\n",
      "Epoch 50/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2595.7820 - mse: 26101402.0000 - mae: 2596.0842\n",
      "Epoch 50: val_loss did not improve from 2538.43384\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2609.5269 - mse: 26297614.0000 - mae: 2609.8298 - val_loss: 2546.5925 - val_mse: 25748494.0000 - val_mae: 2546.9116 - lr: 1.5625e-05\n",
      "Epoch 51/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2604.7905 - mse: 26230628.0000 - mae: 2605.0962\n",
      "Epoch 51: val_loss improved from 2538.43384 to 2534.55322, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2605.9480 - mse: 26223624.0000 - mae: 2606.2537 - val_loss: 2534.5532 - val_mse: 25434652.0000 - val_mae: 2534.8733 - lr: 1.5625e-05\n",
      "Epoch 52/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2597.6780 - mse: 26011294.0000 - mae: 2597.9807\n",
      "Epoch 52: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2609.8789 - mse: 26184948.0000 - mae: 2610.1826 - val_loss: 2548.6082 - val_mse: 25362382.0000 - val_mae: 2548.9282 - lr: 1.5625e-05\n",
      "Epoch 53/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2602.0781 - mse: 25992188.0000 - mae: 2602.3809\n",
      "Epoch 53: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2608.8108 - mse: 26087800.0000 - mae: 2609.1135 - val_loss: 2548.4382 - val_mse: 25413078.0000 - val_mae: 2548.7515 - lr: 1.5625e-05\n",
      "Epoch 54/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2600.7876 - mse: 26165388.0000 - mae: 2601.0938\n",
      "Epoch 54: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2608.4119 - mse: 26279028.0000 - mae: 2608.7180 - val_loss: 2544.3159 - val_mse: 25756950.0000 - val_mae: 2544.6306 - lr: 1.5625e-05\n",
      "Epoch 55/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2606.8198 - mse: 26345618.0000 - mae: 2607.1248\n",
      "Epoch 55: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2607.5952 - mse: 26336190.0000 - mae: 2607.9004 - val_loss: 2549.9902 - val_mse: 25919516.0000 - val_mae: 2550.3047 - lr: 1.5625e-05\n",
      "Epoch 56/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2607.4343 - mse: 26361444.0000 - mae: 2607.7393\n",
      "Epoch 56: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2614.4651 - mse: 26465072.0000 - mae: 2614.7703 - val_loss: 2542.8799 - val_mse: 25705546.0000 - val_mae: 2543.1973 - lr: 1.5625e-05\n",
      "Epoch 57/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2603.1816 - mse: 26243220.0000 - mae: 2603.4890\n",
      "Epoch 57: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2604.9392 - mse: 26243908.0000 - mae: 2605.2471 - val_loss: 2542.4255 - val_mse: 25721772.0000 - val_mae: 2542.7429 - lr: 1.5625e-05\n",
      "Epoch 58/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2607.3569 - mse: 26329104.0000 - mae: 2607.6641\n",
      "Epoch 58: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2608.1221 - mse: 26319628.0000 - mae: 2608.4292 - val_loss: 2539.3042 - val_mse: 25724756.0000 - val_mae: 2539.6157 - lr: 1.5625e-05\n",
      "Epoch 59/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2605.0818 - mse: 26316766.0000 - mae: 2605.3889\n",
      "Epoch 59: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2605.0818 - mse: 26316766.0000 - mae: 2605.3889 - val_loss: 2545.3225 - val_mse: 25942194.0000 - val_mae: 2545.6372 - lr: 1.5625e-05\n",
      "Epoch 60/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2598.8323 - mse: 26307730.0000 - mae: 2599.1392\n",
      "Epoch 60: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2611.8230 - mse: 26494106.0000 - mae: 2612.1311 - val_loss: 2547.1472 - val_mse: 25843334.0000 - val_mae: 2547.4661 - lr: 1.5625e-05\n",
      "Epoch 61/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2594.6772 - mse: 26059712.0000 - mae: 2594.9795\n",
      "Epoch 61: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2607.3992 - mse: 26246576.0000 - mae: 2607.7026 - val_loss: 2541.8667 - val_mse: 25639092.0000 - val_mae: 2542.1680 - lr: 1.5625e-05\n",
      "Epoch 62/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2604.4573 - mse: 26234042.0000 - mae: 2604.7578\n",
      "Epoch 62: val_loss did not improve from 2534.55322\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2605.0476 - mse: 26222710.0000 - mae: 2605.3481 - val_loss: 2535.3650 - val_mse: 25606646.0000 - val_mae: 2535.6724 - lr: 7.8125e-06\n",
      "Epoch 63/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2596.7932 - mse: 26105408.0000 - mae: 2597.0979\n",
      "Epoch 63: val_loss improved from 2534.55322 to 2518.05957, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2605.1145 - mse: 26222688.0000 - mae: 2605.4189 - val_loss: 2518.0596 - val_mse: 25226360.0000 - val_mae: 2518.3684 - lr: 7.8125e-06\n",
      "Epoch 64/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2599.4346 - mse: 26143342.0000 - mae: 2599.7388\n",
      "Epoch 64: val_loss did not improve from 2518.05957\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2601.9797 - mse: 26151102.0000 - mae: 2602.2842 - val_loss: 2530.3479 - val_mse: 25442380.0000 - val_mae: 2530.6511 - lr: 7.8125e-06\n",
      "Epoch 65/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2585.4658 - mse: 25871702.0000 - mae: 2585.7703\n",
      "Epoch 65: val_loss improved from 2518.05957 to 2514.13354, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2600.0054 - mse: 26092368.0000 - mae: 2600.3101 - val_loss: 2514.1335 - val_mse: 25041156.0000 - val_mae: 2514.4351 - lr: 7.8125e-06\n",
      "Epoch 66/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2596.6301 - mse: 26009502.0000 - mae: 2596.9348\n",
      "Epoch 66: val_loss improved from 2514.13354 to 2512.07690, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2597.7930 - mse: 26003592.0000 - mae: 2598.0977 - val_loss: 2512.0769 - val_mse: 24949732.0000 - val_mae: 2512.3789 - lr: 7.8125e-06\n",
      "Epoch 67/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2580.4526 - mse: 25693974.0000 - mae: 2580.7563\n",
      "Epoch 67: val_loss improved from 2512.07690 to 2508.98242, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2596.4844 - mse: 25943452.0000 - mae: 2596.7888 - val_loss: 2508.9824 - val_mse: 25004862.0000 - val_mae: 2509.2854 - lr: 7.8125e-06\n",
      "Epoch 68/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2597.2722 - mse: 25984284.0000 - mae: 2597.5742\n",
      "Epoch 68: val_loss did not improve from 2508.98242\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2597.2722 - mse: 25984284.0000 - mae: 2597.5742 - val_loss: 2511.1548 - val_mse: 24968450.0000 - val_mae: 2511.4556 - lr: 7.8125e-06\n",
      "Epoch 69/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2582.8081 - mse: 25734128.0000 - mae: 2583.1135\n",
      "Epoch 69: val_loss improved from 2508.98242 to 2506.04614, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2595.7456 - mse: 25922990.0000 - mae: 2596.0518 - val_loss: 2506.0461 - val_mse: 25071642.0000 - val_mae: 2506.3472 - lr: 7.8125e-06\n",
      "Epoch 70/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2596.1848 - mse: 26027204.0000 - mae: 2596.4875\n",
      "Epoch 70: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2595.9041 - mse: 25996140.0000 - mae: 2596.2070 - val_loss: 2525.4907 - val_mse: 25374690.0000 - val_mae: 2525.7937 - lr: 7.8125e-06\n",
      "Epoch 71/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2583.2717 - mse: 25871358.0000 - mae: 2583.5764\n",
      "Epoch 71: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2599.4592 - mse: 26124172.0000 - mae: 2599.7646 - val_loss: 2512.5818 - val_mse: 25152386.0000 - val_mae: 2512.8833 - lr: 7.8125e-06\n",
      "Epoch 72/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2594.7930 - mse: 26032722.0000 - mae: 2595.0989\n",
      "Epoch 72: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2595.1074 - mse: 26006906.0000 - mae: 2595.4136 - val_loss: 2506.4658 - val_mse: 25000688.0000 - val_mae: 2506.7656 - lr: 7.8125e-06\n",
      "Epoch 73/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2597.4702 - mse: 26099916.0000 - mae: 2597.7737\n",
      "Epoch 73: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2598.2297 - mse: 26089668.0000 - mae: 2598.5334 - val_loss: 2520.6982 - val_mse: 25244836.0000 - val_mae: 2521.0071 - lr: 7.8125e-06\n",
      "Epoch 74/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2580.1753 - mse: 25737020.0000 - mae: 2580.4807\n",
      "Epoch 74: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2592.8425 - mse: 25915020.0000 - mae: 2593.1484 - val_loss: 2513.4387 - val_mse: 25102740.0000 - val_mae: 2513.7444 - lr: 7.8125e-06\n",
      "Epoch 75/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2587.6133 - mse: 25859866.0000 - mae: 2587.9153\n",
      "Epoch 75: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2588.9336 - mse: 25854718.0000 - mae: 2589.2356 - val_loss: 2517.0515 - val_mse: 25128040.0000 - val_mae: 2517.3582 - lr: 7.8125e-06\n",
      "Epoch 76/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2574.9143 - mse: 25634720.0000 - mae: 2575.2188\n",
      "Epoch 76: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2591.1411 - mse: 25883204.0000 - mae: 2591.4456 - val_loss: 2512.6223 - val_mse: 25030812.0000 - val_mae: 2512.9268 - lr: 7.8125e-06\n",
      "Epoch 77/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2582.6697 - mse: 25772892.0000 - mae: 2582.9719\n",
      "Epoch 77: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2590.4805 - mse: 25878278.0000 - mae: 2590.7834 - val_loss: 2512.3772 - val_mse: 25101846.0000 - val_mae: 2512.6836 - lr: 7.8125e-06\n",
      "Epoch 78/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2572.9517 - mse: 25617006.0000 - mae: 2573.2581\n",
      "Epoch 78: val_loss did not improve from 2506.04614\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2586.2302 - mse: 25804770.0000 - mae: 2586.5376 - val_loss: 2509.7024 - val_mse: 25031270.0000 - val_mae: 2510.0076 - lr: 7.8125e-06\n",
      "Epoch 79/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2565.5979 - mse: 25521452.0000 - mae: 2565.9038\n",
      "Epoch 79: val_loss improved from 2506.04614 to 2501.19189, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2583.9724 - mse: 25784924.0000 - mae: 2584.2788 - val_loss: 2501.1919 - val_mse: 24878486.0000 - val_mae: 2501.4946 - lr: 7.8125e-06\n",
      "Epoch 80/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2567.7600 - mse: 25549940.0000 - mae: 2568.0630\n",
      "Epoch 80: val_loss improved from 2501.19189 to 2497.98706, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2580.8625 - mse: 25736480.0000 - mae: 2581.1663 - val_loss: 2497.9871 - val_mse: 24660094.0000 - val_mae: 2498.2866 - lr: 7.8125e-06\n",
      "Epoch 81/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2580.9785 - mse: 25664416.0000 - mae: 2581.2812\n",
      "Epoch 81: val_loss did not improve from 2497.98706\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2582.2188 - mse: 25658966.0000 - mae: 2582.5215 - val_loss: 2498.2883 - val_mse: 24689598.0000 - val_mae: 2498.5862 - lr: 7.8125e-06\n",
      "Epoch 82/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2570.1560 - mse: 25507426.0000 - mae: 2570.4573\n",
      "Epoch 82: val_loss improved from 2497.98706 to 2496.22705, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2577.7693 - mse: 25612272.0000 - mae: 2578.0708 - val_loss: 2496.2271 - val_mse: 24570324.0000 - val_mae: 2496.5256 - lr: 7.8125e-06\n",
      "Epoch 83/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2580.0381 - mse: 25693328.0000 - mae: 2580.3386\n",
      "Epoch 83: val_loss did not improve from 2496.22705\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2580.1484 - mse: 25666836.0000 - mae: 2580.4495 - val_loss: 2498.4028 - val_mse: 24630382.0000 - val_mae: 2498.7036 - lr: 7.8125e-06\n",
      "Epoch 84/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2581.4185 - mse: 25759660.0000 - mae: 2581.7236\n",
      "Epoch 84: val_loss did not improve from 2496.22705\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2581.4185 - mse: 25759660.0000 - mae: 2581.7236 - val_loss: 2500.8040 - val_mse: 24886908.0000 - val_mae: 2501.1047 - lr: 7.8125e-06\n",
      "Epoch 85/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2577.7124 - mse: 25753120.0000 - mae: 2578.0176\n",
      "Epoch 85: val_loss did not improve from 2496.22705\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2578.9734 - mse: 25747796.0000 - mae: 2579.2788 - val_loss: 2498.9641 - val_mse: 24759490.0000 - val_mae: 2499.2644 - lr: 7.8125e-06\n",
      "Epoch 86/500\n",
      "432/436 [============================>.] - ETA: 0s - loss: 2570.8213 - mse: 25515942.0000 - mae: 2571.1238\n",
      "Epoch 86: val_loss improved from 2496.22705 to 2495.05396, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2578.4050 - mse: 25620614.0000 - mae: 2578.7080 - val_loss: 2495.0540 - val_mse: 24544068.0000 - val_mae: 2495.3569 - lr: 7.8125e-06\n",
      "Epoch 87/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2576.7815 - mse: 25572102.0000 - mae: 2577.0837\n",
      "Epoch 87: val_loss improved from 2495.05396 to 2494.87720, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2579.6135 - mse: 25584198.0000 - mae: 2579.9163 - val_loss: 2494.8772 - val_mse: 24566042.0000 - val_mae: 2495.1775 - lr: 7.8125e-06\n",
      "Epoch 88/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2569.8149 - mse: 25501516.0000 - mae: 2570.1179\n",
      "Epoch 88: val_loss improved from 2494.87720 to 2492.26270, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2577.1196 - mse: 25608592.0000 - mae: 2577.4231 - val_loss: 2492.2627 - val_mse: 24626638.0000 - val_mae: 2492.5637 - lr: 7.8125e-06\n",
      "Epoch 89/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2573.0474 - mse: 25564462.0000 - mae: 2573.3518\n",
      "Epoch 89: val_loss improved from 2492.26270 to 2492.16382, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2574.3628 - mse: 25560106.0000 - mae: 2574.6675 - val_loss: 2492.1638 - val_mse: 24551234.0000 - val_mae: 2492.4661 - lr: 7.8125e-06\n",
      "Epoch 90/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2576.9028 - mse: 25565082.0000 - mae: 2577.2090\n",
      "Epoch 90: val_loss improved from 2492.16382 to 2491.58179, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2577.2305 - mse: 25540062.0000 - mae: 2577.5364 - val_loss: 2491.5818 - val_mse: 24562152.0000 - val_mae: 2491.8843 - lr: 7.8125e-06\n",
      "Epoch 91/500\n",
      "429/436 [============================>.] - ETA: 0s - loss: 2562.1924 - mse: 25358968.0000 - mae: 2562.4954\n",
      "Epoch 91: val_loss improved from 2491.58179 to 2489.56348, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2574.6677 - mse: 25542082.0000 - mae: 2574.9712 - val_loss: 2489.5635 - val_mse: 24561334.0000 - val_mae: 2489.8647 - lr: 7.8125e-06\n",
      "Epoch 92/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2569.9995 - mse: 25478272.0000 - mae: 2570.3059\n",
      "Epoch 92: val_loss did not improve from 2489.56348\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2572.8198 - mse: 25489964.0000 - mae: 2573.1267 - val_loss: 2492.0393 - val_mse: 24586870.0000 - val_mae: 2492.3384 - lr: 7.8125e-06\n",
      "Epoch 93/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2572.9141 - mse: 25492456.0000 - mae: 2573.2197\n",
      "Epoch 93: val_loss did not improve from 2489.56348\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2572.9141 - mse: 25492456.0000 - mae: 2573.2197 - val_loss: 2498.8945 - val_mse: 24541590.0000 - val_mae: 2499.1956 - lr: 7.8125e-06\n",
      "Epoch 94/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2560.1719 - mse: 25289464.0000 - mae: 2560.4771\n",
      "Epoch 94: val_loss did not improve from 2489.56348\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2572.0762 - mse: 25461930.0000 - mae: 2572.3818 - val_loss: 2495.3257 - val_mse: 24668510.0000 - val_mae: 2495.6353 - lr: 7.8125e-06\n",
      "Epoch 95/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2575.4121 - mse: 25521682.0000 - mae: 2575.7166\n",
      "Epoch 95: val_loss improved from 2489.56348 to 2489.27051, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2575.4121 - mse: 25521682.0000 - mae: 2575.7166 - val_loss: 2489.2705 - val_mse: 24492022.0000 - val_mae: 2489.5759 - lr: 7.8125e-06\n",
      "Epoch 96/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2558.1863 - mse: 25311576.0000 - mae: 2558.4912\n",
      "Epoch 96: val_loss improved from 2489.27051 to 2488.96875, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2573.3647 - mse: 25551160.0000 - mae: 2573.6704 - val_loss: 2488.9688 - val_mse: 24540792.0000 - val_mae: 2489.2739 - lr: 7.8125e-06\n",
      "Epoch 97/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2555.4509 - mse: 25233288.0000 - mae: 2555.7554\n",
      "Epoch 97: val_loss improved from 2488.96875 to 2488.38062, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2570.5769 - mse: 25472078.0000 - mae: 2570.8821 - val_loss: 2488.3806 - val_mse: 24475938.0000 - val_mae: 2488.6836 - lr: 7.8125e-06\n",
      "Epoch 98/500\n",
      "427/436 [============================>.] - ETA: 0s - loss: 2555.6572 - mse: 25199016.0000 - mae: 2555.9607\n",
      "Epoch 98: val_loss improved from 2488.38062 to 2486.80176, saving model to new_stne_gru_weight_ns.h5\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2569.7000 - mse: 25419070.0000 - mae: 2570.0044 - val_loss: 2486.8018 - val_mse: 24508672.0000 - val_mae: 2487.1072 - lr: 7.8125e-06\n",
      "Epoch 99/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2563.0259 - mse: 25329136.0000 - mae: 2563.3311\n",
      "Epoch 99: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2575.1182 - mse: 25504554.0000 - mae: 2575.4238 - val_loss: 2505.9902 - val_mse: 24704728.0000 - val_mae: 2506.2961 - lr: 7.8125e-06\n",
      "Epoch 100/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2572.3044 - mse: 25376682.0000 - mae: 2572.6106\n",
      "Epoch 100: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2579.4395 - mse: 25482328.0000 - mae: 2579.7456 - val_loss: 2523.7227 - val_mse: 24945222.0000 - val_mae: 2524.0259 - lr: 7.8125e-06\n",
      "Epoch 101/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2560.5911 - mse: 25144908.0000 - mae: 2560.8970\n",
      "Epoch 101: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2572.3120 - mse: 25317102.0000 - mae: 2572.6189 - val_loss: 2522.9905 - val_mse: 24826046.0000 - val_mae: 2523.2974 - lr: 7.8125e-06\n",
      "Epoch 102/500\n",
      "430/436 [============================>.] - ETA: 0s - loss: 2565.8618 - mse: 25174436.0000 - mae: 2566.1709\n",
      "Epoch 102: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2577.1558 - mse: 25340108.0000 - mae: 2577.4656 - val_loss: 2510.6216 - val_mse: 24664324.0000 - val_mae: 2510.9304 - lr: 7.8125e-06\n",
      "Epoch 103/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2568.4043 - mse: 25290810.0000 - mae: 2568.7080\n",
      "Epoch 103: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2575.5139 - mse: 25396634.0000 - mae: 2575.8176 - val_loss: 2514.7102 - val_mse: 24869374.0000 - val_mae: 2515.0190 - lr: 7.8125e-06\n",
      "Epoch 104/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2565.9194 - mse: 25259012.0000 - mae: 2566.2263\n",
      "Epoch 104: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2572.3450 - mse: 25354104.0000 - mae: 2572.6521 - val_loss: 2511.8687 - val_mse: 24687764.0000 - val_mae: 2512.1794 - lr: 7.8125e-06\n",
      "Epoch 105/500\n",
      "436/436 [==============================] - ETA: 0s - loss: 2572.8904 - mse: 25289336.0000 - mae: 2573.1980\n",
      "Epoch 105: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2572.8904 - mse: 25289336.0000 - mae: 2573.1980 - val_loss: 2513.0110 - val_mse: 24637536.0000 - val_mae: 2513.3220 - lr: 7.8125e-06\n",
      "Epoch 106/500\n",
      "426/436 [============================>.] - ETA: 0s - loss: 2558.4551 - mse: 25142288.0000 - mae: 2558.7612\n",
      "Epoch 106: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2575.2932 - mse: 25396286.0000 - mae: 2575.6006 - val_loss: 2514.5356 - val_mse: 24777116.0000 - val_mae: 2514.8433 - lr: 7.8125e-06\n",
      "Epoch 107/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2565.8499 - mse: 25278680.0000 - mae: 2566.1570\n",
      "Epoch 107: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2572.9219 - mse: 25385024.0000 - mae: 2573.2290 - val_loss: 2512.1812 - val_mse: 24766608.0000 - val_mae: 2512.4888 - lr: 7.8125e-06\n",
      "Epoch 108/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2566.3787 - mse: 25262690.0000 - mae: 2566.6865\n",
      "Epoch 108: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2569.0320 - mse: 25272352.0000 - mae: 2569.3401 - val_loss: 2511.2788 - val_mse: 24811436.0000 - val_mae: 2511.5906 - lr: 7.8125e-06\n",
      "Epoch 109/500\n",
      "428/436 [============================>.] - ETA: 0s - loss: 2555.8181 - mse: 25125828.0000 - mae: 2556.1267\n",
      "Epoch 109: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2570.8135 - mse: 25370152.0000 - mae: 2571.1230 - val_loss: 2507.3862 - val_mse: 24668048.0000 - val_mae: 2507.6819 - lr: 3.9063e-06\n",
      "Epoch 110/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2565.9255 - mse: 25296640.0000 - mae: 2566.2273\n",
      "Epoch 110: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2568.7561 - mse: 25313290.0000 - mae: 2569.0579 - val_loss: 2506.9856 - val_mse: 24633068.0000 - val_mae: 2507.2812 - lr: 3.9063e-06\n",
      "Epoch 111/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2570.8447 - mse: 25421014.0000 - mae: 2571.1477\n",
      "Epoch 111: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2571.3403 - mse: 25399652.0000 - mae: 2571.6438 - val_loss: 2503.6406 - val_mse: 24607522.0000 - val_mae: 2503.9382 - lr: 3.9063e-06\n",
      "Epoch 112/500\n",
      "431/436 [============================>.] - ETA: 0s - loss: 2562.4509 - mse: 25261790.0000 - mae: 2562.7542\n",
      "Epoch 112: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2570.0378 - mse: 25379310.0000 - mae: 2570.3411 - val_loss: 2493.1851 - val_mse: 24577960.0000 - val_mae: 2493.4817 - lr: 3.9063e-06\n",
      "Epoch 113/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2567.8257 - mse: 25356732.0000 - mae: 2568.1294\n",
      "Epoch 113: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2568.3213 - mse: 25335468.0000 - mae: 2568.6250 - val_loss: 2505.3408 - val_mse: 24606326.0000 - val_mae: 2505.6355 - lr: 3.9063e-06\n",
      "Epoch 114/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2562.2686 - mse: 25216042.0000 - mae: 2562.5723\n",
      "Epoch 114: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2565.0356 - mse: 25231080.0000 - mae: 2565.3398 - val_loss: 2504.2778 - val_mse: 24575862.0000 - val_mae: 2504.5764 - lr: 3.9063e-06\n",
      "Epoch 115/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2561.7227 - mse: 25193586.0000 - mae: 2562.0295\n",
      "Epoch 115: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2564.3184 - mse: 25203780.0000 - mae: 2564.6252 - val_loss: 2504.9448 - val_mse: 24546488.0000 - val_mae: 2505.2432 - lr: 3.9063e-06\n",
      "Epoch 116/500\n",
      "434/436 [============================>.] - ETA: 0s - loss: 2569.6372 - mse: 25210858.0000 - mae: 2569.9417\n",
      "Epoch 116: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2570.1057 - mse: 25189884.0000 - mae: 2570.4102 - val_loss: 2499.4375 - val_mse: 24371080.0000 - val_mae: 2499.7373 - lr: 3.9063e-06\n",
      "Epoch 117/500\n",
      "433/436 [============================>.] - ETA: 0s - loss: 2566.8237 - mse: 25143706.0000 - mae: 2567.1284\n",
      "Epoch 117: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 6ms/step - loss: 2569.3687 - mse: 25153390.0000 - mae: 2569.6738 - val_loss: 2501.8423 - val_mse: 24389462.0000 - val_mae: 2502.1411 - lr: 3.9063e-06\n",
      "Epoch 118/500\n",
      "435/436 [============================>.] - ETA: 0s - loss: 2567.4932 - mse: 25144438.0000 - mae: 2567.7983\n",
      "Epoch 118: val_loss did not improve from 2486.80176\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 2568.7073 - mse: 25138818.0000 - mae: 2569.0127 - val_loss: 2500.7024 - val_mse: 24404920.0000 - val_mae: 2501.0012 - lr: 3.9063e-06\n"
     ]
    }
   ],
   "source": [
    "gru_history = gru_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_gru])\n",
    "gru_history_ns = gru_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_gru_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------lstm_history-------------\n",
      "lstm_history Validation Loss: 2434.068603515625\n",
      "lstm_history Validation MSE: 23726474.0\n",
      "lstm_history Validation MAE: 2434.340087890625\n",
      "-------------rnn_history-------------\n",
      "rnn_history Validation Loss: 2202.0009765625\n",
      "rnn_history Validation MSE: 20804716.0\n",
      "rnn_history Validation MAE: 2202.30126953125\n",
      "-------------gru_history-------------\n",
      "gru_history Validation Loss: 2453.491943359375\n",
      "gru_history Validation MSE: 23758404.0\n",
      "gru_history Validation MAE: 2453.795166015625\n",
      "-------------lstm_history_ns-------------\n",
      "lstm_history_ns Validation Loss: 2233.388427734375\n",
      "lstm_history_ns Validation MSE: 20931804.0\n",
      "lstm_history_ns Validation MAE: 2233.674560546875\n",
      "-------------rnn_history_ns-------------\n",
      "rnn_history_ns Validation Loss: 2190.685302734375\n",
      "rnn_history_ns Validation MSE: 20889404.0\n",
      "rnn_history_ns Validation MAE: 2190.9716796875\n",
      "-------------gru_history_ns-------------\n",
      "gru_history_ns Validation Loss: 2486.8017578125\n",
      "gru_history_ns Validation MSE: 24371080.0\n",
      "gru_history_ns Validation MAE: 2487.107177734375\n"
     ]
    }
   ],
   "source": [
    "# 종합 결과\n",
    "\n",
    "history_list = [\"lstm_history\", \"rnn_history\", \"gru_history\", \"lstm_history_ns\", \"rnn_history_ns\", \"gru_history_ns\"]\n",
    "def result(historys) :\n",
    "  for name, history in globals().items() :\n",
    "    if name in history_list :\n",
    "      print(f\"-------------{name}-------------\")\n",
    "      val_loss = min(history.history['val_loss'])\n",
    "      val_mse = min(history.history['val_mse'])\n",
    "      val_mae = min(history.history['val_mae'])\n",
    "      print(f\"{name} Validation Loss:\", val_loss)\n",
    "      print(f\"{name} Validation MSE:\", val_mse)\n",
    "      print(f\"{name} Validation MAE:\", val_mae)\n",
    "\n",
    "result(history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52578, 9)\n"
     ]
    }
   ],
   "source": [
    "St_NotEncode_Basic_data = pd.read_pickle(\"Basic_StandardScalar_final_data\")\n",
    "  \n",
    "print(St_NotEncode_Basic_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler Not Encode Basic Data\n",
      "(52578, 8) (52578,)\n",
      "(42062, 8) (10516, 8) (42062,) (10516,)\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# Feature와 Label 분리하기\n",
    "def Feature_Label(datafile) :\n",
    "    X = datafile.iloc[:,:-1]\n",
    "    y = datafile.iloc[:,-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "print(\"StandardScaler Not Encode Basic Data\")\n",
    "SB_X, SB_y = Feature_Label(St_NotEncode_Basic_data)\n",
    "print(SB_X.shape, SB_y.shape)\n",
    "SB_X_train, SB_X_test, SB_y_train, SB_y_test = train_test_split(SB_X, SB_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(SB_X_train.shape, SB_X_test.shape, SB_y_train.shape, SB_y_test.shape)\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = windowed_dataset(SB_y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
    "test_data = windowed_dataset(SB_y_test, WINDOW_SIZE, BATCH_SIZE, False)\n",
    "train_data_ns = windowed_dataset(SB_y_train, WINDOW_SIZE, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 1)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "for data in train_data.take(1) :\n",
    "  print(f'{data[0].shape}')\n",
    "  print(f'{data[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal',\n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 1]),\n",
    "  LSTM(128, activation='tanh', return_sequences=True),  \n",
    "  LSTM(64, activation='tanh', return_sequences=True),\n",
    "  LSTM(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal', \n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 1]),\n",
    "  SimpleRNN(128, activation='tanh', return_sequences=True),  \n",
    "  SimpleRNN(64, activation='tanh', return_sequences=True),\n",
    "  SimpleRNN(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal',\n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 1]),\n",
    "  GRU(128, activation='tanh', return_sequences=True),  \n",
    "  GRU(64, activation='tanh', return_sequences=True),\n",
    "  GRU(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Huber()\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "lstm_model.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "rnn_model.compile(loss=loss, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse', 'mae'])\n",
    "gru_model.compile(loss=loss, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "            \n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "mc_lstm = ModelCheckpoint('base_stne_lstm_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_lstm_ns = ModelCheckpoint('base_stne_lstm_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_rnn = ModelCheckpoint('base_stne_rnn_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_rnn_ns = ModelCheckpoint('base_stne_rnn_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_gru = ModelCheckpoint('base_stne_gru_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_gru_ns = ModelCheckpoint('base_stne_gru_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1309/Unknown - 15s 7ms/step - loss: 1.7723 - mse: 19.9513 - mae: 2.0935\n",
      "Epoch 1: val_loss improved from inf to 1.79264, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 17s 8ms/step - loss: 1.7718 - mse: 19.9204 - mae: 2.0930 - val_loss: 1.7926 - val_mse: 13.6968 - val_mae: 2.1408 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 1.0062 - mse: 6.2053 - mae: 1.3045\n",
      "Epoch 2: val_loss improved from 1.79264 to 1.47758, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 1.0067 - mse: 6.2063 - mae: 1.3050 - val_loss: 1.4776 - val_mse: 10.8682 - val_mae: 1.8050 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9805 - mse: 6.0533 - mae: 1.2739\n",
      "Epoch 3: val_loss did not improve from 1.47758\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9822 - mse: 6.0605 - mae: 1.2758 - val_loss: 1.6393 - val_mse: 12.1258 - val_mae: 2.0028 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 1.0061 - mse: 6.2617 - mae: 1.3013\n",
      "Epoch 4: val_loss did not improve from 1.47758\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 1.0072 - mse: 6.2833 - mae: 1.3025 - val_loss: 1.5022 - val_mse: 11.1853 - val_mae: 1.8570 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9674 - mse: 6.0220 - mae: 1.2575\n",
      "Epoch 5: val_loss did not improve from 1.47758\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9678 - mse: 6.0254 - mae: 1.2579 - val_loss: 1.5240 - val_mse: 11.1046 - val_mae: 1.8599 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9571 - mse: 5.8778 - mae: 1.2458\n",
      "Epoch 6: val_loss did not improve from 1.47758\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9573 - mse: 5.8785 - mae: 1.2460 - val_loss: 1.6732 - val_mse: 12.5292 - val_mae: 2.0169 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9418 - mse: 5.8110 - mae: 1.2277\n",
      "Epoch 7: val_loss did not improve from 1.47758\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9418 - mse: 5.8110 - mae: 1.2277 - val_loss: 1.6204 - val_mse: 12.1708 - val_mae: 1.9459 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.9431 - mse: 5.8033 - mae: 1.2277\n",
      "Epoch 8: val_loss improved from 1.47758 to 1.47355, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9435 - mse: 5.8069 - mae: 1.2282 - val_loss: 1.4735 - val_mse: 10.7374 - val_mae: 1.8035 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9293 - mse: 5.7294 - mae: 1.2130\n",
      "Epoch 9: val_loss did not improve from 1.47355\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9324 - mse: 5.7560 - mae: 1.2163 - val_loss: 1.4871 - val_mse: 10.8881 - val_mae: 1.8132 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.9334 - mse: 5.7532 - mae: 1.2164\n",
      "Epoch 10: val_loss did not improve from 1.47355\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9364 - mse: 5.7819 - mae: 1.2197 - val_loss: 1.5055 - val_mse: 11.4764 - val_mae: 1.8282 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.9370 - mse: 5.8471 - mae: 1.2199\n",
      "Epoch 11: val_loss did not improve from 1.47355\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9373 - mse: 5.8544 - mae: 1.2203 - val_loss: 1.5135 - val_mse: 11.0276 - val_mae: 1.8304 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.9324 - mse: 5.7506 - mae: 1.2146\n",
      "Epoch 12: val_loss improved from 1.47355 to 1.40923, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9340 - mse: 5.7621 - mae: 1.2163 - val_loss: 1.4092 - val_mse: 10.3822 - val_mae: 1.7268 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9230 - mse: 5.7054 - mae: 1.2053\n",
      "Epoch 13: val_loss did not improve from 1.40923\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9244 - mse: 5.7177 - mae: 1.2068 - val_loss: 1.4470 - val_mse: 10.6848 - val_mae: 1.7683 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9243 - mse: 5.6997 - mae: 1.2052\n",
      "Epoch 14: val_loss did not improve from 1.40923\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9266 - mse: 5.7220 - mae: 1.2076 - val_loss: 1.5590 - val_mse: 11.5652 - val_mae: 1.8711 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.9180 - mse: 5.6838 - mae: 1.1984\n",
      "Epoch 15: val_loss did not improve from 1.40923\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9196 - mse: 5.6943 - mae: 1.2002 - val_loss: 1.4100 - val_mse: 10.1561 - val_mae: 1.7246 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9219 - mse: 5.7162 - mae: 1.2020\n",
      "Epoch 16: val_loss did not improve from 1.40923\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9219 - mse: 5.7162 - mae: 1.2020 - val_loss: 1.4180 - val_mse: 10.3923 - val_mae: 1.7442 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9219 - mse: 5.7066 - mae: 1.2031\n",
      "Epoch 17: val_loss did not improve from 1.40923\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9217 - mse: 5.7052 - mae: 1.2029 - val_loss: 1.4993 - val_mse: 10.9074 - val_mae: 1.8205 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9189 - mse: 5.6883 - mae: 1.1988\n",
      "Epoch 18: val_loss improved from 1.40923 to 1.40212, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9191 - mse: 5.6900 - mae: 1.1990 - val_loss: 1.4021 - val_mse: 9.8952 - val_mae: 1.7202 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9089 - mse: 5.6122 - mae: 1.1883\n",
      "Epoch 19: val_loss did not improve from 1.40212\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9105 - mse: 5.6269 - mae: 1.1901 - val_loss: 1.4788 - val_mse: 10.6117 - val_mae: 1.7969 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9142 - mse: 5.6395 - mae: 1.1940\n",
      "Epoch 20: val_loss did not improve from 1.40212\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9146 - mse: 5.6430 - mae: 1.1944 - val_loss: 1.5450 - val_mse: 11.5451 - val_mae: 1.8613 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.9067 - mse: 5.5611 - mae: 1.1852\n",
      "Epoch 21: val_loss did not improve from 1.40212\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9098 - mse: 5.6067 - mae: 1.1885 - val_loss: 1.4232 - val_mse: 10.3025 - val_mae: 1.7412 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.9181 - mse: 5.6811 - mae: 1.1975\n",
      "Epoch 22: val_loss did not improve from 1.40212\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9197 - mse: 5.6943 - mae: 1.1993 - val_loss: 1.4706 - val_mse: 10.8774 - val_mae: 1.7928 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9180 - mse: 5.6581 - mae: 1.1973\n",
      "Epoch 23: val_loss did not improve from 1.40212\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9184 - mse: 5.6624 - mae: 1.1977 - val_loss: 1.5026 - val_mse: 10.7949 - val_mae: 1.8226 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.9059 - mse: 5.6102 - mae: 1.1845\n",
      "Epoch 24: val_loss did not improve from 1.40212\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9077 - mse: 5.6246 - mae: 1.1865 - val_loss: 1.4883 - val_mse: 10.7362 - val_mae: 1.8031 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.9105 - mse: 5.6207 - mae: 1.1892\n",
      "Epoch 25: val_loss did not improve from 1.40212\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9115 - mse: 5.6260 - mae: 1.1903 - val_loss: 1.5230 - val_mse: 10.9100 - val_mae: 1.8488 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.9009 - mse: 5.5537 - mae: 1.1787\n",
      "Epoch 26: val_loss improved from 1.40212 to 1.36273, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9027 - mse: 5.5697 - mae: 1.1806 - val_loss: 1.3627 - val_mse: 9.7585 - val_mae: 1.6801 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.9030 - mse: 5.5501 - mae: 1.1808\n",
      "Epoch 27: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9033 - mse: 5.5520 - mae: 1.1812 - val_loss: 1.4192 - val_mse: 10.5428 - val_mae: 1.7365 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.9012 - mse: 5.6003 - mae: 1.1789\n",
      "Epoch 28: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9025 - mse: 5.6146 - mae: 1.1805 - val_loss: 1.4138 - val_mse: 10.2898 - val_mae: 1.7203 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9041 - mse: 5.5780 - mae: 1.1813\n",
      "Epoch 29: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9060 - mse: 5.5904 - mae: 1.1833 - val_loss: 1.4156 - val_mse: 10.3003 - val_mae: 1.7317 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8977 - mse: 5.5579 - mae: 1.1745\n",
      "Epoch 30: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8999 - mse: 5.5831 - mae: 1.1768 - val_loss: 1.4771 - val_mse: 10.5162 - val_mae: 1.7956 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8999 - mse: 5.5614 - mae: 1.1775\n",
      "Epoch 31: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9016 - mse: 5.5789 - mae: 1.1792 - val_loss: 1.4731 - val_mse: 10.5471 - val_mae: 1.7921 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.9008 - mse: 5.5606 - mae: 1.1773\n",
      "Epoch 32: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.9021 - mse: 5.5696 - mae: 1.1787 - val_loss: 1.4195 - val_mse: 10.1753 - val_mae: 1.7374 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8959 - mse: 5.5340 - mae: 1.1729\n",
      "Epoch 33: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8963 - mse: 5.5378 - mae: 1.1733 - val_loss: 1.6004 - val_mse: 11.8559 - val_mae: 1.9179 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8946 - mse: 5.5048 - mae: 1.1713\n",
      "Epoch 34: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8968 - mse: 5.5211 - mae: 1.1738 - val_loss: 1.4924 - val_mse: 10.7800 - val_mae: 1.8134 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8912 - mse: 5.4655 - mae: 1.1677\n",
      "Epoch 35: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8944 - mse: 5.5185 - mae: 1.1711 - val_loss: 1.5139 - val_mse: 10.9126 - val_mae: 1.8331 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8956 - mse: 5.5488 - mae: 1.1714\n",
      "Epoch 36: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8956 - mse: 5.5488 - mae: 1.1714 - val_loss: 1.4914 - val_mse: 10.6577 - val_mae: 1.8113 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8860 - mse: 5.4337 - mae: 1.1631\n",
      "Epoch 37: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8883 - mse: 5.4547 - mae: 1.1655 - val_loss: 1.3830 - val_mse: 9.8098 - val_mae: 1.7027 - lr: 5.0000e-04\n",
      "Epoch 38/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8774 - mse: 5.3610 - mae: 1.1535\n",
      "Epoch 38: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8797 - mse: 5.3974 - mae: 1.1559 - val_loss: 1.3949 - val_mse: 9.8833 - val_mae: 1.7133 - lr: 5.0000e-04\n",
      "Epoch 39/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8807 - mse: 5.4233 - mae: 1.1565\n",
      "Epoch 39: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8811 - mse: 5.4289 - mae: 1.1568 - val_loss: 1.4011 - val_mse: 9.9389 - val_mae: 1.7189 - lr: 5.0000e-04\n",
      "Epoch 40/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8801 - mse: 5.4262 - mae: 1.1563\n",
      "Epoch 40: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8803 - mse: 5.4272 - mae: 1.1565 - val_loss: 1.4178 - val_mse: 10.0795 - val_mae: 1.7357 - lr: 5.0000e-04\n",
      "Epoch 41/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8793 - mse: 5.4220 - mae: 1.1542\n",
      "Epoch 41: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8794 - mse: 5.4259 - mae: 1.1544 - val_loss: 1.3665 - val_mse: 9.7309 - val_mae: 1.6831 - lr: 5.0000e-04\n",
      "Epoch 42/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8764 - mse: 5.3901 - mae: 1.1509\n",
      "Epoch 42: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8763 - mse: 5.3877 - mae: 1.1509 - val_loss: 1.3865 - val_mse: 9.7895 - val_mae: 1.7072 - lr: 5.0000e-04\n",
      "Epoch 43/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8754 - mse: 5.4114 - mae: 1.1500\n",
      "Epoch 43: val_loss did not improve from 1.36273\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8756 - mse: 5.4119 - mae: 1.1501 - val_loss: 1.3768 - val_mse: 9.8072 - val_mae: 1.6960 - lr: 5.0000e-04\n",
      "Epoch 44/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8732 - mse: 5.3790 - mae: 1.1476\n",
      "Epoch 44: val_loss improved from 1.36273 to 1.35066, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8742 - mse: 5.3915 - mae: 1.1486 - val_loss: 1.3507 - val_mse: 9.5975 - val_mae: 1.6688 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8739 - mse: 5.3891 - mae: 1.1482\n",
      "Epoch 45: val_loss did not improve from 1.35066\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8752 - mse: 5.3987 - mae: 1.1495 - val_loss: 1.4011 - val_mse: 9.9584 - val_mae: 1.7184 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8704 - mse: 5.3788 - mae: 1.1441\n",
      "Epoch 46: val_loss did not improve from 1.35066\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8713 - mse: 5.3886 - mae: 1.1450 - val_loss: 1.3541 - val_mse: 9.7699 - val_mae: 1.6700 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8717 - mse: 5.3806 - mae: 1.1464\n",
      "Epoch 47: val_loss did not improve from 1.35066\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8733 - mse: 5.3977 - mae: 1.1482 - val_loss: 1.3636 - val_mse: 9.7645 - val_mae: 1.6786 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8718 - mse: 5.3896 - mae: 1.1451\n",
      "Epoch 48: val_loss did not improve from 1.35066\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8725 - mse: 5.3945 - mae: 1.1459 - val_loss: 1.4240 - val_mse: 10.1098 - val_mae: 1.7441 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8705 - mse: 5.3599 - mae: 1.1436\n",
      "Epoch 49: val_loss did not improve from 1.35066\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8706 - mse: 5.3616 - mae: 1.1438 - val_loss: 1.3600 - val_mse: 9.7233 - val_mae: 1.6784 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8710 - mse: 5.3656 - mae: 1.1449\n",
      "Epoch 50: val_loss did not improve from 1.35066\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8721 - mse: 5.3766 - mae: 1.1461 - val_loss: 1.3996 - val_mse: 10.0033 - val_mae: 1.7180 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8687 - mse: 5.3485 - mae: 1.1425\n",
      "Epoch 51: val_loss improved from 1.35066 to 1.34062, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8696 - mse: 5.3535 - mae: 1.1435 - val_loss: 1.3406 - val_mse: 9.5562 - val_mae: 1.6548 - lr: 5.0000e-04\n",
      "Epoch 52/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8691 - mse: 5.3471 - mae: 1.1423\n",
      "Epoch 52: val_loss did not improve from 1.34062\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8698 - mse: 5.3553 - mae: 1.1431 - val_loss: 1.3441 - val_mse: 9.7196 - val_mae: 1.6591 - lr: 5.0000e-04\n",
      "Epoch 53/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8682 - mse: 5.3394 - mae: 1.1420\n",
      "Epoch 53: val_loss did not improve from 1.34062\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8685 - mse: 5.3381 - mae: 1.1424 - val_loss: 1.4091 - val_mse: 10.1228 - val_mae: 1.7241 - lr: 5.0000e-04\n",
      "Epoch 54/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8674 - mse: 5.3314 - mae: 1.1404\n",
      "Epoch 54: val_loss improved from 1.34062 to 1.32927, saving model to base_stne_lstm_weight.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8676 - mse: 5.3326 - mae: 1.1407 - val_loss: 1.3293 - val_mse: 9.4048 - val_mae: 1.6444 - lr: 5.0000e-04\n",
      "Epoch 55/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8674 - mse: 5.3408 - mae: 1.1408\n",
      "Epoch 55: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8674 - mse: 5.3408 - mae: 1.1408 - val_loss: 1.3721 - val_mse: 9.7491 - val_mae: 1.6849 - lr: 5.0000e-04\n",
      "Epoch 56/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8649 - mse: 5.3199 - mae: 1.1389\n",
      "Epoch 56: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8656 - mse: 5.3278 - mae: 1.1397 - val_loss: 1.3956 - val_mse: 9.8866 - val_mae: 1.7075 - lr: 5.0000e-04\n",
      "Epoch 57/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8633 - mse: 5.3231 - mae: 1.1370\n",
      "Epoch 57: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8652 - mse: 5.3356 - mae: 1.1391 - val_loss: 1.3668 - val_mse: 9.7321 - val_mae: 1.6788 - lr: 5.0000e-04\n",
      "Epoch 58/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8643 - mse: 5.3291 - mae: 1.1372\n",
      "Epoch 58: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8655 - mse: 5.3419 - mae: 1.1384 - val_loss: 1.3969 - val_mse: 9.8497 - val_mae: 1.7110 - lr: 5.0000e-04\n",
      "Epoch 59/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8621 - mse: 5.3100 - mae: 1.1360\n",
      "Epoch 59: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8624 - mse: 5.3117 - mae: 1.1362 - val_loss: 1.4009 - val_mse: 10.0237 - val_mae: 1.7162 - lr: 5.0000e-04\n",
      "Epoch 60/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8620 - mse: 5.3136 - mae: 1.1361\n",
      "Epoch 60: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8634 - mse: 5.3248 - mae: 1.1375 - val_loss: 1.3804 - val_mse: 9.9381 - val_mae: 1.6929 - lr: 5.0000e-04\n",
      "Epoch 61/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8578 - mse: 5.2791 - mae: 1.1298\n",
      "Epoch 61: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8604 - mse: 5.3024 - mae: 1.1326 - val_loss: 1.3524 - val_mse: 9.6301 - val_mae: 1.6677 - lr: 5.0000e-04\n",
      "Epoch 62/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8599 - mse: 5.2619 - mae: 1.1328\n",
      "Epoch 62: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8636 - mse: 5.3065 - mae: 1.1368 - val_loss: 1.4103 - val_mse: 10.0889 - val_mae: 1.7246 - lr: 5.0000e-04\n",
      "Epoch 63/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8593 - mse: 5.2855 - mae: 1.1325\n",
      "Epoch 63: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8595 - mse: 5.2865 - mae: 1.1327 - val_loss: 1.3739 - val_mse: 9.8238 - val_mae: 1.6855 - lr: 5.0000e-04\n",
      "Epoch 64/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8574 - mse: 5.2702 - mae: 1.1306\n",
      "Epoch 64: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8596 - mse: 5.2929 - mae: 1.1329 - val_loss: 1.3348 - val_mse: 9.5718 - val_mae: 1.6475 - lr: 5.0000e-04\n",
      "Epoch 65/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8630 - mse: 5.2817 - mae: 1.1376\n",
      "Epoch 65: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8640 - mse: 5.2912 - mae: 1.1387 - val_loss: 1.3693 - val_mse: 9.7142 - val_mae: 1.6860 - lr: 2.5000e-04\n",
      "Epoch 66/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8583 - mse: 5.2488 - mae: 1.1321\n",
      "Epoch 66: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8583 - mse: 5.2487 - mae: 1.1321 - val_loss: 1.3600 - val_mse: 9.6929 - val_mae: 1.6751 - lr: 2.5000e-04\n",
      "Epoch 67/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8595 - mse: 5.2666 - mae: 1.1326\n",
      "Epoch 67: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8602 - mse: 5.2733 - mae: 1.1334 - val_loss: 1.3707 - val_mse: 9.7393 - val_mae: 1.6871 - lr: 2.5000e-04\n",
      "Epoch 68/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8563 - mse: 5.2527 - mae: 1.1294\n",
      "Epoch 68: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8563 - mse: 5.2529 - mae: 1.1294 - val_loss: 1.3506 - val_mse: 9.6550 - val_mae: 1.6661 - lr: 2.5000e-04\n",
      "Epoch 69/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8561 - mse: 5.2559 - mae: 1.1289\n",
      "Epoch 69: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8563 - mse: 5.2548 - mae: 1.1291 - val_loss: 1.3501 - val_mse: 9.6145 - val_mae: 1.6659 - lr: 2.5000e-04\n",
      "Epoch 70/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8554 - mse: 5.2480 - mae: 1.1285\n",
      "Epoch 70: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8554 - mse: 5.2480 - mae: 1.1285 - val_loss: 1.3452 - val_mse: 9.6039 - val_mae: 1.6611 - lr: 2.5000e-04\n",
      "Epoch 71/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8542 - mse: 5.2426 - mae: 1.1271\n",
      "Epoch 71: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8546 - mse: 5.2444 - mae: 1.1276 - val_loss: 1.3428 - val_mse: 9.6123 - val_mae: 1.6581 - lr: 2.5000e-04\n",
      "Epoch 72/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8514 - mse: 5.2313 - mae: 1.1239\n",
      "Epoch 72: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8524 - mse: 5.2375 - mae: 1.1249 - val_loss: 1.3432 - val_mse: 9.6000 - val_mae: 1.6582 - lr: 2.5000e-04\n",
      "Epoch 73/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8507 - mse: 5.2139 - mae: 1.1227\n",
      "Epoch 73: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8514 - mse: 5.2170 - mae: 1.1233 - val_loss: 1.3948 - val_mse: 9.9181 - val_mae: 1.7090 - lr: 2.5000e-04\n",
      "Epoch 74/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8513 - mse: 5.2192 - mae: 1.1231\n",
      "Epoch 74: val_loss did not improve from 1.32927\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8525 - mse: 5.2258 - mae: 1.1242 - val_loss: 1.4170 - val_mse: 10.1302 - val_mae: 1.7348 - lr: 2.5000e-04\n",
      "Epoch 1/500\n",
      "   1313/Unknown - 9s 6ms/step - loss: 0.8547 - mse: 5.2151 - mae: 1.1287\n",
      "Epoch 1: val_loss improved from inf to 1.36588, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8547 - mse: 5.2145 - mae: 1.1288 - val_loss: 1.3659 - val_mse: 9.6735 - val_mae: 1.6804 - lr: 1.2500e-04\n",
      "Epoch 2/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8512 - mse: 5.1981 - mae: 1.1242\n",
      "Epoch 2: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8522 - mse: 5.2060 - mae: 1.1253 - val_loss: 1.3712 - val_mse: 9.6974 - val_mae: 1.6860 - lr: 1.2500e-04\n",
      "Epoch 3/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8515 - mse: 5.2047 - mae: 1.1243\n",
      "Epoch 3: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8515 - mse: 5.2041 - mae: 1.1243 - val_loss: 1.3749 - val_mse: 9.7206 - val_mae: 1.6898 - lr: 1.2500e-04\n",
      "Epoch 4/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8506 - mse: 5.1999 - mae: 1.1233\n",
      "Epoch 4: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8509 - mse: 5.2021 - mae: 1.1236 - val_loss: 1.3777 - val_mse: 9.7379 - val_mae: 1.6927 - lr: 1.2500e-04\n",
      "Epoch 5/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8503 - mse: 5.2019 - mae: 1.1228\n",
      "Epoch 5: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8503 - mse: 5.1998 - mae: 1.1229 - val_loss: 1.3806 - val_mse: 9.7593 - val_mae: 1.6955 - lr: 1.2500e-04\n",
      "Epoch 6/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8488 - mse: 5.1897 - mae: 1.1213\n",
      "Epoch 6: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8498 - mse: 5.1975 - mae: 1.1224 - val_loss: 1.3833 - val_mse: 9.7789 - val_mae: 1.6981 - lr: 1.2500e-04\n",
      "Epoch 7/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8492 - mse: 5.1954 - mae: 1.1217\n",
      "Epoch 7: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8492 - mse: 5.1947 - mae: 1.1217 - val_loss: 1.3862 - val_mse: 9.8007 - val_mae: 1.7009 - lr: 1.2500e-04\n",
      "Epoch 8/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8489 - mse: 5.1961 - mae: 1.1213\n",
      "Epoch 8: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8488 - mse: 5.1922 - mae: 1.1213 - val_loss: 1.3885 - val_mse: 9.8183 - val_mae: 1.7032 - lr: 1.2500e-04\n",
      "Epoch 9/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8482 - mse: 5.1917 - mae: 1.1206\n",
      "Epoch 9: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8482 - mse: 5.1896 - mae: 1.1207 - val_loss: 1.3906 - val_mse: 9.8347 - val_mae: 1.7054 - lr: 1.2500e-04\n",
      "Epoch 10/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8472 - mse: 5.1850 - mae: 1.1195\n",
      "Epoch 10: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8479 - mse: 5.1871 - mae: 1.1202 - val_loss: 1.3926 - val_mse: 9.8494 - val_mae: 1.7073 - lr: 1.2500e-04\n",
      "Epoch 11/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8473 - mse: 5.1864 - mae: 1.1195\n",
      "Epoch 11: val_loss did not improve from 1.36588\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8473 - mse: 5.1843 - mae: 1.1196 - val_loss: 1.3938 - val_mse: 9.8602 - val_mae: 1.7084 - lr: 1.2500e-04\n",
      "Epoch 12/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8598 - mse: 5.2189 - mae: 1.1345\n",
      "Epoch 12: val_loss improved from 1.36588 to 1.35877, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8596 - mse: 5.2147 - mae: 1.1344 - val_loss: 1.3588 - val_mse: 9.6698 - val_mae: 1.6707 - lr: 6.2500e-05\n",
      "Epoch 13/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8563 - mse: 5.2064 - mae: 1.1300\n",
      "Epoch 13: val_loss improved from 1.35877 to 1.35568, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8562 - mse: 5.2053 - mae: 1.1300 - val_loss: 1.3557 - val_mse: 9.6510 - val_mae: 1.6675 - lr: 6.2500e-05\n",
      "Epoch 14/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8552 - mse: 5.2029 - mae: 1.1288\n",
      "Epoch 14: val_loss improved from 1.35568 to 1.35410, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8551 - mse: 5.2018 - mae: 1.1287 - val_loss: 1.3541 - val_mse: 9.6407 - val_mae: 1.6659 - lr: 6.2500e-05\n",
      "Epoch 15/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8545 - mse: 5.2014 - mae: 1.1278\n",
      "Epoch 15: val_loss improved from 1.35410 to 1.35333, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8544 - mse: 5.1988 - mae: 1.1278 - val_loss: 1.3533 - val_mse: 9.6361 - val_mae: 1.6651 - lr: 6.2500e-05\n",
      "Epoch 16/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8540 - mse: 5.2006 - mae: 1.1273\n",
      "Epoch 16: val_loss improved from 1.35333 to 1.35316, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8538 - mse: 5.1962 - mae: 1.1271 - val_loss: 1.3532 - val_mse: 9.6350 - val_mae: 1.6651 - lr: 6.2500e-05\n",
      "Epoch 17/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8534 - mse: 5.1964 - mae: 1.1266\n",
      "Epoch 17: val_loss improved from 1.35316 to 1.35305, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8533 - mse: 5.1936 - mae: 1.1265 - val_loss: 1.3530 - val_mse: 9.6350 - val_mae: 1.6651 - lr: 6.2500e-05\n",
      "Epoch 18/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8528 - mse: 5.1940 - mae: 1.1259\n",
      "Epoch 18: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8528 - mse: 5.1915 - mae: 1.1259 - val_loss: 1.3532 - val_mse: 9.6363 - val_mae: 1.6652 - lr: 6.2500e-05\n",
      "Epoch 19/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8524 - mse: 5.1920 - mae: 1.1254\n",
      "Epoch 19: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8524 - mse: 5.1894 - mae: 1.1254 - val_loss: 1.3532 - val_mse: 9.6384 - val_mae: 1.6653 - lr: 6.2500e-05\n",
      "Epoch 20/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8512 - mse: 5.1801 - mae: 1.1241\n",
      "Epoch 20: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8521 - mse: 5.1876 - mae: 1.1250 - val_loss: 1.3535 - val_mse: 9.6423 - val_mae: 1.6657 - lr: 6.2500e-05\n",
      "Epoch 21/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8519 - mse: 5.1884 - mae: 1.1248\n",
      "Epoch 21: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8516 - mse: 5.1856 - mae: 1.1245 - val_loss: 1.3537 - val_mse: 9.6460 - val_mae: 1.6658 - lr: 6.2500e-05\n",
      "Epoch 22/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8516 - mse: 5.1887 - mae: 1.1245\n",
      "Epoch 22: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 10s 7ms/step - loss: 0.8514 - mse: 5.1842 - mae: 1.1243 - val_loss: 1.3548 - val_mse: 9.6541 - val_mae: 1.6673 - lr: 6.2500e-05\n",
      "Epoch 23/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8510 - mse: 5.1820 - mae: 1.1238\n",
      "Epoch 23: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8510 - mse: 5.1820 - mae: 1.1238 - val_loss: 1.3547 - val_mse: 9.6570 - val_mae: 1.6672 - lr: 6.2500e-05\n",
      "Epoch 24/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8507 - mse: 5.1816 - mae: 1.1235\n",
      "Epoch 24: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8506 - mse: 5.1804 - mae: 1.1234 - val_loss: 1.3548 - val_mse: 9.6598 - val_mae: 1.6674 - lr: 6.2500e-05\n",
      "Epoch 25/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8494 - mse: 5.1709 - mae: 1.1221\n",
      "Epoch 25: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8502 - mse: 5.1783 - mae: 1.1229 - val_loss: 1.3550 - val_mse: 9.6638 - val_mae: 1.6677 - lr: 6.2500e-05\n",
      "Epoch 26/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8499 - mse: 5.1766 - mae: 1.1226\n",
      "Epoch 26: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8499 - mse: 5.1766 - mae: 1.1226 - val_loss: 1.3554 - val_mse: 9.6681 - val_mae: 1.6682 - lr: 6.2500e-05\n",
      "Epoch 27/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8488 - mse: 5.1673 - mae: 1.1213\n",
      "Epoch 27: val_loss did not improve from 1.35305\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8496 - mse: 5.1747 - mae: 1.1222 - val_loss: 1.3556 - val_mse: 9.6720 - val_mae: 1.6684 - lr: 6.2500e-05\n",
      "Epoch 28/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8652 - mse: 5.2378 - mae: 1.1407\n",
      "Epoch 28: val_loss improved from 1.35305 to 1.33734, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8654 - mse: 5.2384 - mae: 1.1409 - val_loss: 1.3373 - val_mse: 9.6066 - val_mae: 1.6465 - lr: 3.1250e-05\n",
      "Epoch 29/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8623 - mse: 5.2224 - mae: 1.1368\n",
      "Epoch 29: val_loss did not improve from 1.33734\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8629 - mse: 5.2292 - mae: 1.1374 - val_loss: 1.3376 - val_mse: 9.6211 - val_mae: 1.6462 - lr: 3.1250e-05\n",
      "Epoch 30/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8623 - mse: 5.2286 - mae: 1.1365\n",
      "Epoch 30: val_loss did not improve from 1.33734\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8619 - mse: 5.2257 - mae: 1.1362 - val_loss: 1.3373 - val_mse: 9.6246 - val_mae: 1.6458 - lr: 3.1250e-05\n",
      "Epoch 31/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8614 - mse: 5.2246 - mae: 1.1356\n",
      "Epoch 31: val_loss improved from 1.33734 to 1.33707, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8612 - mse: 5.2232 - mae: 1.1354 - val_loss: 1.3371 - val_mse: 9.6255 - val_mae: 1.6454 - lr: 3.1250e-05\n",
      "Epoch 32/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8605 - mse: 5.2207 - mae: 1.1344\n",
      "Epoch 32: val_loss improved from 1.33707 to 1.33669, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8607 - mse: 5.2211 - mae: 1.1347 - val_loss: 1.3367 - val_mse: 9.6247 - val_mae: 1.6450 - lr: 3.1250e-05\n",
      "Epoch 33/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8596 - mse: 5.2126 - mae: 1.1334\n",
      "Epoch 33: val_loss improved from 1.33669 to 1.33634, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8602 - mse: 5.2192 - mae: 1.1341 - val_loss: 1.3363 - val_mse: 9.6244 - val_mae: 1.6446 - lr: 3.1250e-05\n",
      "Epoch 34/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8596 - mse: 5.2159 - mae: 1.1333\n",
      "Epoch 34: val_loss improved from 1.33634 to 1.33606, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8598 - mse: 5.2176 - mae: 1.1336 - val_loss: 1.3361 - val_mse: 9.6236 - val_mae: 1.6442 - lr: 3.1250e-05\n",
      "Epoch 35/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8592 - mse: 5.2157 - mae: 1.1328\n",
      "Epoch 35: val_loss improved from 1.33606 to 1.33586, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8595 - mse: 5.2161 - mae: 1.1331 - val_loss: 1.3359 - val_mse: 9.6237 - val_mae: 1.6440 - lr: 3.1250e-05\n",
      "Epoch 36/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8596 - mse: 5.2193 - mae: 1.1332\n",
      "Epoch 36: val_loss improved from 1.33586 to 1.33574, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8592 - mse: 5.2146 - mae: 1.1327 - val_loss: 1.3357 - val_mse: 9.6238 - val_mae: 1.6439 - lr: 3.1250e-05\n",
      "Epoch 37/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8590 - mse: 5.2146 - mae: 1.1326\n",
      "Epoch 37: val_loss improved from 1.33574 to 1.33565, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8589 - mse: 5.2132 - mae: 1.1323 - val_loss: 1.3357 - val_mse: 9.6242 - val_mae: 1.6438 - lr: 3.1250e-05\n",
      "Epoch 38/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8590 - mse: 5.2165 - mae: 1.1324\n",
      "Epoch 38: val_loss improved from 1.33565 to 1.33564, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8586 - mse: 5.2118 - mae: 1.1320 - val_loss: 1.3356 - val_mse: 9.6250 - val_mae: 1.6438 - lr: 3.1250e-05\n",
      "Epoch 39/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8580 - mse: 5.2101 - mae: 1.1314\n",
      "Epoch 39: val_loss improved from 1.33564 to 1.33557, saving model to base_stne_lstm_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8583 - mse: 5.2105 - mae: 1.1316 - val_loss: 1.3356 - val_mse: 9.6250 - val_mae: 1.6438 - lr: 3.1250e-05\n",
      "Epoch 40/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8585 - mse: 5.2139 - mae: 1.1318\n",
      "Epoch 40: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8581 - mse: 5.2092 - mae: 1.1314 - val_loss: 1.3356 - val_mse: 9.6265 - val_mae: 1.6438 - lr: 3.1250e-05\n",
      "Epoch 41/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8582 - mse: 5.2109 - mae: 1.1314\n",
      "Epoch 41: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8578 - mse: 5.2080 - mae: 1.1311 - val_loss: 1.3357 - val_mse: 9.6273 - val_mae: 1.6439 - lr: 3.1250e-05\n",
      "Epoch 42/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8579 - mse: 5.2097 - mae: 1.1311\n",
      "Epoch 42: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8576 - mse: 5.2068 - mae: 1.1308 - val_loss: 1.3357 - val_mse: 9.6281 - val_mae: 1.6438 - lr: 3.1250e-05\n",
      "Epoch 43/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8576 - mse: 5.2092 - mae: 1.1308\n",
      "Epoch 43: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 7ms/step - loss: 0.8574 - mse: 5.2057 - mae: 1.1306 - val_loss: 1.3357 - val_mse: 9.6292 - val_mae: 1.6439 - lr: 3.1250e-05\n",
      "Epoch 44/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8575 - mse: 5.2074 - mae: 1.1306\n",
      "Epoch 44: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 7ms/step - loss: 0.8572 - mse: 5.2045 - mae: 1.1303 - val_loss: 1.3358 - val_mse: 9.6302 - val_mae: 1.6440 - lr: 3.1250e-05\n",
      "Epoch 45/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8572 - mse: 5.2069 - mae: 1.1303\n",
      "Epoch 45: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 7ms/step - loss: 0.8569 - mse: 5.2033 - mae: 1.1300 - val_loss: 1.3359 - val_mse: 9.6319 - val_mae: 1.6441 - lr: 3.1250e-05\n",
      "Epoch 46/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8568 - mse: 5.2022 - mae: 1.1298\n",
      "Epoch 46: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8568 - mse: 5.2022 - mae: 1.1298 - val_loss: 1.3360 - val_mse: 9.6332 - val_mae: 1.6442 - lr: 3.1250e-05\n",
      "Epoch 47/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8570 - mse: 5.2045 - mae: 1.1300\n",
      "Epoch 47: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8566 - mse: 5.2012 - mae: 1.1296 - val_loss: 1.3361 - val_mse: 9.6338 - val_mae: 1.6442 - lr: 3.1250e-05\n",
      "Epoch 48/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8568 - mse: 5.2035 - mae: 1.1298\n",
      "Epoch 48: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8564 - mse: 5.2001 - mae: 1.1293 - val_loss: 1.3362 - val_mse: 9.6356 - val_mae: 1.6444 - lr: 3.1250e-05\n",
      "Epoch 49/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8717 - mse: 5.2767 - mae: 1.1475\n",
      "Epoch 49: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8719 - mse: 5.2767 - mae: 1.1478 - val_loss: 1.3623 - val_mse: 9.8275 - val_mae: 1.6700 - lr: 1.5625e-05\n",
      "Epoch 50/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8698 - mse: 5.2628 - mae: 1.1447\n",
      "Epoch 50: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8698 - mse: 5.2628 - mae: 1.1447 - val_loss: 1.3665 - val_mse: 9.8687 - val_mae: 1.6741 - lr: 1.5625e-05\n",
      "Epoch 51/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8691 - mse: 5.2625 - mae: 1.1438\n",
      "Epoch 51: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8689 - mse: 5.2589 - mae: 1.1436 - val_loss: 1.3680 - val_mse: 9.8830 - val_mae: 1.6755 - lr: 1.5625e-05\n",
      "Epoch 52/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8685 - mse: 5.2577 - mae: 1.1430\n",
      "Epoch 52: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8683 - mse: 5.2566 - mae: 1.1429 - val_loss: 1.3684 - val_mse: 9.8878 - val_mae: 1.6760 - lr: 1.5625e-05\n",
      "Epoch 53/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8678 - mse: 5.2549 - mae: 1.1423\n",
      "Epoch 53: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8678 - mse: 5.2549 - mae: 1.1423 - val_loss: 1.3685 - val_mse: 9.8884 - val_mae: 1.6761 - lr: 1.5625e-05\n",
      "Epoch 54/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8672 - mse: 5.2516 - mae: 1.1416\n",
      "Epoch 54: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 11s 8ms/step - loss: 0.8675 - mse: 5.2536 - mae: 1.1419 - val_loss: 1.3683 - val_mse: 9.8877 - val_mae: 1.6759 - lr: 1.5625e-05\n",
      "Epoch 55/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8674 - mse: 5.2560 - mae: 1.1417\n",
      "Epoch 55: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8672 - mse: 5.2524 - mae: 1.1414 - val_loss: 1.3682 - val_mse: 9.8864 - val_mae: 1.6758 - lr: 1.5625e-05\n",
      "Epoch 56/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8671 - mse: 5.2550 - mae: 1.1413\n",
      "Epoch 56: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8669 - mse: 5.2513 - mae: 1.1411 - val_loss: 1.3679 - val_mse: 9.8850 - val_mae: 1.6756 - lr: 1.5625e-05\n",
      "Epoch 57/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8669 - mse: 5.2539 - mae: 1.1410\n",
      "Epoch 57: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 10s 8ms/step - loss: 0.8666 - mse: 5.2503 - mae: 1.1408 - val_loss: 1.3677 - val_mse: 9.8832 - val_mae: 1.6754 - lr: 1.5625e-05\n",
      "Epoch 58/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8668 - mse: 5.2537 - mae: 1.1409\n",
      "Epoch 58: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 11s 8ms/step - loss: 0.8664 - mse: 5.2494 - mae: 1.1405 - val_loss: 1.3675 - val_mse: 9.8812 - val_mae: 1.6752 - lr: 1.5625e-05\n",
      "Epoch 59/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8724 - mse: 5.2752 - mae: 1.1481\n",
      "Epoch 59: val_loss did not improve from 1.33557\n",
      "1315/1315 [==============================] - 11s 9ms/step - loss: 0.8722 - mse: 5.2742 - mae: 1.1480 - val_loss: 1.3759 - val_mse: 9.9380 - val_mae: 1.6827 - lr: 7.8125e-06\n"
     ]
    }
   ],
   "source": [
    "lstm_history = lstm_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_lstm])\n",
    "lstm_history_ns = lstm_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_lstm_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1315/Unknown - 7s 3ms/step - loss: 1.9293 - mse: 24.0021 - mae: 2.2459\n",
      "Epoch 1: val_loss improved from inf to 1.87577, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 8s 4ms/step - loss: 1.9293 - mse: 24.0021 - mae: 2.2459 - val_loss: 1.8758 - val_mse: 15.4675 - val_mae: 2.2270 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 1.0322 - mse: 6.4794 - mae: 1.3329\n",
      "Epoch 2: val_loss improved from 1.87577 to 1.65703, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 1.0322 - mse: 6.4794 - mae: 1.3329 - val_loss: 1.6570 - val_mse: 12.6467 - val_mae: 2.0041 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 1.0069 - mse: 6.2504 - mae: 1.3047\n",
      "Epoch 3: val_loss did not improve from 1.65703\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 1.0082 - mse: 6.2668 - mae: 1.3060 - val_loss: 1.7465 - val_mse: 12.8565 - val_mae: 2.1024 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9934 - mse: 6.1459 - mae: 1.2884\n",
      "Epoch 4: val_loss improved from 1.65703 to 1.64329, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9934 - mse: 6.1459 - mae: 1.2884 - val_loss: 1.6433 - val_mse: 12.2924 - val_mae: 1.9773 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "1298/1315 [============================>.] - ETA: 0s - loss: 0.9782 - mse: 6.0036 - mae: 1.2710\n",
      "Epoch 5: val_loss improved from 1.64329 to 1.59421, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9854 - mse: 6.0896 - mae: 1.2784 - val_loss: 1.5942 - val_mse: 11.8245 - val_mae: 1.9215 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.9809 - mse: 6.0691 - mae: 1.2722\n",
      "Epoch 6: val_loss improved from 1.59421 to 1.56574, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9856 - mse: 6.1285 - mae: 1.2772 - val_loss: 1.5657 - val_mse: 11.5755 - val_mae: 1.8898 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9766 - mse: 6.0724 - mae: 1.2668\n",
      "Epoch 7: val_loss improved from 1.56574 to 1.54729, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9776 - mse: 6.0842 - mae: 1.2679 - val_loss: 1.5473 - val_mse: 11.2240 - val_mae: 1.8783 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "1296/1315 [============================>.] - ETA: 0s - loss: 0.9905 - mse: 6.1271 - mae: 1.2886\n",
      "Epoch 8: val_loss did not improve from 1.54729\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9974 - mse: 6.1981 - mae: 1.2958 - val_loss: 1.6130 - val_mse: 12.2722 - val_mae: 1.9237 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.9720 - mse: 5.9612 - mae: 1.2663\n",
      "Epoch 9: val_loss did not improve from 1.54729\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9761 - mse: 6.0382 - mae: 1.2705 - val_loss: 1.5477 - val_mse: 11.3640 - val_mae: 1.8714 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "1295/1315 [============================>.] - ETA: 0s - loss: 0.9667 - mse: 5.9232 - mae: 1.2575\n",
      "Epoch 10: val_loss did not improve from 1.54729\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9753 - mse: 6.0334 - mae: 1.2664 - val_loss: 1.6629 - val_mse: 12.7692 - val_mae: 1.9952 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "1295/1315 [============================>.] - ETA: 0s - loss: 0.9646 - mse: 5.9880 - mae: 1.2539\n",
      "Epoch 11: val_loss did not improve from 1.54729\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9692 - mse: 6.0362 - mae: 1.2589 - val_loss: 1.5650 - val_mse: 11.3156 - val_mae: 1.8943 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.9678 - mse: 6.0041 - mae: 1.2561\n",
      "Epoch 12: val_loss did not improve from 1.54729\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9702 - mse: 6.0231 - mae: 1.2586 - val_loss: 1.6877 - val_mse: 12.5122 - val_mae: 2.0117 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.9638 - mse: 5.9638 - mae: 1.2524\n",
      "Epoch 13: val_loss improved from 1.54729 to 1.54568, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9697 - mse: 6.0465 - mae: 1.2585 - val_loss: 1.5457 - val_mse: 11.5480 - val_mae: 1.8637 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9694 - mse: 6.0206 - mae: 1.2578\n",
      "Epoch 14: val_loss did not improve from 1.54568\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9694 - mse: 6.0206 - mae: 1.2578 - val_loss: 1.5871 - val_mse: 11.7058 - val_mae: 1.9104 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9671 - mse: 5.9914 - mae: 1.2582\n",
      "Epoch 15: val_loss did not improve from 1.54568\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9675 - mse: 5.9976 - mae: 1.2586 - val_loss: 1.5849 - val_mse: 12.0648 - val_mae: 1.9441 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.9791 - mse: 6.0202 - mae: 1.2722\n",
      "Epoch 16: val_loss improved from 1.54568 to 1.49451, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9846 - mse: 6.1008 - mae: 1.2779 - val_loss: 1.4945 - val_mse: 10.8258 - val_mae: 1.8182 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "1302/1315 [============================>.] - ETA: 0s - loss: 0.9654 - mse: 5.9500 - mae: 1.2553\n",
      "Epoch 17: val_loss improved from 1.49451 to 1.47307, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9704 - mse: 6.0030 - mae: 1.2605 - val_loss: 1.4731 - val_mse: 11.0264 - val_mae: 1.7939 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9504 - mse: 5.9168 - mae: 1.2359\n",
      "Epoch 18: val_loss did not improve from 1.47307\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9504 - mse: 5.9168 - mae: 1.2359 - val_loss: 1.5782 - val_mse: 11.6707 - val_mae: 1.8991 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9592 - mse: 5.9486 - mae: 1.2465\n",
      "Epoch 19: val_loss did not improve from 1.47307\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9592 - mse: 5.9486 - mae: 1.2465 - val_loss: 1.6904 - val_mse: 12.8166 - val_mae: 2.0177 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9699 - mse: 5.9972 - mae: 1.2561\n",
      "Epoch 20: val_loss improved from 1.47307 to 1.43497, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9698 - mse: 5.9963 - mae: 1.2560 - val_loss: 1.4350 - val_mse: 10.4283 - val_mae: 1.7518 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9521 - mse: 5.9421 - mae: 1.2369\n",
      "Epoch 21: val_loss did not improve from 1.43497\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9528 - mse: 5.9466 - mae: 1.2377 - val_loss: 1.4692 - val_mse: 10.7580 - val_mae: 1.7935 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9530 - mse: 5.8912 - mae: 1.2383\n",
      "Epoch 22: val_loss did not improve from 1.43497\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9530 - mse: 5.8912 - mae: 1.2383 - val_loss: 1.4695 - val_mse: 10.7169 - val_mae: 1.7930 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9696 - mse: 6.0047 - mae: 1.2548\n",
      "Epoch 23: val_loss improved from 1.43497 to 1.43341, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9697 - mse: 6.0068 - mae: 1.2548 - val_loss: 1.4334 - val_mse: 10.1815 - val_mae: 1.7712 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.9412 - mse: 5.8514 - mae: 1.2267\n",
      "Epoch 24: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9430 - mse: 5.8732 - mae: 1.2286 - val_loss: 1.6098 - val_mse: 11.8768 - val_mae: 1.9241 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.9456 - mse: 5.8515 - mae: 1.2327\n",
      "Epoch 25: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9496 - mse: 5.8944 - mae: 1.2369 - val_loss: 1.7713 - val_mse: 13.7937 - val_mae: 2.1011 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9610 - mse: 5.9449 - mae: 1.2492\n",
      "Epoch 26: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9611 - mse: 5.9464 - mae: 1.2492 - val_loss: 1.5572 - val_mse: 11.5545 - val_mae: 1.8752 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "1298/1315 [============================>.] - ETA: 0s - loss: 0.9472 - mse: 5.8835 - mae: 1.2319\n",
      "Epoch 27: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9514 - mse: 5.9223 - mae: 1.2363 - val_loss: 1.5100 - val_mse: 11.1578 - val_mae: 1.8354 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.9361 - mse: 5.7355 - mae: 1.2214\n",
      "Epoch 28: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9408 - mse: 5.8030 - mae: 1.2263 - val_loss: 1.4723 - val_mse: 10.9077 - val_mae: 1.7987 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.9491 - mse: 5.8583 - mae: 1.2360\n",
      "Epoch 29: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9512 - mse: 5.8784 - mae: 1.2382 - val_loss: 1.5014 - val_mse: 10.9754 - val_mae: 1.8215 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "1294/1315 [============================>.] - ETA: 0s - loss: 0.9411 - mse: 5.8123 - mae: 1.2261\n",
      "Epoch 30: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9479 - mse: 5.8814 - mae: 1.2333 - val_loss: 1.5361 - val_mse: 11.3630 - val_mae: 1.8638 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "1305/1315 [============================>.] - ETA: 0s - loss: 0.9465 - mse: 5.7989 - mae: 1.2306\n",
      "Epoch 31: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9502 - mse: 5.8532 - mae: 1.2344 - val_loss: 1.6485 - val_mse: 12.5765 - val_mae: 1.9789 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.9423 - mse: 5.7792 - mae: 1.2289\n",
      "Epoch 32: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9437 - mse: 5.8000 - mae: 1.2302 - val_loss: 1.4931 - val_mse: 10.7307 - val_mae: 1.8341 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.9466 - mse: 5.8292 - mae: 1.2311\n",
      "Epoch 33: val_loss did not improve from 1.43341\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9497 - mse: 5.8640 - mae: 1.2344 - val_loss: 1.4490 - val_mse: 10.4045 - val_mae: 1.7680 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.9107 - mse: 5.5550 - mae: 1.1919\n",
      "Epoch 34: val_loss improved from 1.43341 to 1.39305, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9177 - mse: 5.6437 - mae: 1.1990 - val_loss: 1.3930 - val_mse: 9.9679 - val_mae: 1.7126 - lr: 5.0000e-04\n",
      "Epoch 35/500\n",
      "1296/1315 [============================>.] - ETA: 0s - loss: 0.9031 - mse: 5.5174 - mae: 1.1828\n",
      "Epoch 35: val_loss did not improve from 1.39305\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9093 - mse: 5.5972 - mae: 1.1892 - val_loss: 1.3962 - val_mse: 9.8991 - val_mae: 1.7110 - lr: 5.0000e-04\n",
      "Epoch 36/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.9036 - mse: 5.5510 - mae: 1.1826\n",
      "Epoch 36: val_loss did not improve from 1.39305\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9058 - mse: 5.5756 - mae: 1.1849 - val_loss: 1.3999 - val_mse: 9.9468 - val_mae: 1.7093 - lr: 5.0000e-04\n",
      "Epoch 37/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.9005 - mse: 5.5103 - mae: 1.1791\n",
      "Epoch 37: val_loss did not improve from 1.39305\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9046 - mse: 5.5556 - mae: 1.1834 - val_loss: 1.4081 - val_mse: 10.0840 - val_mae: 1.7248 - lr: 5.0000e-04\n",
      "Epoch 38/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8993 - mse: 5.5102 - mae: 1.1783\n",
      "Epoch 38: val_loss improved from 1.39305 to 1.38250, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9032 - mse: 5.5544 - mae: 1.1824 - val_loss: 1.3825 - val_mse: 9.9241 - val_mae: 1.7236 - lr: 5.0000e-04\n",
      "Epoch 39/500\n",
      "1305/1315 [============================>.] - ETA: 0s - loss: 0.8998 - mse: 5.5501 - mae: 1.1776\n",
      "Epoch 39: val_loss did not improve from 1.38250\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9022 - mse: 5.5650 - mae: 1.1802 - val_loss: 1.4197 - val_mse: 10.0707 - val_mae: 1.7438 - lr: 5.0000e-04\n",
      "Epoch 40/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8928 - mse: 5.4607 - mae: 1.1709\n",
      "Epoch 40: val_loss did not improve from 1.38250\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8995 - mse: 5.5236 - mae: 1.1781 - val_loss: 1.4522 - val_mse: 10.4513 - val_mae: 1.7750 - lr: 5.0000e-04\n",
      "Epoch 41/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8941 - mse: 5.4925 - mae: 1.1720\n",
      "Epoch 41: val_loss improved from 1.38250 to 1.38098, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8943 - mse: 5.4943 - mae: 1.1722 - val_loss: 1.3810 - val_mse: 9.8982 - val_mae: 1.7042 - lr: 5.0000e-04\n",
      "Epoch 42/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8994 - mse: 5.5476 - mae: 1.1769\n",
      "Epoch 42: val_loss improved from 1.38098 to 1.34243, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.9011 - mse: 5.5686 - mae: 1.1787 - val_loss: 1.3424 - val_mse: 9.9229 - val_mae: 1.6620 - lr: 5.0000e-04\n",
      "Epoch 43/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8943 - mse: 5.5012 - mae: 1.1726\n",
      "Epoch 43: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8950 - mse: 5.5105 - mae: 1.1733 - val_loss: 1.4102 - val_mse: 9.9796 - val_mae: 1.7279 - lr: 5.0000e-04\n",
      "Epoch 44/500\n",
      "1299/1315 [============================>.] - ETA: 0s - loss: 0.8866 - mse: 5.4120 - mae: 1.1640\n",
      "Epoch 44: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8914 - mse: 5.4734 - mae: 1.1693 - val_loss: 1.3464 - val_mse: 9.5936 - val_mae: 1.6651 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8907 - mse: 5.4689 - mae: 1.1691\n",
      "Epoch 45: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8912 - mse: 5.4733 - mae: 1.1696 - val_loss: 1.4499 - val_mse: 10.2761 - val_mae: 1.7693 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8901 - mse: 5.4547 - mae: 1.1682\n",
      "Epoch 46: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8921 - mse: 5.4769 - mae: 1.1704 - val_loss: 1.3683 - val_mse: 9.7143 - val_mae: 1.6937 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8921 - mse: 5.4737 - mae: 1.1713\n",
      "Epoch 47: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 5s 3ms/step - loss: 0.8970 - mse: 5.5243 - mae: 1.1764 - val_loss: 1.3916 - val_mse: 10.0679 - val_mae: 1.7030 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8915 - mse: 5.4795 - mae: 1.1694\n",
      "Epoch 48: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8928 - mse: 5.4916 - mae: 1.1709 - val_loss: 1.3883 - val_mse: 9.9391 - val_mae: 1.7125 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8893 - mse: 5.4687 - mae: 1.1670\n",
      "Epoch 49: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8907 - mse: 5.4843 - mae: 1.1685 - val_loss: 1.4114 - val_mse: 9.9982 - val_mae: 1.7268 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8870 - mse: 5.4192 - mae: 1.1666\n",
      "Epoch 50: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8876 - mse: 5.4257 - mae: 1.1673 - val_loss: 1.3873 - val_mse: 10.0150 - val_mae: 1.6982 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8819 - mse: 5.4139 - mae: 1.1600\n",
      "Epoch 51: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8852 - mse: 5.4469 - mae: 1.1635 - val_loss: 1.3987 - val_mse: 10.0038 - val_mae: 1.7151 - lr: 5.0000e-04\n",
      "Epoch 52/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8847 - mse: 5.4166 - mae: 1.1622\n",
      "Epoch 52: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8901 - mse: 5.4690 - mae: 1.1680 - val_loss: 1.3967 - val_mse: 9.9125 - val_mae: 1.7050 - lr: 5.0000e-04\n",
      "Epoch 53/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8750 - mse: 5.3444 - mae: 1.1514\n",
      "Epoch 53: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8765 - mse: 5.3608 - mae: 1.1531 - val_loss: 1.3922 - val_mse: 9.8335 - val_mae: 1.7078 - lr: 2.5000e-04\n",
      "Epoch 54/500\n",
      "1296/1315 [============================>.] - ETA: 0s - loss: 0.8713 - mse: 5.3106 - mae: 1.1471\n",
      "Epoch 54: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8764 - mse: 5.3635 - mae: 1.1526 - val_loss: 1.3754 - val_mse: 9.7176 - val_mae: 1.6876 - lr: 2.5000e-04\n",
      "Epoch 55/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8653 - mse: 5.2719 - mae: 1.1418\n",
      "Epoch 55: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8737 - mse: 5.3515 - mae: 1.1508 - val_loss: 1.3656 - val_mse: 9.7477 - val_mae: 1.6816 - lr: 2.5000e-04\n",
      "Epoch 56/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8657 - mse: 5.2745 - mae: 1.1417\n",
      "Epoch 56: val_loss did not improve from 1.34243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8709 - mse: 5.3353 - mae: 1.1470 - val_loss: 1.3712 - val_mse: 9.6954 - val_mae: 1.6863 - lr: 2.5000e-04\n",
      "Epoch 57/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8688 - mse: 5.3156 - mae: 1.1443\n",
      "Epoch 57: val_loss improved from 1.34243 to 1.34037, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8708 - mse: 5.3391 - mae: 1.1464 - val_loss: 1.3404 - val_mse: 9.5502 - val_mae: 1.6598 - lr: 2.5000e-04\n",
      "Epoch 58/500\n",
      "1295/1315 [============================>.] - ETA: 0s - loss: 0.8630 - mse: 5.2776 - mae: 1.1383\n",
      "Epoch 58: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8680 - mse: 5.3222 - mae: 1.1437 - val_loss: 1.3507 - val_mse: 9.5783 - val_mae: 1.6666 - lr: 2.5000e-04\n",
      "Epoch 59/500\n",
      "1302/1315 [============================>.] - ETA: 0s - loss: 0.8604 - mse: 5.2346 - mae: 1.1346\n",
      "Epoch 59: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8672 - mse: 5.3188 - mae: 1.1417 - val_loss: 1.3708 - val_mse: 9.7166 - val_mae: 1.6880 - lr: 2.5000e-04\n",
      "Epoch 60/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8654 - mse: 5.3097 - mae: 1.1410\n",
      "Epoch 60: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8677 - mse: 5.3345 - mae: 1.1434 - val_loss: 1.3408 - val_mse: 9.6154 - val_mae: 1.6516 - lr: 2.5000e-04\n",
      "Epoch 61/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8601 - mse: 5.2735 - mae: 1.1344\n",
      "Epoch 61: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8631 - mse: 5.3010 - mae: 1.1376 - val_loss: 1.3487 - val_mse: 9.5540 - val_mae: 1.6585 - lr: 2.5000e-04\n",
      "Epoch 62/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8620 - mse: 5.2799 - mae: 1.1365\n",
      "Epoch 62: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8636 - mse: 5.2967 - mae: 1.1381 - val_loss: 1.3661 - val_mse: 9.7072 - val_mae: 1.6863 - lr: 2.5000e-04\n",
      "Epoch 63/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8628 - mse: 5.2665 - mae: 1.1374\n",
      "Epoch 63: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8651 - mse: 5.3019 - mae: 1.1398 - val_loss: 1.3693 - val_mse: 9.6934 - val_mae: 1.6826 - lr: 2.5000e-04\n",
      "Epoch 64/500\n",
      "1293/1315 [============================>.] - ETA: 0s - loss: 0.8551 - mse: 5.2005 - mae: 1.1283\n",
      "Epoch 64: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8620 - mse: 5.2901 - mae: 1.1357 - val_loss: 1.3704 - val_mse: 9.7248 - val_mae: 1.6861 - lr: 2.5000e-04\n",
      "Epoch 65/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8612 - mse: 5.2663 - mae: 1.1358\n",
      "Epoch 65: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8624 - mse: 5.2785 - mae: 1.1370 - val_loss: 1.3534 - val_mse: 9.6677 - val_mae: 1.6684 - lr: 2.5000e-04\n",
      "Epoch 66/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8604 - mse: 5.2595 - mae: 1.1345\n",
      "Epoch 66: val_loss did not improve from 1.34037\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8642 - mse: 5.2938 - mae: 1.1385 - val_loss: 1.3500 - val_mse: 9.6161 - val_mae: 1.6641 - lr: 2.5000e-04\n",
      "Epoch 67/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8612 - mse: 5.2851 - mae: 1.1355\n",
      "Epoch 67: val_loss improved from 1.34037 to 1.33842, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8616 - mse: 5.2865 - mae: 1.1359 - val_loss: 1.3384 - val_mse: 9.6061 - val_mae: 1.6545 - lr: 2.5000e-04\n",
      "Epoch 68/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8609 - mse: 5.2628 - mae: 1.1350\n",
      "Epoch 68: val_loss improved from 1.33842 to 1.33385, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8609 - mse: 5.2628 - mae: 1.1350 - val_loss: 1.3338 - val_mse: 9.5437 - val_mae: 1.6432 - lr: 2.5000e-04\n",
      "Epoch 69/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8589 - mse: 5.2590 - mae: 1.1324\n",
      "Epoch 69: val_loss did not improve from 1.33385\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8600 - mse: 5.2685 - mae: 1.1335 - val_loss: 1.3796 - val_mse: 9.7998 - val_mae: 1.6965 - lr: 2.5000e-04\n",
      "Epoch 70/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8567 - mse: 5.2548 - mae: 1.1302\n",
      "Epoch 70: val_loss improved from 1.33385 to 1.33099, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8567 - mse: 5.2548 - mae: 1.1302 - val_loss: 1.3310 - val_mse: 9.5199 - val_mae: 1.6457 - lr: 2.5000e-04\n",
      "Epoch 71/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8583 - mse: 5.2427 - mae: 1.1321\n",
      "Epoch 71: val_loss did not improve from 1.33099\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8600 - mse: 5.2744 - mae: 1.1339 - val_loss: 1.3765 - val_mse: 9.7465 - val_mae: 1.6997 - lr: 2.5000e-04\n",
      "Epoch 72/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8562 - mse: 5.2401 - mae: 1.1295\n",
      "Epoch 72: val_loss did not improve from 1.33099\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8571 - mse: 5.2470 - mae: 1.1305 - val_loss: 1.3333 - val_mse: 9.5151 - val_mae: 1.6474 - lr: 2.5000e-04\n",
      "Epoch 73/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8533 - mse: 5.2108 - mae: 1.1261\n",
      "Epoch 73: val_loss improved from 1.33099 to 1.32243, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8551 - mse: 5.2499 - mae: 1.1279 - val_loss: 1.3224 - val_mse: 9.4681 - val_mae: 1.6328 - lr: 2.5000e-04\n",
      "Epoch 74/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8536 - mse: 5.2263 - mae: 1.1260\n",
      "Epoch 74: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8541 - mse: 5.2346 - mae: 1.1266 - val_loss: 1.3502 - val_mse: 9.6970 - val_mae: 1.6642 - lr: 2.5000e-04\n",
      "Epoch 75/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8553 - mse: 5.2429 - mae: 1.1280\n",
      "Epoch 75: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8558 - mse: 5.2495 - mae: 1.1285 - val_loss: 1.3590 - val_mse: 9.6630 - val_mae: 1.6793 - lr: 2.5000e-04\n",
      "Epoch 76/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.8498 - mse: 5.1849 - mae: 1.1228\n",
      "Epoch 76: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8543 - mse: 5.2242 - mae: 1.1276 - val_loss: 1.3674 - val_mse: 9.7147 - val_mae: 1.6825 - lr: 2.5000e-04\n",
      "Epoch 77/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8503 - mse: 5.1876 - mae: 1.1227\n",
      "Epoch 77: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8538 - mse: 5.2295 - mae: 1.1264 - val_loss: 1.3602 - val_mse: 9.6831 - val_mae: 1.6719 - lr: 2.5000e-04\n",
      "Epoch 78/500\n",
      "1299/1315 [============================>.] - ETA: 0s - loss: 0.8492 - mse: 5.1724 - mae: 1.1218\n",
      "Epoch 78: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8546 - mse: 5.2318 - mae: 1.1274 - val_loss: 1.3525 - val_mse: 9.6522 - val_mae: 1.6629 - lr: 2.5000e-04\n",
      "Epoch 79/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8505 - mse: 5.1878 - mae: 1.1224\n",
      "Epoch 79: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8525 - mse: 5.2194 - mae: 1.1246 - val_loss: 1.3531 - val_mse: 9.6518 - val_mae: 1.6619 - lr: 2.5000e-04\n",
      "Epoch 80/500\n",
      "1302/1315 [============================>.] - ETA: 0s - loss: 0.8469 - mse: 5.1627 - mae: 1.1187\n",
      "Epoch 80: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8509 - mse: 5.2031 - mae: 1.1229 - val_loss: 1.3412 - val_mse: 9.5851 - val_mae: 1.6572 - lr: 2.5000e-04\n",
      "Epoch 81/500\n",
      "1304/1315 [============================>.] - ETA: 0s - loss: 0.8488 - mse: 5.1699 - mae: 1.1205\n",
      "Epoch 81: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8521 - mse: 5.2242 - mae: 1.1240 - val_loss: 1.3337 - val_mse: 9.5504 - val_mae: 1.6676 - lr: 2.5000e-04\n",
      "Epoch 82/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8495 - mse: 5.2003 - mae: 1.1211\n",
      "Epoch 82: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8505 - mse: 5.2085 - mae: 1.1223 - val_loss: 1.3529 - val_mse: 9.6399 - val_mae: 1.6640 - lr: 2.5000e-04\n",
      "Epoch 83/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8504 - mse: 5.2155 - mae: 1.1227\n",
      "Epoch 83: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8503 - mse: 5.2147 - mae: 1.1227 - val_loss: 1.3606 - val_mse: 9.6813 - val_mae: 1.6747 - lr: 2.5000e-04\n",
      "Epoch 84/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8530 - mse: 5.1871 - mae: 1.1265\n",
      "Epoch 84: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8574 - mse: 5.2333 - mae: 1.1311 - val_loss: 1.3372 - val_mse: 9.5229 - val_mae: 1.6493 - lr: 1.2500e-04\n",
      "Epoch 85/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8495 - mse: 5.1599 - mae: 1.1228\n",
      "Epoch 85: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8536 - mse: 5.2181 - mae: 1.1271 - val_loss: 1.3402 - val_mse: 9.5489 - val_mae: 1.6527 - lr: 1.2500e-04\n",
      "Epoch 86/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8514 - mse: 5.1969 - mae: 1.1242\n",
      "Epoch 86: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8520 - mse: 5.2154 - mae: 1.1248 - val_loss: 1.3380 - val_mse: 9.5250 - val_mae: 1.6505 - lr: 1.2500e-04\n",
      "Epoch 87/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8470 - mse: 5.1621 - mae: 1.1195\n",
      "Epoch 87: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8492 - mse: 5.2001 - mae: 1.1217 - val_loss: 1.3250 - val_mse: 9.4544 - val_mae: 1.6374 - lr: 1.2500e-04\n",
      "Epoch 88/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8440 - mse: 5.1602 - mae: 1.1163\n",
      "Epoch 88: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8494 - mse: 5.2061 - mae: 1.1221 - val_loss: 1.3338 - val_mse: 9.4842 - val_mae: 1.6459 - lr: 1.2500e-04\n",
      "Epoch 89/500\n",
      "1304/1315 [============================>.] - ETA: 0s - loss: 0.8448 - mse: 5.1583 - mae: 1.1166\n",
      "Epoch 89: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8478 - mse: 5.1868 - mae: 1.1198 - val_loss: 1.3399 - val_mse: 9.5382 - val_mae: 1.6572 - lr: 1.2500e-04\n",
      "Epoch 90/500\n",
      "1305/1315 [============================>.] - ETA: 0s - loss: 0.8467 - mse: 5.1695 - mae: 1.1192\n",
      "Epoch 90: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8496 - mse: 5.1911 - mae: 1.1223 - val_loss: 1.3248 - val_mse: 9.4132 - val_mae: 1.6392 - lr: 1.2500e-04\n",
      "Epoch 91/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8493 - mse: 5.1888 - mae: 1.1219\n",
      "Epoch 91: val_loss did not improve from 1.32243\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8493 - mse: 5.1888 - mae: 1.1219 - val_loss: 1.3293 - val_mse: 9.4363 - val_mae: 1.6450 - lr: 1.2500e-04\n",
      "Epoch 92/500\n",
      "1304/1315 [============================>.] - ETA: 0s - loss: 0.8455 - mse: 5.1433 - mae: 1.1184\n",
      "Epoch 92: val_loss improved from 1.32243 to 1.31383, saving model to base_stne_rnn_weight.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8485 - mse: 5.1854 - mae: 1.1216 - val_loss: 1.3138 - val_mse: 9.3681 - val_mae: 1.6275 - lr: 1.2500e-04\n",
      "Epoch 93/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8466 - mse: 5.1714 - mae: 1.1184\n",
      "Epoch 93: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8467 - mse: 5.1732 - mae: 1.1186 - val_loss: 1.3224 - val_mse: 9.4265 - val_mae: 1.6339 - lr: 1.2500e-04\n",
      "Epoch 94/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8424 - mse: 5.1041 - mae: 1.1144\n",
      "Epoch 94: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8485 - mse: 5.1842 - mae: 1.1208 - val_loss: 1.3299 - val_mse: 9.5041 - val_mae: 1.6420 - lr: 1.2500e-04\n",
      "Epoch 95/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8479 - mse: 5.1722 - mae: 1.1209\n",
      "Epoch 95: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8502 - mse: 5.1952 - mae: 1.1234 - val_loss: 1.3321 - val_mse: 9.4744 - val_mae: 1.6485 - lr: 1.2500e-04\n",
      "Epoch 96/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8471 - mse: 5.1736 - mae: 1.1192\n",
      "Epoch 96: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8500 - mse: 5.1993 - mae: 1.1222 - val_loss: 1.3283 - val_mse: 9.4694 - val_mae: 1.6414 - lr: 1.2500e-04\n",
      "Epoch 97/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8469 - mse: 5.1685 - mae: 1.1189\n",
      "Epoch 97: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8477 - mse: 5.1724 - mae: 1.1197 - val_loss: 1.3255 - val_mse: 9.4149 - val_mae: 1.6391 - lr: 1.2500e-04\n",
      "Epoch 98/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8494 - mse: 5.1754 - mae: 1.1219\n",
      "Epoch 98: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8498 - mse: 5.1826 - mae: 1.1223 - val_loss: 1.3342 - val_mse: 9.4799 - val_mae: 1.6492 - lr: 1.2500e-04\n",
      "Epoch 99/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8423 - mse: 5.1058 - mae: 1.1153\n",
      "Epoch 99: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8467 - mse: 5.1570 - mae: 1.1198 - val_loss: 1.3396 - val_mse: 9.5334 - val_mae: 1.6523 - lr: 1.2500e-04\n",
      "Epoch 100/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8482 - mse: 5.1714 - mae: 1.1210\n",
      "Epoch 100: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8495 - mse: 5.1910 - mae: 1.1223 - val_loss: 1.3336 - val_mse: 9.4993 - val_mae: 1.6486 - lr: 1.2500e-04\n",
      "Epoch 101/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.8433 - mse: 5.1116 - mae: 1.1151\n",
      "Epoch 101: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8484 - mse: 5.1795 - mae: 1.1204 - val_loss: 1.3294 - val_mse: 9.4897 - val_mae: 1.6416 - lr: 1.2500e-04\n",
      "Epoch 102/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8472 - mse: 5.1863 - mae: 1.1196\n",
      "Epoch 102: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8470 - mse: 5.1850 - mae: 1.1194 - val_loss: 1.3346 - val_mse: 9.5087 - val_mae: 1.6463 - lr: 1.2500e-04\n",
      "Epoch 103/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8537 - mse: 5.1484 - mae: 1.1287\n",
      "Epoch 103: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8594 - mse: 5.2262 - mae: 1.1346 - val_loss: 1.3311 - val_mse: 9.4568 - val_mae: 1.6417 - lr: 6.2500e-05\n",
      "Epoch 104/500\n",
      "1305/1315 [============================>.] - ETA: 0s - loss: 0.8536 - mse: 5.1809 - mae: 1.1288\n",
      "Epoch 104: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8554 - mse: 5.2050 - mae: 1.1307 - val_loss: 1.3261 - val_mse: 9.4319 - val_mae: 1.6359 - lr: 6.2500e-05\n",
      "Epoch 105/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8511 - mse: 5.1643 - mae: 1.1257\n",
      "Epoch 105: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8539 - mse: 5.1916 - mae: 1.1287 - val_loss: 1.3189 - val_mse: 9.3994 - val_mae: 1.6293 - lr: 6.2500e-05\n",
      "Epoch 106/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8527 - mse: 5.1836 - mae: 1.1272\n",
      "Epoch 106: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8540 - mse: 5.1924 - mae: 1.1286 - val_loss: 1.3293 - val_mse: 9.4704 - val_mae: 1.6395 - lr: 6.2500e-05\n",
      "Epoch 107/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.8488 - mse: 5.1282 - mae: 1.1228\n",
      "Epoch 107: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8539 - mse: 5.1922 - mae: 1.1282 - val_loss: 1.3272 - val_mse: 9.4357 - val_mae: 1.6377 - lr: 6.2500e-05\n",
      "Epoch 108/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8520 - mse: 5.1741 - mae: 1.1258\n",
      "Epoch 108: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8541 - mse: 5.1928 - mae: 1.1280 - val_loss: 1.3213 - val_mse: 9.4077 - val_mae: 1.6320 - lr: 6.2500e-05\n",
      "Epoch 109/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8529 - mse: 5.1929 - mae: 1.1268\n",
      "Epoch 109: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8524 - mse: 5.1886 - mae: 1.1263 - val_loss: 1.3229 - val_mse: 9.4056 - val_mae: 1.6342 - lr: 6.2500e-05\n",
      "Epoch 110/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8526 - mse: 5.1925 - mae: 1.1264\n",
      "Epoch 110: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8527 - mse: 5.1936 - mae: 1.1265 - val_loss: 1.3249 - val_mse: 9.4201 - val_mae: 1.6349 - lr: 6.2500e-05\n",
      "Epoch 111/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8507 - mse: 5.1681 - mae: 1.1245\n",
      "Epoch 111: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8520 - mse: 5.1753 - mae: 1.1260 - val_loss: 1.3295 - val_mse: 9.4486 - val_mae: 1.6397 - lr: 6.2500e-05\n",
      "Epoch 112/500\n",
      "1299/1315 [============================>.] - ETA: 0s - loss: 0.8461 - mse: 5.1233 - mae: 1.1198\n",
      "Epoch 112: val_loss did not improve from 1.31383\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8517 - mse: 5.1927 - mae: 1.1256 - val_loss: 1.3263 - val_mse: 9.4094 - val_mae: 1.6372 - lr: 6.2500e-05\n",
      "Epoch 1/500\n",
      "   1311/Unknown - 4s 3ms/step - loss: 0.8614 - mse: 5.2235 - mae: 1.1372\n",
      "Epoch 1: val_loss improved from inf to 1.32809, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8612 - mse: 5.2210 - mae: 1.1371 - val_loss: 1.3281 - val_mse: 9.4408 - val_mae: 1.6367 - lr: 3.1250e-05\n",
      "Epoch 2/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8567 - mse: 5.1936 - mae: 1.1316\n",
      "Epoch 2: val_loss did not improve from 1.32809\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8573 - mse: 5.2024 - mae: 1.1323 - val_loss: 1.3286 - val_mse: 9.4473 - val_mae: 1.6373 - lr: 3.1250e-05\n",
      "Epoch 3/500\n",
      "1296/1315 [============================>.] - ETA: 0s - loss: 0.8535 - mse: 5.1706 - mae: 1.1280\n",
      "Epoch 3: val_loss did not improve from 1.32809\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8565 - mse: 5.1989 - mae: 1.1312 - val_loss: 1.3283 - val_mse: 9.4479 - val_mae: 1.6371 - lr: 3.1250e-05\n",
      "Epoch 4/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8557 - mse: 5.1953 - mae: 1.1303\n",
      "Epoch 4: val_loss did not improve from 1.32809\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8560 - mse: 5.1971 - mae: 1.1306 - val_loss: 1.3282 - val_mse: 9.4488 - val_mae: 1.6369 - lr: 3.1250e-05\n",
      "Epoch 5/500\n",
      "1299/1315 [============================>.] - ETA: 0s - loss: 0.8543 - mse: 5.1817 - mae: 1.1286\n",
      "Epoch 5: val_loss improved from 1.32809 to 1.32808, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8556 - mse: 5.1957 - mae: 1.1301 - val_loss: 1.3281 - val_mse: 9.4496 - val_mae: 1.6368 - lr: 3.1250e-05\n",
      "Epoch 6/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.8542 - mse: 5.1801 - mae: 1.1285\n",
      "Epoch 6: val_loss improved from 1.32808 to 1.32787, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8553 - mse: 5.1943 - mae: 1.1297 - val_loss: 1.3279 - val_mse: 9.4499 - val_mae: 1.6366 - lr: 3.1250e-05\n",
      "Epoch 7/500\n",
      "1302/1315 [============================>.] - ETA: 0s - loss: 0.8540 - mse: 5.1807 - mae: 1.1283\n",
      "Epoch 7: val_loss improved from 1.32787 to 1.32783, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8551 - mse: 5.1929 - mae: 1.1294 - val_loss: 1.3278 - val_mse: 9.4503 - val_mae: 1.6367 - lr: 3.1250e-05\n",
      "Epoch 8/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8548 - mse: 5.1919 - mae: 1.1292\n",
      "Epoch 8: val_loss improved from 1.32783 to 1.32760, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8548 - mse: 5.1919 - mae: 1.1292 - val_loss: 1.3276 - val_mse: 9.4500 - val_mae: 1.6365 - lr: 3.1250e-05\n",
      "Epoch 9/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8540 - mse: 5.1816 - mae: 1.1282\n",
      "Epoch 9: val_loss improved from 1.32760 to 1.32733, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8546 - mse: 5.1905 - mae: 1.1289 - val_loss: 1.3273 - val_mse: 9.4489 - val_mae: 1.6362 - lr: 3.1250e-05\n",
      "Epoch 10/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8544 - mse: 5.1924 - mae: 1.1286\n",
      "Epoch 10: val_loss improved from 1.32733 to 1.32706, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8544 - mse: 5.1897 - mae: 1.1286 - val_loss: 1.3271 - val_mse: 9.4480 - val_mae: 1.6359 - lr: 3.1250e-05\n",
      "Epoch 11/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8542 - mse: 5.1910 - mae: 1.1284\n",
      "Epoch 11: val_loss improved from 1.32706 to 1.32696, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8541 - mse: 5.1883 - mae: 1.1284 - val_loss: 1.3270 - val_mse: 9.4479 - val_mae: 1.6358 - lr: 3.1250e-05\n",
      "Epoch 12/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8518 - mse: 5.1662 - mae: 1.1259\n",
      "Epoch 12: val_loss improved from 1.32696 to 1.32684, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8539 - mse: 5.1874 - mae: 1.1282 - val_loss: 1.3268 - val_mse: 9.4476 - val_mae: 1.6357 - lr: 3.1250e-05\n",
      "Epoch 13/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8537 - mse: 5.1864 - mae: 1.1280\n",
      "Epoch 13: val_loss did not improve from 1.32684\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8537 - mse: 5.1864 - mae: 1.1280 - val_loss: 1.3269 - val_mse: 9.4481 - val_mae: 1.6358 - lr: 3.1250e-05\n",
      "Epoch 14/500\n",
      "1294/1315 [============================>.] - ETA: 0s - loss: 0.8504 - mse: 5.1552 - mae: 1.1244\n",
      "Epoch 14: val_loss did not improve from 1.32684\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8535 - mse: 5.1853 - mae: 1.1277 - val_loss: 1.3269 - val_mse: 9.4481 - val_mae: 1.6358 - lr: 3.1250e-05\n",
      "Epoch 15/500\n",
      "1302/1315 [============================>.] - ETA: 0s - loss: 0.8524 - mse: 5.1721 - mae: 1.1264\n",
      "Epoch 15: val_loss did not improve from 1.32684\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8534 - mse: 5.1843 - mae: 1.1275 - val_loss: 1.3270 - val_mse: 9.4484 - val_mae: 1.6358 - lr: 3.1250e-05\n",
      "Epoch 16/500\n",
      "1299/1315 [============================>.] - ETA: 0s - loss: 0.8518 - mse: 5.1692 - mae: 1.1258\n",
      "Epoch 16: val_loss improved from 1.32684 to 1.32676, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8532 - mse: 5.1833 - mae: 1.1273 - val_loss: 1.3268 - val_mse: 9.4475 - val_mae: 1.6356 - lr: 3.1250e-05\n",
      "Epoch 17/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8531 - mse: 5.1835 - mae: 1.1272\n",
      "Epoch 17: val_loss improved from 1.32676 to 1.32658, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8530 - mse: 5.1822 - mae: 1.1271 - val_loss: 1.3266 - val_mse: 9.4467 - val_mae: 1.6354 - lr: 3.1250e-05\n",
      "Epoch 18/500\n",
      "1305/1315 [============================>.] - ETA: 0s - loss: 0.8511 - mse: 5.1619 - mae: 1.1250\n",
      "Epoch 18: val_loss improved from 1.32658 to 1.32644, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8528 - mse: 5.1813 - mae: 1.1269 - val_loss: 1.3264 - val_mse: 9.4462 - val_mae: 1.6353 - lr: 3.1250e-05\n",
      "Epoch 19/500\n",
      "1305/1315 [============================>.] - ETA: 0s - loss: 0.8510 - mse: 5.1611 - mae: 1.1248\n",
      "Epoch 19: val_loss improved from 1.32644 to 1.32632, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8527 - mse: 5.1806 - mae: 1.1267 - val_loss: 1.3263 - val_mse: 9.4457 - val_mae: 1.6351 - lr: 3.1250e-05\n",
      "Epoch 20/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8528 - mse: 5.1827 - mae: 1.1268\n",
      "Epoch 20: val_loss improved from 1.32632 to 1.32586, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8525 - mse: 5.1795 - mae: 1.1265 - val_loss: 1.3259 - val_mse: 9.4431 - val_mae: 1.6346 - lr: 3.1250e-05\n",
      "Epoch 21/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8518 - mse: 5.1699 - mae: 1.1256\n",
      "Epoch 21: val_loss improved from 1.32586 to 1.32576, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8524 - mse: 5.1788 - mae: 1.1263 - val_loss: 1.3258 - val_mse: 9.4426 - val_mae: 1.6345 - lr: 3.1250e-05\n",
      "Epoch 22/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8519 - mse: 5.1762 - mae: 1.1259\n",
      "Epoch 22: val_loss improved from 1.32576 to 1.32547, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8522 - mse: 5.1779 - mae: 1.1261 - val_loss: 1.3255 - val_mse: 9.4410 - val_mae: 1.6343 - lr: 3.1250e-05\n",
      "Epoch 23/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.8510 - mse: 5.1631 - mae: 1.1248\n",
      "Epoch 23: val_loss improved from 1.32547 to 1.32541, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8521 - mse: 5.1773 - mae: 1.1260 - val_loss: 1.3254 - val_mse: 9.4414 - val_mae: 1.6342 - lr: 3.1250e-05\n",
      "Epoch 24/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8520 - mse: 5.1792 - mae: 1.1258\n",
      "Epoch 24: val_loss improved from 1.32541 to 1.32530, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8519 - mse: 5.1764 - mae: 1.1258 - val_loss: 1.3253 - val_mse: 9.4408 - val_mae: 1.6341 - lr: 3.1250e-05\n",
      "Epoch 25/500\n",
      "1305/1315 [============================>.] - ETA: 0s - loss: 0.8501 - mse: 5.1561 - mae: 1.1238\n",
      "Epoch 25: val_loss improved from 1.32530 to 1.32501, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8518 - mse: 5.1755 - mae: 1.1257 - val_loss: 1.3250 - val_mse: 9.4397 - val_mae: 1.6338 - lr: 3.1250e-05\n",
      "Epoch 26/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8509 - mse: 5.1683 - mae: 1.1247\n",
      "Epoch 26: val_loss improved from 1.32501 to 1.32468, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8516 - mse: 5.1746 - mae: 1.1255 - val_loss: 1.3247 - val_mse: 9.4382 - val_mae: 1.6334 - lr: 3.1250e-05\n",
      "Epoch 27/500\n",
      "1299/1315 [============================>.] - ETA: 0s - loss: 0.8502 - mse: 5.1600 - mae: 1.1238\n",
      "Epoch 27: val_loss improved from 1.32468 to 1.32407, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8515 - mse: 5.1740 - mae: 1.1254 - val_loss: 1.3241 - val_mse: 9.4352 - val_mae: 1.6328 - lr: 3.1250e-05\n",
      "Epoch 28/500\n",
      "1305/1315 [============================>.] - ETA: 0s - loss: 0.8497 - mse: 5.1536 - mae: 1.1233\n",
      "Epoch 28: val_loss improved from 1.32407 to 1.32375, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8514 - mse: 5.1730 - mae: 1.1252 - val_loss: 1.3238 - val_mse: 9.4332 - val_mae: 1.6325 - lr: 3.1250e-05\n",
      "Epoch 29/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8509 - mse: 5.1716 - mae: 1.1246\n",
      "Epoch 29: val_loss improved from 1.32375 to 1.32339, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8512 - mse: 5.1723 - mae: 1.1250 - val_loss: 1.3234 - val_mse: 9.4321 - val_mae: 1.6321 - lr: 3.1250e-05\n",
      "Epoch 30/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8491 - mse: 5.1504 - mae: 1.1226\n",
      "Epoch 30: val_loss improved from 1.32339 to 1.32307, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8511 - mse: 5.1715 - mae: 1.1249 - val_loss: 1.3231 - val_mse: 9.4314 - val_mae: 1.6318 - lr: 3.1250e-05\n",
      "Epoch 31/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8511 - mse: 5.1721 - mae: 1.1249\n",
      "Epoch 31: val_loss improved from 1.32307 to 1.32290, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8510 - mse: 5.1708 - mae: 1.1247 - val_loss: 1.3229 - val_mse: 9.4313 - val_mae: 1.6316 - lr: 3.1250e-05\n",
      "Epoch 32/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8511 - mse: 5.1726 - mae: 1.1247\n",
      "Epoch 32: val_loss improved from 1.32290 to 1.32271, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8508 - mse: 5.1700 - mae: 1.1246 - val_loss: 1.3227 - val_mse: 9.4306 - val_mae: 1.6314 - lr: 3.1250e-05\n",
      "Epoch 33/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8507 - mse: 5.1692 - mae: 1.1244\n",
      "Epoch 33: val_loss improved from 1.32271 to 1.32237, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8507 - mse: 5.1692 - mae: 1.1244 - val_loss: 1.3224 - val_mse: 9.4299 - val_mae: 1.6310 - lr: 3.1250e-05\n",
      "Epoch 34/500\n",
      "1304/1315 [============================>.] - ETA: 0s - loss: 0.8492 - mse: 5.1516 - mae: 1.1227\n",
      "Epoch 34: val_loss improved from 1.32237 to 1.32229, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8506 - mse: 5.1684 - mae: 1.1243 - val_loss: 1.3223 - val_mse: 9.4292 - val_mae: 1.6308 - lr: 3.1250e-05\n",
      "Epoch 35/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8507 - mse: 5.1702 - mae: 1.1242\n",
      "Epoch 35: val_loss improved from 1.32229 to 1.32228, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8504 - mse: 5.1675 - mae: 1.1241 - val_loss: 1.3223 - val_mse: 9.4297 - val_mae: 1.6308 - lr: 3.1250e-05\n",
      "Epoch 36/500\n",
      "1304/1315 [============================>.] - ETA: 0s - loss: 0.8490 - mse: 5.1501 - mae: 1.1224\n",
      "Epoch 36: val_loss improved from 1.32228 to 1.32220, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8503 - mse: 5.1669 - mae: 1.1240 - val_loss: 1.3222 - val_mse: 9.4301 - val_mae: 1.6307 - lr: 3.1250e-05\n",
      "Epoch 37/500\n",
      "1299/1315 [============================>.] - ETA: 0s - loss: 0.8488 - mse: 5.1519 - mae: 1.1223\n",
      "Epoch 37: val_loss improved from 1.32220 to 1.32214, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8502 - mse: 5.1659 - mae: 1.1238 - val_loss: 1.3221 - val_mse: 9.4298 - val_mae: 1.6306 - lr: 3.1250e-05\n",
      "Epoch 38/500\n",
      "1296/1315 [============================>.] - ETA: 0s - loss: 0.8471 - mse: 5.1368 - mae: 1.1204\n",
      "Epoch 38: val_loss did not improve from 1.32214\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8501 - mse: 5.1652 - mae: 1.1236 - val_loss: 1.3222 - val_mse: 9.4309 - val_mae: 1.6307 - lr: 3.1250e-05\n",
      "Epoch 39/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8479 - mse: 5.1431 - mae: 1.1213\n",
      "Epoch 39: val_loss improved from 1.32214 to 1.32186, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8499 - mse: 5.1643 - mae: 1.1235 - val_loss: 1.3219 - val_mse: 9.4289 - val_mae: 1.6302 - lr: 3.1250e-05\n",
      "Epoch 40/500\n",
      "1295/1315 [============================>.] - ETA: 0s - loss: 0.8467 - mse: 5.1324 - mae: 1.1200\n",
      "Epoch 40: val_loss improved from 1.32186 to 1.32185, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8498 - mse: 5.1635 - mae: 1.1233 - val_loss: 1.3218 - val_mse: 9.4285 - val_mae: 1.6303 - lr: 3.1250e-05\n",
      "Epoch 41/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8500 - mse: 5.1673 - mae: 1.1234\n",
      "Epoch 41: val_loss improved from 1.32185 to 1.32179, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8497 - mse: 5.1629 - mae: 1.1232 - val_loss: 1.3218 - val_mse: 9.4288 - val_mae: 1.6302 - lr: 3.1250e-05\n",
      "Epoch 42/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8498 - mse: 5.1645 - mae: 1.1231\n",
      "Epoch 42: val_loss improved from 1.32179 to 1.32153, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8495 - mse: 5.1619 - mae: 1.1230 - val_loss: 1.3215 - val_mse: 9.4272 - val_mae: 1.6299 - lr: 3.1250e-05\n",
      "Epoch 43/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8492 - mse: 5.1595 - mae: 1.1226\n",
      "Epoch 43: val_loss did not improve from 1.32153\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8494 - mse: 5.1611 - mae: 1.1228 - val_loss: 1.3217 - val_mse: 9.4279 - val_mae: 1.6300 - lr: 3.1250e-05\n",
      "Epoch 44/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.8482 - mse: 5.1464 - mae: 1.1215\n",
      "Epoch 44: val_loss improved from 1.32153 to 1.32148, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8493 - mse: 5.1605 - mae: 1.1227 - val_loss: 1.3215 - val_mse: 9.4274 - val_mae: 1.6298 - lr: 3.1250e-05\n",
      "Epoch 45/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8480 - mse: 5.1493 - mae: 1.1212\n",
      "Epoch 45: val_loss improved from 1.32148 to 1.32132, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8492 - mse: 5.1599 - mae: 1.1225 - val_loss: 1.3213 - val_mse: 9.4267 - val_mae: 1.6297 - lr: 3.1250e-05\n",
      "Epoch 46/500\n",
      "1298/1315 [============================>.] - ETA: 0s - loss: 0.8476 - mse: 5.1429 - mae: 1.1208\n",
      "Epoch 46: val_loss improved from 1.32132 to 1.32120, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8490 - mse: 5.1588 - mae: 1.1223 - val_loss: 1.3212 - val_mse: 9.4263 - val_mae: 1.6296 - lr: 3.1250e-05\n",
      "Epoch 47/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8483 - mse: 5.1519 - mae: 1.1214\n",
      "Epoch 47: val_loss improved from 1.32120 to 1.32097, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8489 - mse: 5.1583 - mae: 1.1223 - val_loss: 1.3210 - val_mse: 9.4250 - val_mae: 1.6293 - lr: 3.1250e-05\n",
      "Epoch 48/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8486 - mse: 5.1557 - mae: 1.1218\n",
      "Epoch 48: val_loss improved from 1.32097 to 1.32092, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8488 - mse: 5.1573 - mae: 1.1221 - val_loss: 1.3209 - val_mse: 9.4259 - val_mae: 1.6292 - lr: 3.1250e-05\n",
      "Epoch 49/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8489 - mse: 5.1584 - mae: 1.1221\n",
      "Epoch 49: val_loss improved from 1.32092 to 1.32069, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8487 - mse: 5.1570 - mae: 1.1220 - val_loss: 1.3207 - val_mse: 9.4243 - val_mae: 1.6289 - lr: 3.1250e-05\n",
      "Epoch 50/500\n",
      "1295/1315 [============================>.] - ETA: 0s - loss: 0.8454 - mse: 5.1245 - mae: 1.1184\n",
      "Epoch 50: val_loss improved from 1.32069 to 1.32060, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8485 - mse: 5.1557 - mae: 1.1218 - val_loss: 1.3206 - val_mse: 9.4239 - val_mae: 1.6289 - lr: 3.1250e-05\n",
      "Epoch 51/500\n",
      "1298/1315 [============================>.] - ETA: 0s - loss: 0.8471 - mse: 5.1392 - mae: 1.1201\n",
      "Epoch 51: val_loss improved from 1.32060 to 1.32048, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8485 - mse: 5.1552 - mae: 1.1217 - val_loss: 1.3205 - val_mse: 9.4239 - val_mae: 1.6287 - lr: 3.1250e-05\n",
      "Epoch 52/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8463 - mse: 5.1332 - mae: 1.1193\n",
      "Epoch 52: val_loss improved from 1.32048 to 1.32038, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8484 - mse: 5.1545 - mae: 1.1216 - val_loss: 1.3204 - val_mse: 9.4238 - val_mae: 1.6286 - lr: 3.1250e-05\n",
      "Epoch 53/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8485 - mse: 5.1565 - mae: 1.1216\n",
      "Epoch 53: val_loss did not improve from 1.32038\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8482 - mse: 5.1538 - mae: 1.1214 - val_loss: 1.3205 - val_mse: 9.4238 - val_mae: 1.6287 - lr: 3.1250e-05\n",
      "Epoch 54/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8470 - mse: 5.1426 - mae: 1.1200\n",
      "Epoch 54: val_loss improved from 1.32038 to 1.32026, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8482 - mse: 5.1532 - mae: 1.1213 - val_loss: 1.3203 - val_mse: 9.4236 - val_mae: 1.6285 - lr: 3.1250e-05\n",
      "Epoch 55/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8483 - mse: 5.1550 - mae: 1.1214\n",
      "Epoch 55: val_loss improved from 1.32026 to 1.31992, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8481 - mse: 5.1524 - mae: 1.1212 - val_loss: 1.3199 - val_mse: 9.4228 - val_mae: 1.6281 - lr: 3.1250e-05\n",
      "Epoch 56/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8474 - mse: 5.1429 - mae: 1.1203\n",
      "Epoch 56: val_loss did not improve from 1.31992\n",
      "1315/1315 [==============================] - 3s 3ms/step - loss: 0.8479 - mse: 5.1517 - mae: 1.1210 - val_loss: 1.3200 - val_mse: 9.4223 - val_mae: 1.6282 - lr: 3.1250e-05\n",
      "Epoch 57/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8473 - mse: 5.1422 - mae: 1.1202\n",
      "Epoch 57: val_loss improved from 1.31992 to 1.31975, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8478 - mse: 5.1510 - mae: 1.1209 - val_loss: 1.3198 - val_mse: 9.4230 - val_mae: 1.6279 - lr: 3.1250e-05\n",
      "Epoch 58/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.8466 - mse: 5.1362 - mae: 1.1195\n",
      "Epoch 58: val_loss did not improve from 1.31975\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8477 - mse: 5.1503 - mae: 1.1207 - val_loss: 1.3199 - val_mse: 9.4237 - val_mae: 1.6280 - lr: 3.1250e-05\n",
      "Epoch 59/500\n",
      "1304/1315 [============================>.] - ETA: 0s - loss: 0.8462 - mse: 5.1331 - mae: 1.1191\n",
      "Epoch 59: val_loss improved from 1.31975 to 1.31961, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8476 - mse: 5.1500 - mae: 1.1206 - val_loss: 1.3196 - val_mse: 9.4222 - val_mae: 1.6277 - lr: 3.1250e-05\n",
      "Epoch 60/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8472 - mse: 5.1483 - mae: 1.1200\n",
      "Epoch 60: val_loss improved from 1.31961 to 1.31954, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8475 - mse: 5.1490 - mae: 1.1205 - val_loss: 1.3195 - val_mse: 9.4219 - val_mae: 1.6276 - lr: 3.1250e-05\n",
      "Epoch 61/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8474 - mse: 5.1484 - mae: 1.1204\n",
      "Epoch 61: val_loss did not improve from 1.31954\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8474 - mse: 5.1484 - mae: 1.1204 - val_loss: 1.3196 - val_mse: 9.4228 - val_mae: 1.6277 - lr: 3.1250e-05\n",
      "Epoch 62/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8462 - mse: 5.1373 - mae: 1.1190\n",
      "Epoch 62: val_loss improved from 1.31954 to 1.31930, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8473 - mse: 5.1479 - mae: 1.1203 - val_loss: 1.3193 - val_mse: 9.4214 - val_mae: 1.6274 - lr: 3.1250e-05\n",
      "Epoch 63/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8473 - mse: 5.1486 - mae: 1.1203\n",
      "Epoch 63: val_loss did not improve from 1.31930\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8472 - mse: 5.1473 - mae: 1.1201 - val_loss: 1.3194 - val_mse: 9.4231 - val_mae: 1.6275 - lr: 3.1250e-05\n",
      "Epoch 64/500\n",
      "1295/1315 [============================>.] - ETA: 0s - loss: 0.8440 - mse: 5.1153 - mae: 1.1167\n",
      "Epoch 64: val_loss did not improve from 1.31930\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8471 - mse: 5.1465 - mae: 1.1200 - val_loss: 1.3194 - val_mse: 9.4229 - val_mae: 1.6274 - lr: 3.1250e-05\n",
      "Epoch 65/500\n",
      "1297/1315 [============================>.] - ETA: 0s - loss: 0.8449 - mse: 5.1246 - mae: 1.1176\n",
      "Epoch 65: val_loss improved from 1.31930 to 1.31915, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8470 - mse: 5.1460 - mae: 1.1199 - val_loss: 1.3191 - val_mse: 9.4220 - val_mae: 1.6272 - lr: 3.1250e-05\n",
      "Epoch 66/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8472 - mse: 5.1486 - mae: 1.1200\n",
      "Epoch 66: val_loss did not improve from 1.31915\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8469 - mse: 5.1452 - mae: 1.1197 - val_loss: 1.3193 - val_mse: 9.4233 - val_mae: 1.6273 - lr: 3.1250e-05\n",
      "Epoch 67/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8468 - mse: 5.1446 - mae: 1.1196\n",
      "Epoch 67: val_loss improved from 1.31915 to 1.31910, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8468 - mse: 5.1446 - mae: 1.1196 - val_loss: 1.3191 - val_mse: 9.4223 - val_mae: 1.6271 - lr: 3.1250e-05\n",
      "Epoch 68/500\n",
      "1295/1315 [============================>.] - ETA: 0s - loss: 0.8436 - mse: 5.1126 - mae: 1.1162\n",
      "Epoch 68: val_loss did not improve from 1.31910\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8467 - mse: 5.1438 - mae: 1.1195 - val_loss: 1.3191 - val_mse: 9.4230 - val_mae: 1.6271 - lr: 3.1250e-05\n",
      "Epoch 69/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8462 - mse: 5.1346 - mae: 1.1189\n",
      "Epoch 69: val_loss improved from 1.31910 to 1.31898, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8466 - mse: 5.1435 - mae: 1.1194 - val_loss: 1.3190 - val_mse: 9.4223 - val_mae: 1.6270 - lr: 3.1250e-05\n",
      "Epoch 70/500\n",
      "1296/1315 [============================>.] - ETA: 0s - loss: 0.8436 - mse: 5.1146 - mae: 1.1161\n",
      "Epoch 70: val_loss improved from 1.31898 to 1.31889, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8465 - mse: 5.1430 - mae: 1.1193 - val_loss: 1.3189 - val_mse: 9.4226 - val_mae: 1.6269 - lr: 3.1250e-05\n",
      "Epoch 71/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8464 - mse: 5.1422 - mae: 1.1192\n",
      "Epoch 71: val_loss did not improve from 1.31889\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8464 - mse: 5.1422 - mae: 1.1192 - val_loss: 1.3189 - val_mse: 9.4228 - val_mae: 1.6269 - lr: 3.1250e-05\n",
      "Epoch 72/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8466 - mse: 5.1442 - mae: 1.1193\n",
      "Epoch 72: val_loss improved from 1.31889 to 1.31878, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8463 - mse: 5.1416 - mae: 1.1191 - val_loss: 1.3188 - val_mse: 9.4221 - val_mae: 1.6268 - lr: 3.1250e-05\n",
      "Epoch 73/500\n",
      "1296/1315 [============================>.] - ETA: 0s - loss: 0.8434 - mse: 5.1130 - mae: 1.1159\n",
      "Epoch 73: val_loss did not improve from 1.31878\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8463 - mse: 5.1413 - mae: 1.1190 - val_loss: 1.3190 - val_mse: 9.4245 - val_mae: 1.6270 - lr: 3.1250e-05\n",
      "Epoch 74/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8456 - mse: 5.1316 - mae: 1.1183\n",
      "Epoch 74: val_loss improved from 1.31878 to 1.31876, saving model to base_stne_rnn_weight_ns.h5\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8462 - mse: 5.1405 - mae: 1.1189 - val_loss: 1.3188 - val_mse: 9.4224 - val_mae: 1.6267 - lr: 3.1250e-05\n",
      "Epoch 75/500\n",
      "1298/1315 [============================>.] - ETA: 0s - loss: 0.8447 - mse: 5.1238 - mae: 1.1173\n",
      "Epoch 75: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8461 - mse: 5.1401 - mae: 1.1189 - val_loss: 1.3189 - val_mse: 9.4239 - val_mae: 1.6269 - lr: 3.1250e-05\n",
      "Epoch 76/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8455 - mse: 5.1306 - mae: 1.1181\n",
      "Epoch 76: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8460 - mse: 5.1395 - mae: 1.1188 - val_loss: 1.3189 - val_mse: 9.4243 - val_mae: 1.6269 - lr: 3.1250e-05\n",
      "Epoch 77/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8453 - mse: 5.1325 - mae: 1.1179\n",
      "Epoch 77: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8459 - mse: 5.1390 - mae: 1.1187 - val_loss: 1.3189 - val_mse: 9.4247 - val_mae: 1.6269 - lr: 3.1250e-05\n",
      "Epoch 78/500\n",
      "1300/1315 [============================>.] - ETA: 0s - loss: 0.8455 - mse: 5.1299 - mae: 1.1181\n",
      "Epoch 78: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8459 - mse: 5.1388 - mae: 1.1187 - val_loss: 1.3190 - val_mse: 9.4249 - val_mae: 1.6270 - lr: 3.1250e-05\n",
      "Epoch 79/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8463 - mse: 5.1430 - mae: 1.1189\n",
      "Epoch 79: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8459 - mse: 5.1387 - mae: 1.1187 - val_loss: 1.3188 - val_mse: 9.4249 - val_mae: 1.6269 - lr: 3.1250e-05\n",
      "Epoch 80/500\n",
      "1293/1315 [============================>.] - ETA: 0s - loss: 0.8412 - mse: 5.0754 - mae: 1.1137\n",
      "Epoch 80: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8459 - mse: 5.1383 - mae: 1.1186 - val_loss: 1.3190 - val_mse: 9.4262 - val_mae: 1.6271 - lr: 3.1250e-05\n",
      "Epoch 81/500\n",
      "1298/1315 [============================>.] - ETA: 0s - loss: 0.8444 - mse: 5.1216 - mae: 1.1170\n",
      "Epoch 81: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8458 - mse: 5.1379 - mae: 1.1186 - val_loss: 1.3190 - val_mse: 9.4260 - val_mae: 1.6271 - lr: 3.1250e-05\n",
      "Epoch 82/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8459 - mse: 5.1387 - mae: 1.1186\n",
      "Epoch 82: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8457 - mse: 5.1373 - mae: 1.1185 - val_loss: 1.3192 - val_mse: 9.4276 - val_mae: 1.6273 - lr: 3.1250e-05\n",
      "Epoch 83/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8629 - mse: 5.2188 - mae: 1.1381\n",
      "Epoch 83: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8627 - mse: 5.2173 - mae: 1.1379 - val_loss: 1.3273 - val_mse: 9.5372 - val_mae: 1.6362 - lr: 1.5625e-05\n",
      "Epoch 84/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8601 - mse: 5.2049 - mae: 1.1351\n",
      "Epoch 84: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8596 - mse: 5.2013 - mae: 1.1347 - val_loss: 1.3276 - val_mse: 9.5454 - val_mae: 1.6363 - lr: 1.5625e-05\n",
      "Epoch 85/500\n",
      "1303/1315 [============================>.] - ETA: 0s - loss: 0.8581 - mse: 5.1855 - mae: 1.1330\n",
      "Epoch 85: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8590 - mse: 5.1987 - mae: 1.1341 - val_loss: 1.3277 - val_mse: 9.5461 - val_mae: 1.6363 - lr: 1.5625e-05\n",
      "Epoch 86/500\n",
      "1302/1315 [============================>.] - ETA: 0s - loss: 0.8578 - mse: 5.1856 - mae: 1.1326\n",
      "Epoch 86: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8587 - mse: 5.1972 - mae: 1.1336 - val_loss: 1.3277 - val_mse: 9.5467 - val_mae: 1.6364 - lr: 1.5625e-05\n",
      "Epoch 87/500\n",
      "1302/1315 [============================>.] - ETA: 0s - loss: 0.8574 - mse: 5.1842 - mae: 1.1322\n",
      "Epoch 87: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8583 - mse: 5.1958 - mae: 1.1333 - val_loss: 1.3278 - val_mse: 9.5471 - val_mae: 1.6364 - lr: 1.5625e-05\n",
      "Epoch 88/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8585 - mse: 5.1994 - mae: 1.1334\n",
      "Epoch 88: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8580 - mse: 5.1948 - mae: 1.1330 - val_loss: 1.3281 - val_mse: 9.5498 - val_mae: 1.6368 - lr: 1.5625e-05\n",
      "Epoch 89/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8574 - mse: 5.1857 - mae: 1.1321\n",
      "Epoch 89: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8578 - mse: 5.1940 - mae: 1.1327 - val_loss: 1.3286 - val_mse: 9.5533 - val_mae: 1.6373 - lr: 1.5625e-05\n",
      "Epoch 90/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8576 - mse: 5.1931 - mae: 1.1324\n",
      "Epoch 90: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8576 - mse: 5.1931 - mae: 1.1324 - val_loss: 1.3288 - val_mse: 9.5549 - val_mae: 1.6375 - lr: 1.5625e-05\n",
      "Epoch 91/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8573 - mse: 5.1924 - mae: 1.1320\n",
      "Epoch 91: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8574 - mse: 5.1923 - mae: 1.1322 - val_loss: 1.3291 - val_mse: 9.5568 - val_mae: 1.6378 - lr: 1.5625e-05\n",
      "Epoch 92/500\n",
      "1298/1315 [============================>.] - ETA: 0s - loss: 0.8558 - mse: 5.1764 - mae: 1.1304\n",
      "Epoch 92: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8572 - mse: 5.1914 - mae: 1.1320 - val_loss: 1.3291 - val_mse: 9.5574 - val_mae: 1.6378 - lr: 1.5625e-05\n",
      "Epoch 93/500\n",
      "1301/1315 [============================>.] - ETA: 0s - loss: 0.8654 - mse: 5.2210 - mae: 1.1418\n",
      "Epoch 93: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8659 - mse: 5.2291 - mae: 1.1424 - val_loss: 1.3449 - val_mse: 9.6873 - val_mae: 1.6528 - lr: 7.8125e-06\n",
      "Epoch 94/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8640 - mse: 5.2169 - mae: 1.1391\n",
      "Epoch 94: val_loss did not improve from 1.31876\n",
      "1315/1315 [==============================] - 4s 3ms/step - loss: 0.8636 - mse: 5.2126 - mae: 1.1387 - val_loss: 1.3469 - val_mse: 9.7099 - val_mae: 1.6547 - lr: 7.8125e-06\n"
     ]
    }
   ],
   "source": [
    "rnn_history = rnn_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_rnn])\n",
    "rnn_history_ns = rnn_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_rnn_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "   1309/Unknown - 12s 6ms/step - loss: 1.5703 - mse: 15.7796 - mae: 1.8903\n",
      "Epoch 1: val_loss improved from inf to 1.58644, saving model to base_stne_gru_weight.h5\n",
      "1315/1315 [==============================] - 14s 7ms/step - loss: 1.5699 - mse: 15.7608 - mae: 1.8899 - val_loss: 1.5864 - val_mse: 11.8529 - val_mae: 1.9552 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 1.0130 - mse: 6.2071 - mae: 1.3131\n",
      "Epoch 2: val_loss improved from 1.58644 to 1.47950, saving model to base_stne_gru_weight.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 1.0153 - mse: 6.2435 - mae: 1.3156 - val_loss: 1.4795 - val_mse: 10.7758 - val_mae: 1.8046 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.9814 - mse: 6.1084 - mae: 1.2734\n",
      "Epoch 3: val_loss did not improve from 1.47950\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9826 - mse: 6.1205 - mae: 1.2747 - val_loss: 1.6218 - val_mse: 12.1435 - val_mae: 1.9686 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9522 - mse: 5.8268 - mae: 1.2409\n",
      "Epoch 4: val_loss improved from 1.47950 to 1.42036, saving model to base_stne_gru_weight.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9557 - mse: 5.8753 - mae: 1.2446 - val_loss: 1.4204 - val_mse: 10.4153 - val_mae: 1.7371 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.9546 - mse: 5.8842 - mae: 1.2421\n",
      "Epoch 5: val_loss did not improve from 1.42036\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9561 - mse: 5.9001 - mae: 1.2436 - val_loss: 1.6098 - val_mse: 11.5916 - val_mae: 1.9314 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.9637 - mse: 5.9764 - mae: 1.2509\n",
      "Epoch 6: val_loss did not improve from 1.42036\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9653 - mse: 5.9959 - mae: 1.2526 - val_loss: 1.5608 - val_mse: 11.5689 - val_mae: 1.8894 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.9442 - mse: 5.8236 - mae: 1.2294\n",
      "Epoch 7: val_loss did not improve from 1.42036\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9451 - mse: 5.8275 - mae: 1.2304 - val_loss: 1.5006 - val_mse: 10.6507 - val_mae: 1.8255 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.9389 - mse: 5.7812 - mae: 1.2234\n",
      "Epoch 8: val_loss did not improve from 1.42036\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9394 - mse: 5.7899 - mae: 1.2240 - val_loss: 1.5817 - val_mse: 11.4804 - val_mae: 1.9145 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.9348 - mse: 5.7821 - mae: 1.2177\n",
      "Epoch 9: val_loss did not improve from 1.42036\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9372 - mse: 5.8026 - mae: 1.2203 - val_loss: 1.6221 - val_mse: 12.3931 - val_mae: 1.9495 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.9238 - mse: 5.7254 - mae: 1.2047\n",
      "Epoch 10: val_loss improved from 1.42036 to 1.40109, saving model to base_stne_gru_weight.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9261 - mse: 5.7468 - mae: 1.2073 - val_loss: 1.4011 - val_mse: 10.2441 - val_mae: 1.7112 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.9213 - mse: 5.6777 - mae: 1.2030\n",
      "Epoch 11: val_loss did not improve from 1.40109\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9248 - mse: 5.7137 - mae: 1.2067 - val_loss: 1.4043 - val_mse: 10.2460 - val_mae: 1.7248 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.9317 - mse: 5.7409 - mae: 1.2142\n",
      "Epoch 12: val_loss did not improve from 1.40109\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9336 - mse: 5.7621 - mae: 1.2161 - val_loss: 1.4735 - val_mse: 10.4933 - val_mae: 1.8059 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.9250 - mse: 5.7105 - mae: 1.2057\n",
      "Epoch 13: val_loss did not improve from 1.40109\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9250 - mse: 5.7087 - mae: 1.2057 - val_loss: 1.5855 - val_mse: 12.0549 - val_mae: 1.9080 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.9166 - mse: 5.6233 - mae: 1.1972\n",
      "Epoch 14: val_loss did not improve from 1.40109\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9203 - mse: 5.6739 - mae: 1.2012 - val_loss: 1.5729 - val_mse: 11.8502 - val_mae: 1.8929 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9236 - mse: 5.7141 - mae: 1.2023\n",
      "Epoch 15: val_loss did not improve from 1.40109\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9236 - mse: 5.7142 - mae: 1.2023 - val_loss: 1.4689 - val_mse: 10.7818 - val_mae: 1.7920 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.9233 - mse: 5.6767 - mae: 1.2003\n",
      "Epoch 16: val_loss did not improve from 1.40109\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9250 - mse: 5.7010 - mae: 1.2022 - val_loss: 1.5259 - val_mse: 11.3504 - val_mae: 1.8427 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.9125 - mse: 5.6254 - mae: 1.1878\n",
      "Epoch 17: val_loss improved from 1.40109 to 1.36013, saving model to base_stne_gru_weight.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9153 - mse: 5.6506 - mae: 1.1908 - val_loss: 1.3601 - val_mse: 9.9309 - val_mae: 1.6764 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.9172 - mse: 5.6550 - mae: 1.1941\n",
      "Epoch 18: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9176 - mse: 5.6607 - mae: 1.1945 - val_loss: 1.5194 - val_mse: 10.8495 - val_mae: 1.8403 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.9217 - mse: 5.6991 - mae: 1.1986\n",
      "Epoch 19: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9222 - mse: 5.7035 - mae: 1.1991 - val_loss: 1.4890 - val_mse: 11.0888 - val_mae: 1.8021 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.9239 - mse: 5.6693 - mae: 1.2026\n",
      "Epoch 20: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9247 - mse: 5.6778 - mae: 1.2033 - val_loss: 1.4120 - val_mse: 10.0759 - val_mae: 1.7279 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.9151 - mse: 5.6414 - mae: 1.1925\n",
      "Epoch 21: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9161 - mse: 5.6525 - mae: 1.1936 - val_loss: 1.6311 - val_mse: 11.9676 - val_mae: 1.9491 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.9106 - mse: 5.6253 - mae: 1.1872\n",
      "Epoch 22: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9113 - mse: 5.6322 - mae: 1.1879 - val_loss: 1.4342 - val_mse: 10.2371 - val_mae: 1.7529 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9222 - mse: 5.6886 - mae: 1.1983\n",
      "Epoch 23: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9222 - mse: 5.6886 - mae: 1.1983 - val_loss: 1.5192 - val_mse: 10.9934 - val_mae: 1.8325 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.9112 - mse: 5.6056 - mae: 1.1862\n",
      "Epoch 24: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9118 - mse: 5.6167 - mae: 1.1867 - val_loss: 1.4680 - val_mse: 10.6103 - val_mae: 1.7870 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.9031 - mse: 5.5654 - mae: 1.1779\n",
      "Epoch 25: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9056 - mse: 5.5918 - mae: 1.1805 - val_loss: 1.4415 - val_mse: 10.6972 - val_mae: 1.7524 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.9089 - mse: 5.5989 - mae: 1.1842\n",
      "Epoch 26: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9089 - mse: 5.5989 - mae: 1.1842 - val_loss: 1.4775 - val_mse: 10.8531 - val_mae: 1.7960 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.9095 - mse: 5.6286 - mae: 1.1858\n",
      "Epoch 27: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.9104 - mse: 5.6382 - mae: 1.1867 - val_loss: 1.3829 - val_mse: 10.0915 - val_mae: 1.6936 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8908 - mse: 5.4731 - mae: 1.1664\n",
      "Epoch 28: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8907 - mse: 5.4718 - mae: 1.1661 - val_loss: 1.4468 - val_mse: 10.2670 - val_mae: 1.7563 - lr: 5.0000e-04\n",
      "Epoch 29/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8831 - mse: 5.4335 - mae: 1.1570\n",
      "Epoch 29: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8840 - mse: 5.4437 - mae: 1.1579 - val_loss: 1.4108 - val_mse: 9.9794 - val_mae: 1.7273 - lr: 5.0000e-04\n",
      "Epoch 30/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8814 - mse: 5.4191 - mae: 1.1564\n",
      "Epoch 30: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8827 - mse: 5.4295 - mae: 1.1579 - val_loss: 1.4113 - val_mse: 10.0559 - val_mae: 1.7302 - lr: 5.0000e-04\n",
      "Epoch 31/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8855 - mse: 5.4231 - mae: 1.1608\n",
      "Epoch 31: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8858 - mse: 5.4264 - mae: 1.1612 - val_loss: 1.4202 - val_mse: 10.1849 - val_mae: 1.7347 - lr: 5.0000e-04\n",
      "Epoch 32/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8794 - mse: 5.3986 - mae: 1.1537\n",
      "Epoch 32: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8814 - mse: 5.4222 - mae: 1.1557 - val_loss: 1.4694 - val_mse: 10.4917 - val_mae: 1.7896 - lr: 5.0000e-04\n",
      "Epoch 33/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8768 - mse: 5.3704 - mae: 1.1499\n",
      "Epoch 33: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8802 - mse: 5.4223 - mae: 1.1533 - val_loss: 1.3874 - val_mse: 9.9350 - val_mae: 1.7034 - lr: 5.0000e-04\n",
      "Epoch 34/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8825 - mse: 5.4233 - mae: 1.1568\n",
      "Epoch 34: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8839 - mse: 5.4347 - mae: 1.1583 - val_loss: 1.4226 - val_mse: 10.1056 - val_mae: 1.7373 - lr: 5.0000e-04\n",
      "Epoch 35/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8771 - mse: 5.3859 - mae: 1.1504\n",
      "Epoch 35: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8785 - mse: 5.3981 - mae: 1.1518 - val_loss: 1.4874 - val_mse: 10.8025 - val_mae: 1.8058 - lr: 5.0000e-04\n",
      "Epoch 36/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8762 - mse: 5.3741 - mae: 1.1502\n",
      "Epoch 36: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8761 - mse: 5.3731 - mae: 1.1501 - val_loss: 1.4156 - val_mse: 10.1401 - val_mae: 1.7256 - lr: 5.0000e-04\n",
      "Epoch 37/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8786 - mse: 5.4031 - mae: 1.1516\n",
      "Epoch 37: val_loss did not improve from 1.36013\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8789 - mse: 5.4058 - mae: 1.1518 - val_loss: 1.4385 - val_mse: 10.1944 - val_mae: 1.7523 - lr: 5.0000e-04\n",
      "Epoch 1/500\n",
      "   1314/Unknown - 8s 6ms/step - loss: 0.8677 - mse: 5.3301 - mae: 1.1414\n",
      "Epoch 1: val_loss improved from inf to 1.43392, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8680 - mse: 5.3318 - mae: 1.1417 - val_loss: 1.4339 - val_mse: 10.1680 - val_mae: 1.7489 - lr: 2.5000e-04\n",
      "Epoch 2/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8634 - mse: 5.3098 - mae: 1.1364\n",
      "Epoch 2: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8635 - mse: 5.3088 - mae: 1.1364 - val_loss: 1.4516 - val_mse: 10.2975 - val_mae: 1.7683 - lr: 2.5000e-04\n",
      "Epoch 3/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8618 - mse: 5.3017 - mae: 1.1345\n",
      "Epoch 3: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8627 - mse: 5.3052 - mae: 1.1356 - val_loss: 1.4455 - val_mse: 10.2754 - val_mae: 1.7616 - lr: 2.5000e-04\n",
      "Epoch 4/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8615 - mse: 5.3015 - mae: 1.1344\n",
      "Epoch 4: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8616 - mse: 5.2991 - mae: 1.1345 - val_loss: 1.4575 - val_mse: 10.3595 - val_mae: 1.7742 - lr: 2.5000e-04\n",
      "Epoch 5/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8606 - mse: 5.2966 - mae: 1.1332\n",
      "Epoch 5: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8609 - mse: 5.2958 - mae: 1.1336 - val_loss: 1.4542 - val_mse: 10.3361 - val_mae: 1.7712 - lr: 2.5000e-04\n",
      "Epoch 6/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8593 - mse: 5.2901 - mae: 1.1317\n",
      "Epoch 6: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8602 - mse: 5.2937 - mae: 1.1327 - val_loss: 1.4605 - val_mse: 10.3883 - val_mae: 1.7779 - lr: 2.5000e-04\n",
      "Epoch 7/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8595 - mse: 5.2944 - mae: 1.1318\n",
      "Epoch 7: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8595 - mse: 5.2903 - mae: 1.1319 - val_loss: 1.4645 - val_mse: 10.4232 - val_mae: 1.7826 - lr: 2.5000e-04\n",
      "Epoch 8/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8567 - mse: 5.2671 - mae: 1.1298\n",
      "Epoch 8: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8588 - mse: 5.2862 - mae: 1.1321 - val_loss: 1.4692 - val_mse: 10.4610 - val_mae: 1.7880 - lr: 2.5000e-04\n",
      "Epoch 9/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8567 - mse: 5.2676 - mae: 1.1288\n",
      "Epoch 9: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8583 - mse: 5.2830 - mae: 1.1306 - val_loss: 1.4593 - val_mse: 10.4007 - val_mae: 1.7769 - lr: 2.5000e-04\n",
      "Epoch 10/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8583 - mse: 5.2795 - mae: 1.1308\n",
      "Epoch 10: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8583 - mse: 5.2795 - mae: 1.1308 - val_loss: 1.4610 - val_mse: 10.4279 - val_mae: 1.7781 - lr: 2.5000e-04\n",
      "Epoch 11/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8574 - mse: 5.2803 - mae: 1.1296\n",
      "Epoch 11: val_loss did not improve from 1.43392\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8575 - mse: 5.2782 - mae: 1.1299 - val_loss: 1.4653 - val_mse: 10.4722 - val_mae: 1.7825 - lr: 2.5000e-04\n",
      "Epoch 12/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8660 - mse: 5.3093 - mae: 1.1412\n",
      "Epoch 12: val_loss improved from 1.43392 to 1.42920, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8660 - mse: 5.3068 - mae: 1.1413 - val_loss: 1.4292 - val_mse: 10.1837 - val_mae: 1.7434 - lr: 1.2500e-04\n",
      "Epoch 13/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8636 - mse: 5.2994 - mae: 1.1379\n",
      "Epoch 13: val_loss improved from 1.42920 to 1.42664, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8636 - mse: 5.2994 - mae: 1.1379 - val_loss: 1.4266 - val_mse: 10.1711 - val_mae: 1.7406 - lr: 1.2500e-04\n",
      "Epoch 14/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8610 - mse: 5.2782 - mae: 1.1347\n",
      "Epoch 14: val_loss improved from 1.42664 to 1.42609, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8626 - mse: 5.2949 - mae: 1.1365 - val_loss: 1.4261 - val_mse: 10.1691 - val_mae: 1.7397 - lr: 1.2500e-04\n",
      "Epoch 15/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8607 - mse: 5.2862 - mae: 1.1342\n",
      "Epoch 15: val_loss did not improve from 1.42609\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8615 - mse: 5.2898 - mae: 1.1351 - val_loss: 1.4267 - val_mse: 10.1734 - val_mae: 1.7403 - lr: 1.2500e-04\n",
      "Epoch 16/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8586 - mse: 5.2634 - mae: 1.1319\n",
      "Epoch 16: val_loss improved from 1.42609 to 1.42479, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8607 - mse: 5.2850 - mae: 1.1341 - val_loss: 1.4248 - val_mse: 10.1625 - val_mae: 1.7383 - lr: 1.2500e-04\n",
      "Epoch 17/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8599 - mse: 5.2860 - mae: 1.1330\n",
      "Epoch 17: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8598 - mse: 5.2818 - mae: 1.1330 - val_loss: 1.4259 - val_mse: 10.1665 - val_mae: 1.7394 - lr: 1.2500e-04\n",
      "Epoch 18/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8592 - mse: 5.2774 - mae: 1.1323\n",
      "Epoch 18: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8592 - mse: 5.2774 - mae: 1.1323 - val_loss: 1.4257 - val_mse: 10.1667 - val_mae: 1.7392 - lr: 1.2500e-04\n",
      "Epoch 19/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8583 - mse: 5.2768 - mae: 1.1311\n",
      "Epoch 19: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8584 - mse: 5.2748 - mae: 1.1313 - val_loss: 1.4297 - val_mse: 10.1920 - val_mae: 1.7434 - lr: 1.2500e-04\n",
      "Epoch 20/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8581 - mse: 5.2723 - mae: 1.1307\n",
      "Epoch 20: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8581 - mse: 5.2723 - mae: 1.1307 - val_loss: 1.4286 - val_mse: 10.1812 - val_mae: 1.7422 - lr: 1.2500e-04\n",
      "Epoch 21/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8575 - mse: 5.2722 - mae: 1.1299\n",
      "Epoch 21: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8577 - mse: 5.2703 - mae: 1.1301 - val_loss: 1.4322 - val_mse: 10.2072 - val_mae: 1.7460 - lr: 1.2500e-04\n",
      "Epoch 22/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8573 - mse: 5.2713 - mae: 1.1296\n",
      "Epoch 22: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8572 - mse: 5.2674 - mae: 1.1296 - val_loss: 1.4321 - val_mse: 10.2042 - val_mae: 1.7459 - lr: 1.2500e-04\n",
      "Epoch 23/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8551 - mse: 5.2488 - mae: 1.1273\n",
      "Epoch 23: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8569 - mse: 5.2667 - mae: 1.1291 - val_loss: 1.4354 - val_mse: 10.2284 - val_mae: 1.7495 - lr: 1.2500e-04\n",
      "Epoch 24/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8566 - mse: 5.2674 - mae: 1.1288\n",
      "Epoch 24: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8566 - mse: 5.2652 - mae: 1.1288 - val_loss: 1.4371 - val_mse: 10.2392 - val_mae: 1.7512 - lr: 1.2500e-04\n",
      "Epoch 25/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8561 - mse: 5.2649 - mae: 1.1281\n",
      "Epoch 25: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8562 - mse: 5.2632 - mae: 1.1283 - val_loss: 1.4356 - val_mse: 10.2260 - val_mae: 1.7496 - lr: 1.2500e-04\n",
      "Epoch 26/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8542 - mse: 5.2444 - mae: 1.1262\n",
      "Epoch 26: val_loss did not improve from 1.42479\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8559 - mse: 5.2618 - mae: 1.1280 - val_loss: 1.4362 - val_mse: 10.2298 - val_mae: 1.7502 - lr: 1.2500e-04\n",
      "Epoch 27/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8691 - mse: 5.2986 - mae: 1.1435\n",
      "Epoch 27: val_loss improved from 1.42479 to 1.40587, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8690 - mse: 5.2973 - mae: 1.1434 - val_loss: 1.4059 - val_mse: 10.0030 - val_mae: 1.7184 - lr: 6.2500e-05\n",
      "Epoch 28/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8638 - mse: 5.2646 - mae: 1.1376\n",
      "Epoch 28: val_loss improved from 1.40587 to 1.40023, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8653 - mse: 5.2816 - mae: 1.1393 - val_loss: 1.4002 - val_mse: 9.9603 - val_mae: 1.7127 - lr: 6.2500e-05\n",
      "Epoch 29/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8637 - mse: 5.2763 - mae: 1.1373\n",
      "Epoch 29: val_loss improved from 1.40023 to 1.39732, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8639 - mse: 5.2759 - mae: 1.1376 - val_loss: 1.3973 - val_mse: 9.9416 - val_mae: 1.7094 - lr: 6.2500e-05\n",
      "Epoch 30/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8626 - mse: 5.2714 - mae: 1.1359\n",
      "Epoch 30: val_loss improved from 1.39732 to 1.39680, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8629 - mse: 5.2711 - mae: 1.1363 - val_loss: 1.3968 - val_mse: 9.9352 - val_mae: 1.7089 - lr: 6.2500e-05\n",
      "Epoch 31/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8613 - mse: 5.2633 - mae: 1.1345\n",
      "Epoch 31: val_loss improved from 1.39680 to 1.39550, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8621 - mse: 5.2676 - mae: 1.1354 - val_loss: 1.3955 - val_mse: 9.9272 - val_mae: 1.7074 - lr: 6.2500e-05\n",
      "Epoch 32/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8600 - mse: 5.2481 - mae: 1.1330\n",
      "Epoch 32: val_loss did not improve from 1.39550\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8615 - mse: 5.2649 - mae: 1.1347 - val_loss: 1.3956 - val_mse: 9.9246 - val_mae: 1.7075 - lr: 6.2500e-05\n",
      "Epoch 33/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8588 - mse: 5.2404 - mae: 1.1317\n",
      "Epoch 33: val_loss improved from 1.39550 to 1.39482, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8609 - mse: 5.2619 - mae: 1.1339 - val_loss: 1.3948 - val_mse: 9.9212 - val_mae: 1.7067 - lr: 6.2500e-05\n",
      "Epoch 34/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8604 - mse: 5.2600 - mae: 1.1332\n",
      "Epoch 34: val_loss did not improve from 1.39482\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8603 - mse: 5.2588 - mae: 1.1331 - val_loss: 1.3951 - val_mse: 9.9200 - val_mae: 1.7070 - lr: 6.2500e-05\n",
      "Epoch 35/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8599 - mse: 5.2579 - mae: 1.1327\n",
      "Epoch 35: val_loss improved from 1.39482 to 1.39427, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8598 - mse: 5.2566 - mae: 1.1326 - val_loss: 1.3943 - val_mse: 9.9153 - val_mae: 1.7061 - lr: 6.2500e-05\n",
      "Epoch 36/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8593 - mse: 5.2555 - mae: 1.1319\n",
      "Epoch 36: val_loss did not improve from 1.39427\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8593 - mse: 5.2534 - mae: 1.1320 - val_loss: 1.3948 - val_mse: 9.9163 - val_mae: 1.7066 - lr: 6.2500e-05\n",
      "Epoch 37/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8587 - mse: 5.2518 - mae: 1.1312\n",
      "Epoch 37: val_loss did not improve from 1.39427\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8589 - mse: 5.2514 - mae: 1.1315 - val_loss: 1.3943 - val_mse: 9.9133 - val_mae: 1.7062 - lr: 6.2500e-05\n",
      "Epoch 38/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8581 - mse: 5.2466 - mae: 1.1306\n",
      "Epoch 38: val_loss did not improve from 1.39427\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8584 - mse: 5.2483 - mae: 1.1309 - val_loss: 1.3950 - val_mse: 9.9153 - val_mae: 1.7069 - lr: 6.2500e-05\n",
      "Epoch 39/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8582 - mse: 5.2495 - mae: 1.1306\n",
      "Epoch 39: val_loss improved from 1.39427 to 1.39426, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8581 - mse: 5.2469 - mae: 1.1305 - val_loss: 1.3943 - val_mse: 9.9102 - val_mae: 1.7062 - lr: 6.2500e-05\n",
      "Epoch 40/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8573 - mse: 5.2421 - mae: 1.1296\n",
      "Epoch 40: val_loss did not improve from 1.39426\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8576 - mse: 5.2439 - mae: 1.1299 - val_loss: 1.3946 - val_mse: 9.9113 - val_mae: 1.7065 - lr: 6.2500e-05\n",
      "Epoch 41/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8573 - mse: 5.2463 - mae: 1.1295\n",
      "Epoch 41: val_loss did not improve from 1.39426\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8572 - mse: 5.2422 - mae: 1.1294 - val_loss: 1.3943 - val_mse: 9.9088 - val_mae: 1.7062 - lr: 6.2500e-05\n",
      "Epoch 42/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8567 - mse: 5.2395 - mae: 1.1288\n",
      "Epoch 42: val_loss did not improve from 1.39426\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8567 - mse: 5.2395 - mae: 1.1288 - val_loss: 1.3949 - val_mse: 9.9114 - val_mae: 1.7067 - lr: 6.2500e-05\n",
      "Epoch 43/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8544 - mse: 5.2175 - mae: 1.1263\n",
      "Epoch 43: val_loss did not improve from 1.39426\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8564 - mse: 5.2381 - mae: 1.1285 - val_loss: 1.3943 - val_mse: 9.9077 - val_mae: 1.7061 - lr: 6.2500e-05\n",
      "Epoch 44/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8545 - mse: 5.2197 - mae: 1.1263\n",
      "Epoch 44: val_loss did not improve from 1.39426\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8560 - mse: 5.2357 - mae: 1.1279 - val_loss: 1.3947 - val_mse: 9.9084 - val_mae: 1.7064 - lr: 6.2500e-05\n",
      "Epoch 45/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8537 - mse: 5.2135 - mae: 1.1254\n",
      "Epoch 45: val_loss did not improve from 1.39426\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8557 - mse: 5.2340 - mae: 1.1275 - val_loss: 1.3944 - val_mse: 9.9081 - val_mae: 1.7059 - lr: 6.2500e-05\n",
      "Epoch 46/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8679 - mse: 5.2615 - mae: 1.1425\n",
      "Epoch 46: val_loss improved from 1.39426 to 1.37486, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8697 - mse: 5.2811 - mae: 1.1444 - val_loss: 1.3749 - val_mse: 9.7706 - val_mae: 1.6852 - lr: 3.1250e-05\n",
      "Epoch 47/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8668 - mse: 5.2663 - mae: 1.1405\n",
      "Epoch 47: val_loss improved from 1.37486 to 1.37171, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8667 - mse: 5.2651 - mae: 1.1404 - val_loss: 1.3717 - val_mse: 9.7559 - val_mae: 1.6819 - lr: 3.1250e-05\n",
      "Epoch 48/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8659 - mse: 5.2626 - mae: 1.1392\n",
      "Epoch 48: val_loss improved from 1.37171 to 1.36983, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8657 - mse: 5.2614 - mae: 1.1391 - val_loss: 1.3698 - val_mse: 9.7464 - val_mae: 1.6800 - lr: 3.1250e-05\n",
      "Epoch 49/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8644 - mse: 5.2553 - mae: 1.1375\n",
      "Epoch 49: val_loss improved from 1.36983 to 1.36865, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8650 - mse: 5.2587 - mae: 1.1382 - val_loss: 1.3686 - val_mse: 9.7403 - val_mae: 1.6789 - lr: 3.1250e-05\n",
      "Epoch 50/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8646 - mse: 5.2588 - mae: 1.1376\n",
      "Epoch 50: val_loss improved from 1.36865 to 1.36769, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8644 - mse: 5.2564 - mae: 1.1375 - val_loss: 1.3677 - val_mse: 9.7351 - val_mae: 1.6779 - lr: 3.1250e-05\n",
      "Epoch 51/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8643 - mse: 5.2577 - mae: 1.1373\n",
      "Epoch 51: val_loss improved from 1.36769 to 1.36699, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8640 - mse: 5.2546 - mae: 1.1370 - val_loss: 1.3670 - val_mse: 9.7317 - val_mae: 1.6772 - lr: 3.1250e-05\n",
      "Epoch 52/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8623 - mse: 5.2376 - mae: 1.1352\n",
      "Epoch 52: val_loss improved from 1.36699 to 1.36636, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8636 - mse: 5.2527 - mae: 1.1366 - val_loss: 1.3664 - val_mse: 9.7286 - val_mae: 1.6766 - lr: 3.1250e-05\n",
      "Epoch 53/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8632 - mse: 5.2526 - mae: 1.1361\n",
      "Epoch 53: val_loss improved from 1.36636 to 1.36587, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8632 - mse: 5.2512 - mae: 1.1361 - val_loss: 1.3659 - val_mse: 9.7263 - val_mae: 1.6761 - lr: 3.1250e-05\n",
      "Epoch 54/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8623 - mse: 5.2463 - mae: 1.1350\n",
      "Epoch 54: val_loss improved from 1.36587 to 1.36556, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8629 - mse: 5.2496 - mae: 1.1357 - val_loss: 1.3656 - val_mse: 9.7252 - val_mae: 1.6758 - lr: 3.1250e-05\n",
      "Epoch 55/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8629 - mse: 5.2516 - mae: 1.1356\n",
      "Epoch 55: val_loss improved from 1.36556 to 1.36509, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8626 - mse: 5.2485 - mae: 1.1354 - val_loss: 1.3651 - val_mse: 9.7228 - val_mae: 1.6753 - lr: 3.1250e-05\n",
      "Epoch 56/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8625 - mse: 5.2483 - mae: 1.1352\n",
      "Epoch 56: val_loss improved from 1.36509 to 1.36484, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8623 - mse: 5.2471 - mae: 1.1351 - val_loss: 1.3648 - val_mse: 9.7219 - val_mae: 1.6750 - lr: 3.1250e-05\n",
      "Epoch 57/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8623 - mse: 5.2491 - mae: 1.1350\n",
      "Epoch 57: val_loss improved from 1.36484 to 1.36453, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8620 - mse: 5.2459 - mae: 1.1347 - val_loss: 1.3645 - val_mse: 9.7203 - val_mae: 1.6747 - lr: 3.1250e-05\n",
      "Epoch 58/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8621 - mse: 5.2492 - mae: 1.1347\n",
      "Epoch 58: val_loss improved from 1.36453 to 1.36435, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8618 - mse: 5.2448 - mae: 1.1344 - val_loss: 1.3644 - val_mse: 9.7199 - val_mae: 1.6745 - lr: 3.1250e-05\n",
      "Epoch 59/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8614 - mse: 5.2418 - mae: 1.1339\n",
      "Epoch 59: val_loss improved from 1.36435 to 1.36404, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8616 - mse: 5.2436 - mae: 1.1341 - val_loss: 1.3640 - val_mse: 9.7180 - val_mae: 1.6742 - lr: 3.1250e-05\n",
      "Epoch 60/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8616 - mse: 5.2454 - mae: 1.1341\n",
      "Epoch 60: val_loss improved from 1.36404 to 1.36389, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8613 - mse: 5.2423 - mae: 1.1338 - val_loss: 1.3639 - val_mse: 9.7180 - val_mae: 1.6740 - lr: 3.1250e-05\n",
      "Epoch 61/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8613 - mse: 5.2437 - mae: 1.1337\n",
      "Epoch 61: val_loss improved from 1.36389 to 1.36366, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8611 - mse: 5.2412 - mae: 1.1335 - val_loss: 1.3637 - val_mse: 9.7169 - val_mae: 1.6738 - lr: 3.1250e-05\n",
      "Epoch 62/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8603 - mse: 5.2370 - mae: 1.1326\n",
      "Epoch 62: val_loss improved from 1.36366 to 1.36350, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8609 - mse: 5.2402 - mae: 1.1333 - val_loss: 1.3635 - val_mse: 9.7166 - val_mae: 1.6736 - lr: 3.1250e-05\n",
      "Epoch 63/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8608 - mse: 5.2415 - mae: 1.1331\n",
      "Epoch 63: val_loss improved from 1.36350 to 1.36332, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8606 - mse: 5.2390 - mae: 1.1330 - val_loss: 1.3633 - val_mse: 9.7161 - val_mae: 1.6734 - lr: 3.1250e-05\n",
      "Epoch 64/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8607 - mse: 5.2421 - mae: 1.1330\n",
      "Epoch 64: val_loss did not improve from 1.36332\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8604 - mse: 5.2377 - mae: 1.1327 - val_loss: 1.3634 - val_mse: 9.7166 - val_mae: 1.6735 - lr: 3.1250e-05\n",
      "Epoch 65/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8604 - mse: 5.2391 - mae: 1.1326\n",
      "Epoch 65: val_loss improved from 1.36332 to 1.36332, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8602 - mse: 5.2366 - mae: 1.1325 - val_loss: 1.3633 - val_mse: 9.7165 - val_mae: 1.6734 - lr: 3.1250e-05\n",
      "Epoch 66/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8602 - mse: 5.2385 - mae: 1.1325\n",
      "Epoch 66: val_loss improved from 1.36332 to 1.36318, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8599 - mse: 5.2354 - mae: 1.1322 - val_loss: 1.3632 - val_mse: 9.7162 - val_mae: 1.6733 - lr: 3.1250e-05\n",
      "Epoch 67/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8601 - mse: 5.2387 - mae: 1.1322\n",
      "Epoch 67: val_loss improved from 1.36318 to 1.36306, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8597 - mse: 5.2343 - mae: 1.1320 - val_loss: 1.3631 - val_mse: 9.7160 - val_mae: 1.6732 - lr: 3.1250e-05\n",
      "Epoch 68/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8597 - mse: 5.2357 - mae: 1.1318\n",
      "Epoch 68: val_loss improved from 1.36306 to 1.36302, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8595 - mse: 5.2332 - mae: 1.1317 - val_loss: 1.3630 - val_mse: 9.7161 - val_mae: 1.6732 - lr: 3.1250e-05\n",
      "Epoch 69/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8596 - mse: 5.2366 - mae: 1.1317\n",
      "Epoch 69: val_loss improved from 1.36302 to 1.36282, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8593 - mse: 5.2322 - mae: 1.1314 - val_loss: 1.3628 - val_mse: 9.7156 - val_mae: 1.6730 - lr: 3.1250e-05\n",
      "Epoch 70/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8591 - mse: 5.2326 - mae: 1.1311\n",
      "Epoch 70: val_loss improved from 1.36282 to 1.36275, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8591 - mse: 5.2311 - mae: 1.1312 - val_loss: 1.3628 - val_mse: 9.7156 - val_mae: 1.6729 - lr: 3.1250e-05\n",
      "Epoch 71/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8588 - mse: 5.2299 - mae: 1.1309\n",
      "Epoch 71: val_loss improved from 1.36275 to 1.36271, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8588 - mse: 5.2299 - mae: 1.1309 - val_loss: 1.3627 - val_mse: 9.7158 - val_mae: 1.6729 - lr: 3.1250e-05\n",
      "Epoch 72/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8590 - mse: 5.2334 - mae: 1.1310\n",
      "Epoch 72: val_loss improved from 1.36271 to 1.36266, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8587 - mse: 5.2290 - mae: 1.1307 - val_loss: 1.3627 - val_mse: 9.7162 - val_mae: 1.6728 - lr: 3.1250e-05\n",
      "Epoch 73/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8585 - mse: 5.2295 - mae: 1.1304\n",
      "Epoch 73: val_loss improved from 1.36266 to 1.36250, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 9s 6ms/step - loss: 0.8585 - mse: 5.2279 - mae: 1.1305 - val_loss: 1.3625 - val_mse: 9.7161 - val_mae: 1.6727 - lr: 3.1250e-05\n",
      "Epoch 74/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8581 - mse: 5.2252 - mae: 1.1300\n",
      "Epoch 74: val_loss improved from 1.36250 to 1.36247, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8583 - mse: 5.2270 - mae: 1.1302 - val_loss: 1.3625 - val_mse: 9.7163 - val_mae: 1.6727 - lr: 3.1250e-05\n",
      "Epoch 75/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8582 - mse: 5.2271 - mae: 1.1302\n",
      "Epoch 75: val_loss improved from 1.36247 to 1.36237, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8581 - mse: 5.2258 - mae: 1.1300 - val_loss: 1.3624 - val_mse: 9.7164 - val_mae: 1.6726 - lr: 3.1250e-05\n",
      "Epoch 76/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8565 - mse: 5.2101 - mae: 1.1283\n",
      "Epoch 76: val_loss did not improve from 1.36237\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8578 - mse: 5.2245 - mae: 1.1297 - val_loss: 1.3624 - val_mse: 9.7171 - val_mae: 1.6726 - lr: 3.1250e-05\n",
      "Epoch 77/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8564 - mse: 5.2093 - mae: 1.1282\n",
      "Epoch 77: val_loss improved from 1.36237 to 1.36236, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8577 - mse: 5.2236 - mae: 1.1295 - val_loss: 1.3624 - val_mse: 9.7177 - val_mae: 1.6725 - lr: 3.1250e-05\n",
      "Epoch 78/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8576 - mse: 5.2238 - mae: 1.1295\n",
      "Epoch 78: val_loss did not improve from 1.36236\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8575 - mse: 5.2225 - mae: 1.1293 - val_loss: 1.3624 - val_mse: 9.7182 - val_mae: 1.6726 - lr: 3.1250e-05\n",
      "Epoch 79/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8573 - mse: 5.2217 - mae: 1.1291\n",
      "Epoch 79: val_loss improved from 1.36236 to 1.36230, saving model to base_stne_gru_weight_ns.h5\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8573 - mse: 5.2217 - mae: 1.1291 - val_loss: 1.3623 - val_mse: 9.7184 - val_mae: 1.6725 - lr: 3.1250e-05\n",
      "Epoch 80/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8566 - mse: 5.2174 - mae: 1.1282\n",
      "Epoch 80: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8571 - mse: 5.2205 - mae: 1.1289 - val_loss: 1.3624 - val_mse: 9.7193 - val_mae: 1.6726 - lr: 3.1250e-05\n",
      "Epoch 81/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8571 - mse: 5.2209 - mae: 1.1289\n",
      "Epoch 81: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8570 - mse: 5.2196 - mae: 1.1287 - val_loss: 1.3624 - val_mse: 9.7201 - val_mae: 1.6726 - lr: 3.1250e-05\n",
      "Epoch 82/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8568 - mse: 5.2185 - mae: 1.1285\n",
      "Epoch 82: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8568 - mse: 5.2185 - mae: 1.1285 - val_loss: 1.3624 - val_mse: 9.7203 - val_mae: 1.6726 - lr: 3.1250e-05\n",
      "Epoch 83/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8567 - mse: 5.2194 - mae: 1.1283\n",
      "Epoch 83: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8566 - mse: 5.2178 - mae: 1.1283 - val_loss: 1.3624 - val_mse: 9.7213 - val_mae: 1.6726 - lr: 3.1250e-05\n",
      "Epoch 84/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8559 - mse: 5.2138 - mae: 1.1275\n",
      "Epoch 84: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8565 - mse: 5.2169 - mae: 1.1281 - val_loss: 1.3624 - val_mse: 9.7218 - val_mae: 1.6727 - lr: 3.1250e-05\n",
      "Epoch 85/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8562 - mse: 5.2157 - mae: 1.1279\n",
      "Epoch 85: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8562 - mse: 5.2157 - mae: 1.1279 - val_loss: 1.3625 - val_mse: 9.7229 - val_mae: 1.6727 - lr: 3.1250e-05\n",
      "Epoch 86/500\n",
      "1306/1315 [============================>.] - ETA: 0s - loss: 0.8712 - mse: 5.2802 - mae: 1.1460\n",
      "Epoch 86: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 6ms/step - loss: 0.8729 - mse: 5.2982 - mae: 1.1477 - val_loss: 1.3787 - val_mse: 9.8668 - val_mae: 1.6877 - lr: 1.5625e-05\n",
      "Epoch 87/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8705 - mse: 5.2845 - mae: 1.1442\n",
      "Epoch 87: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8705 - mse: 5.2845 - mae: 1.1442 - val_loss: 1.3795 - val_mse: 9.8845 - val_mae: 1.6887 - lr: 1.5625e-05\n",
      "Epoch 88/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8697 - mse: 5.2822 - mae: 1.1431\n",
      "Epoch 88: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8696 - mse: 5.2806 - mae: 1.1431 - val_loss: 1.3799 - val_mse: 9.8912 - val_mae: 1.6892 - lr: 1.5625e-05\n",
      "Epoch 89/500\n",
      "1314/1315 [============================>.] - ETA: 0s - loss: 0.8687 - mse: 5.2758 - mae: 1.1420\n",
      "Epoch 89: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8690 - mse: 5.2780 - mae: 1.1423 - val_loss: 1.3799 - val_mse: 9.8934 - val_mae: 1.6892 - lr: 1.5625e-05\n",
      "Epoch 90/500\n",
      "1307/1315 [============================>.] - ETA: 0s - loss: 0.8673 - mse: 5.2626 - mae: 1.1404\n",
      "Epoch 90: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8685 - mse: 5.2763 - mae: 1.1417 - val_loss: 1.3797 - val_mse: 9.8944 - val_mae: 1.6891 - lr: 1.5625e-05\n",
      "Epoch 91/500\n",
      "1308/1315 [============================>.] - ETA: 0s - loss: 0.8676 - mse: 5.2719 - mae: 1.1406\n",
      "Epoch 91: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8681 - mse: 5.2747 - mae: 1.1412 - val_loss: 1.3797 - val_mse: 9.8951 - val_mae: 1.6890 - lr: 1.5625e-05\n",
      "Epoch 92/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8678 - mse: 5.2750 - mae: 1.1409\n",
      "Epoch 92: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8678 - mse: 5.2733 - mae: 1.1409 - val_loss: 1.3796 - val_mse: 9.8955 - val_mae: 1.6890 - lr: 1.5625e-05\n",
      "Epoch 93/500\n",
      "1310/1315 [============================>.] - ETA: 0s - loss: 0.8679 - mse: 5.2762 - mae: 1.1408\n",
      "Epoch 93: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 8s 6ms/step - loss: 0.8675 - mse: 5.2722 - mae: 1.1405 - val_loss: 1.3796 - val_mse: 9.8961 - val_mae: 1.6889 - lr: 1.5625e-05\n",
      "Epoch 94/500\n",
      "1312/1315 [============================>.] - ETA: 0s - loss: 0.8676 - mse: 5.2740 - mae: 1.1405\n",
      "Epoch 94: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8673 - mse: 5.2712 - mae: 1.1403 - val_loss: 1.3795 - val_mse: 9.8964 - val_mae: 1.6888 - lr: 1.5625e-05\n",
      "Epoch 95/500\n",
      "1315/1315 [==============================] - ETA: 0s - loss: 0.8671 - mse: 5.2703 - mae: 1.1400\n",
      "Epoch 95: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8671 - mse: 5.2703 - mae: 1.1400 - val_loss: 1.3794 - val_mse: 9.8969 - val_mae: 1.6887 - lr: 1.5625e-05\n",
      "Epoch 96/500\n",
      "1311/1315 [============================>.] - ETA: 0s - loss: 0.8765 - mse: 5.3173 - mae: 1.1516\n",
      "Epoch 96: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8763 - mse: 5.3152 - mae: 1.1515 - val_loss: 1.3932 - val_mse: 10.0076 - val_mae: 1.7027 - lr: 7.8125e-06\n",
      "Epoch 97/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8747 - mse: 5.3022 - mae: 1.1491\n",
      "Epoch 97: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8746 - mse: 5.3016 - mae: 1.1490 - val_loss: 1.3958 - val_mse: 10.0342 - val_mae: 1.7053 - lr: 7.8125e-06\n",
      "Epoch 98/500\n",
      "1309/1315 [============================>.] - ETA: 0s - loss: 0.8740 - mse: 5.2990 - mae: 1.1480\n",
      "Epoch 98: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 7ms/step - loss: 0.8740 - mse: 5.2973 - mae: 1.1480 - val_loss: 1.3965 - val_mse: 10.0431 - val_mae: 1.7060 - lr: 7.8125e-06\n",
      "Epoch 99/500\n",
      "1313/1315 [============================>.] - ETA: 0s - loss: 0.8736 - mse: 5.2956 - mae: 1.1475\n",
      "Epoch 99: val_loss did not improve from 1.36230\n",
      "1315/1315 [==============================] - 9s 6ms/step - loss: 0.8735 - mse: 5.2950 - mae: 1.1475 - val_loss: 1.3969 - val_mse: 10.0478 - val_mae: 1.7065 - lr: 7.8125e-06\n"
     ]
    }
   ],
   "source": [
    "gru_history = gru_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_gru])\n",
    "gru_history_ns = gru_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_gru_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------lstm_history-------------\n",
      "lstm_history Validation Loss: 1.3292748928070068\n",
      "lstm_history Validation MSE: 9.404826164245605\n",
      "lstm_history Validation MAE: 1.6443835496902466\n",
      "-------------rnn_history-------------\n",
      "rnn_history Validation Loss: 1.3138341903686523\n",
      "rnn_history Validation MSE: 9.36812686920166\n",
      "rnn_history Validation MAE: 1.627488374710083\n",
      "-------------gru_history-------------\n",
      "gru_history Validation Loss: 1.3601264953613281\n",
      "gru_history Validation MSE: 9.930880546569824\n",
      "gru_history Validation MAE: 1.676413655281067\n",
      "-------------lstm_history_ns-------------\n",
      "lstm_history_ns Validation Loss: 1.3355653285980225\n",
      "lstm_history_ns Validation MSE: 9.606619834899902\n",
      "lstm_history_ns Validation MAE: 1.643784999847412\n",
      "-------------rnn_history_ns-------------\n",
      "rnn_history_ns Validation Loss: 1.3187620639801025\n",
      "rnn_history_ns Validation MSE: 9.421436309814453\n",
      "rnn_history_ns Validation MAE: 1.6267199516296387\n",
      "-------------gru_history_ns-------------\n",
      "gru_history_ns Validation Loss: 1.3622968196868896\n",
      "gru_history_ns Validation MSE: 9.715559005737305\n",
      "gru_history_ns Validation MAE: 1.672486424446106\n"
     ]
    }
   ],
   "source": [
    "# 종합 결과\n",
    "\n",
    "history_list = [\"lstm_history\", \"rnn_history\", \"gru_history\", \"lstm_history_ns\", \"rnn_history_ns\", \"gru_history_ns\"]\n",
    "def result(historys) :\n",
    "  for name, history in globals().items() :\n",
    "    if name in history_list :\n",
    "      print(f\"-------------{name}-------------\")\n",
    "      val_loss = min(history.history['val_loss'])\n",
    "      val_mse = min(history.history['val_mse'])\n",
    "      val_mae = min(history.history['val_mae'])\n",
    "      print(f\"{name} Validation Loss:\", val_loss)\n",
    "      print(f\"{name} Validation MSE:\", val_mse)\n",
    "      print(f\"{name} Validation MAE:\", val_mae)\n",
    "\n",
    "result(history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla, lstm_cell_45_layer_call_fn, lstm_cell_45_layer_call_and_return_conditional_losses, lstm_cell_46_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/lstm_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/lstm_model\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/rnn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/rnn_model\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, gru_cell_45_layer_call_fn, gru_cell_45_layer_call_and_return_conditional_losses, gru_cell_46_layer_call_fn, gru_cell_46_layer_call_and_return_conditional_losses while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/gru_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/gru_model\\assets\n"
     ]
    }
   ],
   "source": [
    "lstm_save_path = f\"./Models/lstm_model\"\n",
    "rnn_save_path = f\"./Models/rnn_model\"\n",
    "gru_save_path = f\"./Models/gru_model\"\n",
    "\n",
    "save_model(lstm_model, lstm_save_path, overwrite=True)\n",
    "save_model(rnn_model, rnn_save_path, overwrite=True)\n",
    "save_model(gru_model, gru_save_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기 테스트\n",
    "\n",
    "load_path = f\"./Models/lstm_model\"\n",
    "loaded_model = load_model(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight 0 :\n",
      "[[[ 4.50252369e-02  7.39237666e-02  7.30416179e-02 -1.51226074e-02\n",
      "    3.04443017e-02 -4.09716889e-02  5.30016422e-03  7.02002198e-02\n",
      "   -6.55354485e-02  2.08604187e-02  5.25219142e-02 -2.17764191e-02\n",
      "   -7.78335258e-02  4.99977320e-02 -6.48089796e-02  2.48604268e-03\n",
      "   -6.46748170e-02  6.59905672e-02 -7.64652193e-02 -5.87883890e-02\n",
      "    5.52380830e-02 -1.57087073e-02 -3.95580567e-02 -4.84729148e-02\n",
      "   -6.56510293e-02  1.01358443e-02 -7.00003803e-02  5.29963374e-02\n",
      "   -2.82853059e-02  2.02218518e-02  3.81401181e-02  5.83110452e-02]]\n",
      "\n",
      " [[-5.85548021e-02 -3.62046063e-03  6.99618757e-02 -3.64702716e-02\n",
      "    1.08989775e-02  5.57470769e-02  6.81544393e-02 -2.57991888e-02\n",
      "    2.77585313e-02  2.06179395e-02 -6.57043904e-02  7.52088726e-02\n",
      "    6.40983433e-02  5.02517819e-02 -7.03691840e-02 -2.22762078e-02\n",
      "    4.17561084e-02  1.91112831e-02  5.96239120e-02 -1.05139613e-03\n",
      "   -2.13136040e-02  2.55293474e-02 -2.86225937e-02 -3.25790271e-02\n",
      "   -3.53750326e-02  1.72039494e-02 -1.10209361e-02 -3.95445824e-02\n",
      "   -4.29529138e-02  5.06311655e-02  6.02465719e-02  4.53864634e-02]]\n",
      "\n",
      " [[-1.04941428e-04  4.48773205e-02  3.50532606e-02 -4.65617031e-02\n",
      "   -3.54966633e-02 -5.22074178e-02 -5.23956046e-02  6.80828393e-02\n",
      "   -5.55353239e-02 -8.24460387e-03  4.55203429e-02  1.84561014e-02\n",
      "   -4.34303917e-02  2.24088430e-02 -7.32026473e-02  3.82797569e-02\n",
      "    7.56152719e-02 -7.02000707e-02 -3.21917236e-04 -6.66462407e-02\n",
      "   -4.34893407e-02  2.14538127e-02  4.75751758e-02 -8.55018944e-03\n",
      "   -9.12650675e-03  6.63032532e-02  4.12123352e-02 -1.57387741e-02\n",
      "    5.33317178e-02 -3.30708697e-02 -3.37038711e-02  2.24135220e-02]]\n",
      "\n",
      " [[ 5.14413118e-02  2.57866010e-02 -2.77772211e-02 -1.71271451e-02\n",
      "   -1.58472992e-02 -5.23242950e-02  3.28763500e-02 -4.54114117e-02\n",
      "   -3.20533440e-02 -7.28493929e-02  5.60442805e-02 -2.95887403e-02\n",
      "    6.36528283e-02  3.59787792e-03 -1.06168091e-02 -5.42679653e-02\n",
      "    4.45135832e-02 -7.58307353e-02  1.99794322e-02  1.44037828e-02\n",
      "   -4.10043038e-02 -1.23767555e-03  4.63155285e-02  7.59056658e-02\n",
      "    5.49008697e-02  5.56736588e-02  2.20674872e-02 -1.31541565e-02\n",
      "    5.59104681e-02 -2.52590105e-02 -1.65480115e-02  7.21946657e-02]]\n",
      "\n",
      " [[ 8.98889452e-03  3.90676260e-02  5.79381883e-02  1.10911876e-02\n",
      "   -3.01019661e-02  2.23490968e-02 -3.29647399e-02 -2.89321318e-02\n",
      "   -4.35334966e-02  5.80558628e-02 -7.63583258e-02 -5.53682558e-02\n",
      "    7.33324289e-02  6.07131124e-02  8.88315588e-03  2.14000791e-03\n",
      "    3.47187743e-02  3.08166891e-02 -1.66366771e-02 -1.03636980e-02\n",
      "    6.18728101e-02 -2.54461244e-02 -4.09144126e-02 -8.60190392e-03\n",
      "   -3.22372094e-02 -1.80445537e-02  1.35636851e-02  5.54945320e-02\n",
      "   -2.93891020e-02  3.79184857e-02  3.85943428e-02  6.85306787e-02]]\n",
      "\n",
      " [[-1.75710656e-02  9.50080156e-03  7.38149285e-02 -6.19919039e-02\n",
      "   -2.23359466e-03 -3.16545479e-02  2.50290781e-02 -2.14294344e-03\n",
      "    9.13640112e-03 -6.13190159e-02  6.91021234e-02  2.49641761e-02\n",
      "    2.44128779e-02  1.32509917e-02  1.91015601e-02 -6.37173802e-02\n",
      "   -3.15235071e-02 -6.21261001e-02 -6.42717704e-02  4.05710340e-02\n",
      "    7.04596192e-03 -5.09308726e-02  1.55389458e-02  2.58823782e-02\n",
      "   -6.90922141e-03  4.18556333e-02 -2.88660750e-02 -6.74243122e-02\n",
      "    2.19158456e-02  3.70993018e-02  9.14556533e-03  4.53303754e-02]]\n",
      "\n",
      " [[ 6.15952164e-02  5.92143089e-02  6.11773580e-02 -6.03712350e-03\n",
      "   -8.70677084e-03 -8.20077956e-04  3.38124335e-02 -6.14392012e-03\n",
      "   -2.83236876e-02 -6.06433302e-03 -3.58598828e-02 -3.63476574e-04\n",
      "   -1.71183497e-02  2.54415795e-02 -3.89038809e-02  1.74834803e-02\n",
      "   -4.63295430e-02 -2.03516707e-02  7.56922662e-02 -5.05952537e-02\n",
      "   -1.70379430e-02  6.55379146e-02 -6.35417700e-02 -6.60596266e-02\n",
      "   -6.27804399e-02 -3.04847285e-02  2.15525776e-02  3.40652838e-02\n",
      "    5.53198308e-02  5.96818775e-02  5.48852235e-02 -3.09994034e-02]]\n",
      "\n",
      " [[-1.28668174e-02  3.36294994e-02 -3.40094566e-02 -1.61830857e-02\n",
      "   -3.50438729e-02 -1.71022378e-02  4.87904400e-02 -5.50742336e-02\n",
      "    4.05987278e-02  5.18860370e-02 -1.36326030e-02 -1.44433752e-02\n",
      "    2.18075439e-02  5.86148053e-02  3.31838876e-02 -5.17357290e-02\n",
      "    3.39067727e-03  4.37646359e-02 -2.27630772e-02 -3.49880047e-02\n",
      "   -7.72964805e-02 -7.01041147e-02  6.11778945e-02  2.43723765e-02\n",
      "   -4.51625660e-02  4.11613658e-02 -6.88571557e-02  4.31903824e-02\n",
      "   -5.93309812e-02  1.34266689e-02  2.93411314e-04 -4.10059914e-02]]\n",
      "\n",
      " [[-1.07045099e-02  3.58022749e-04  1.91023350e-02 -6.93123415e-02\n",
      "   -5.42771555e-02  3.36487964e-02 -7.13661015e-02  3.36629227e-02\n",
      "   -4.69044447e-02  3.41556221e-02 -4.49645594e-02  6.79933876e-02\n",
      "    5.71554750e-02 -8.68358463e-03 -6.22451082e-02  1.98204741e-02\n",
      "    1.59490481e-02 -2.94399001e-02 -1.35478154e-02  5.57687283e-02\n",
      "    5.45056760e-02  6.52196854e-02 -1.29338726e-02 -2.90689617e-02\n",
      "    1.82171911e-02  4.97739017e-03  6.98340386e-02 -9.95266438e-03\n",
      "   -4.97450754e-02  5.13338298e-02 -4.53049652e-02 -6.01930395e-02]]\n",
      "\n",
      " [[-4.52283770e-03 -4.47259098e-03 -3.09504420e-02  3.08802426e-02\n",
      "   -3.43516059e-02 -5.43828756e-02 -2.05489323e-02  1.66775435e-02\n",
      "   -9.80854034e-03  7.14407116e-02 -3.89373451e-02  8.47525895e-04\n",
      "    6.41578436e-02 -5.81782386e-02  9.12202895e-03 -1.54416896e-02\n",
      "    8.33789259e-03  7.71851838e-03  1.37399212e-02 -1.50595382e-02\n",
      "    5.78891188e-02  2.36082450e-02 -4.37184423e-03  7.14755505e-02\n",
      "   -6.41733781e-02 -1.08016953e-02  5.88302761e-02 -7.73329213e-02\n",
      "    2.21822336e-02  2.99072638e-02 -7.04630464e-02  5.00229299e-02]]\n",
      "\n",
      " [[-4.53817137e-02  4.37123105e-02  7.20726103e-02 -9.05767828e-03\n",
      "    5.24135828e-02  1.41710863e-02 -3.42506357e-02 -6.76249564e-02\n",
      "    1.04982480e-02  4.90561873e-02  6.47850335e-02 -8.55448097e-03\n",
      "   -2.23566517e-02  5.65777719e-02  6.83689117e-02 -2.35803686e-02\n",
      "    2.07880735e-02  2.22872496e-02  7.67560601e-02  1.15525797e-02\n",
      "    6.81848079e-02  7.67081380e-02 -4.83384058e-02 -6.17583320e-02\n",
      "   -4.58294563e-02 -6.34793118e-02 -4.49260250e-02  4.50358689e-02\n",
      "    2.60458887e-03 -7.40773827e-02  5.30657768e-02  5.04769832e-02]]\n",
      "\n",
      " [[ 2.15588510e-02 -6.33383468e-02 -6.18841015e-02  4.43196818e-02\n",
      "    7.05685765e-02 -2.09987387e-02  1.87355354e-02  7.47137368e-02\n",
      "   -2.03423686e-02 -3.28817368e-02 -2.51351371e-02  4.98047620e-02\n",
      "   -6.72276244e-02 -4.55730781e-02  7.58696049e-02  4.69102934e-02\n",
      "    5.13028651e-02 -7.42063448e-02  6.22453690e-02  6.31028265e-02\n",
      "   -6.31748214e-02  3.11543867e-02  2.90415287e-02  2.56963819e-02\n",
      "    2.04682276e-02  7.19222724e-02  6.24129325e-02 -4.59953360e-02\n",
      "   -2.47194283e-02 -4.99401689e-02  2.96152458e-02  4.52014357e-02]]\n",
      "\n",
      " [[-1.54120475e-03  1.71078816e-02  3.02157253e-02 -6.04861714e-02\n",
      "    1.66028738e-04 -5.44932038e-02 -6.33859187e-02 -7.21682087e-02\n",
      "   -6.34187534e-02 -2.60820948e-02 -8.22476298e-03 -6.84775710e-02\n",
      "    6.68128431e-02 -3.25492546e-02  3.84401903e-02  2.37191841e-02\n",
      "   -4.13752794e-02 -1.42443180e-03  1.50282681e-02  4.61847335e-02\n",
      "    1.73469782e-02 -3.39138508e-02 -3.29250209e-02  3.15269008e-02\n",
      "    5.50997406e-02  1.89776793e-02  5.53374588e-02  3.81151363e-02\n",
      "   -6.50382638e-02 -3.57947499e-02  3.66317183e-02 -1.34271383e-03]]\n",
      "\n",
      " [[ 4.02950719e-02  7.24310279e-02 -2.33944058e-02  5.41646332e-02\n",
      "   -7.65223801e-03  4.71964628e-02 -6.92619532e-02 -5.93400747e-02\n",
      "    1.17924735e-02 -6.12399466e-02  4.97872084e-02 -4.17390168e-02\n",
      "   -4.92814071e-02 -7.31439218e-02  1.12885311e-02  7.54910260e-02\n",
      "    6.74416870e-02  5.63031286e-02 -1.87746808e-02  1.62460655e-02\n",
      "   -1.67049468e-04 -5.47675714e-02  2.82759145e-02  4.56124991e-02\n",
      "    6.96798712e-02  7.70435929e-02  1.38890892e-02 -3.90490629e-02\n",
      "    1.14313364e-02  3.01892981e-02 -1.07105970e-02 -9.95390862e-03]]\n",
      "\n",
      " [[ 3.63093540e-02 -7.46879727e-03  3.51744816e-02  3.18051875e-02\n",
      "    1.57436728e-02 -5.25797084e-02 -1.75917447e-02  7.33515173e-02\n",
      "    1.63891315e-02  2.11264342e-02 -3.48181538e-02  1.23788118e-02\n",
      "    3.14508006e-02  4.75567281e-02 -7.63928890e-02  1.24571621e-02\n",
      "   -5.56596220e-02 -7.09992498e-02  2.72543803e-02  3.78852785e-02\n",
      "    5.26302904e-02 -1.48098394e-02  5.86191565e-02  7.16342926e-02\n",
      "   -6.94651380e-02 -7.37746060e-03 -7.24656656e-02 -4.10766527e-02\n",
      "   -4.88221943e-02 -4.12253626e-02 -1.92821547e-02 -2.69129872e-04]]\n",
      "\n",
      " [[-6.54839650e-02  6.31337911e-02  6.35416061e-03 -2.99825855e-02\n",
      "   -3.97125967e-02  2.23189518e-02  1.36761069e-02  1.28708780e-03\n",
      "   -1.47031173e-02 -3.64892781e-02 -1.95170976e-02 -3.20776217e-02\n",
      "   -7.00619817e-02 -5.59622198e-02 -3.04606557e-02 -6.27195835e-02\n",
      "   -3.71421203e-02  2.58601978e-02 -2.88005173e-03  3.81924585e-02\n",
      "    1.85146108e-02 -8.97461921e-03 -2.47160159e-02  1.57853439e-02\n",
      "    6.74650073e-03 -7.34103993e-02 -3.34073603e-02 -2.00905912e-02\n",
      "    5.28740883e-02 -5.39676175e-02  4.66813147e-03  4.62642983e-02]]\n",
      "\n",
      " [[ 4.68810424e-02 -4.21449766e-02  2.22807154e-02  7.16651231e-03\n",
      "   -5.28049245e-02  4.89141643e-02 -3.83779407e-04 -3.40647288e-02\n",
      "   -7.05252215e-02 -4.67453077e-02  5.94795495e-02  7.23255724e-02\n",
      "   -5.00731394e-02  2.53464505e-02  2.65645981e-02  7.78306872e-02\n",
      "    6.35563433e-02  6.22300357e-02  5.32159507e-02 -4.10762802e-02\n",
      "    1.30004510e-02  1.74047053e-03 -2.06787847e-02  4.43488359e-02\n",
      "    2.90063545e-02 -7.22604245e-02 -4.88194823e-02 -2.39399821e-03\n",
      "    1.82171911e-02 -3.09940577e-02  4.51839641e-02 -2.19680220e-02]]\n",
      "\n",
      " [[ 4.26996350e-02  4.61254269e-02  5.56655377e-02 -4.06919792e-02\n",
      "    1.73526779e-02 -6.51351288e-02 -3.74176763e-02  3.97525206e-02\n",
      "    7.74200261e-02  1.43687204e-02 -2.57290304e-02 -5.64947128e-02\n",
      "   -7.38706887e-02 -4.41826656e-02  6.26410544e-02 -6.88837022e-02\n",
      "    4.57622707e-02 -5.64545467e-02  5.79271913e-02  6.71636313e-02\n",
      "    7.02551454e-02  2.40870789e-02 -5.17067909e-02  2.34279111e-02\n",
      "    5.71446270e-02 -6.76840544e-03  5.12303114e-02 -1.54847875e-02\n",
      "    5.75402528e-02 -4.59282026e-02  1.43431276e-02 -3.20432298e-02]]\n",
      "\n",
      " [[ 5.02499342e-02 -1.40141770e-02  7.14894384e-02 -1.89665705e-03\n",
      "   -6.09778315e-02  4.12445366e-02  9.76015627e-03 -7.43429512e-02\n",
      "   -1.97321251e-02 -4.26748358e-02  7.47706443e-02 -6.23230673e-02\n",
      "    2.73877010e-02 -6.10923320e-02 -6.55681342e-02 -3.10372487e-02\n",
      "   -3.30679119e-04  4.79390621e-02 -7.75790885e-02 -7.43663311e-03\n",
      "   -3.54797915e-02 -3.76577787e-02  4.12864238e-03 -5.40330634e-02\n",
      "    7.07615763e-02 -4.11055684e-02 -3.96191254e-02  5.05544245e-04\n",
      "    6.99947029e-03  4.01496887e-02  6.94292188e-02 -8.53113085e-03]]\n",
      "\n",
      " [[-6.77345246e-02  1.65603533e-02  6.27760589e-02 -1.87157691e-02\n",
      "   -4.78452779e-02 -6.17176294e-02 -3.53177004e-02 -7.51679614e-02\n",
      "    6.04148209e-02 -7.07584918e-02  7.14363754e-02 -7.71110803e-02\n",
      "    5.78467101e-02  6.80626780e-02 -1.06009096e-02  3.27200890e-02\n",
      "   -2.05135569e-02 -2.46470980e-02 -5.06239682e-02  3.10604870e-02\n",
      "   -3.02724689e-02 -6.11644387e-02  2.57810578e-02 -6.08447678e-02\n",
      "   -7.32074529e-02  4.75766510e-03 -1.37074217e-02 -2.18772776e-02\n",
      "    3.08283642e-02 -7.09593222e-02 -2.38854699e-02  5.26012927e-02]]\n",
      "\n",
      " [[ 6.57850355e-02  9.14113224e-03 -2.15252340e-02 -4.51061949e-02\n",
      "    3.19021493e-02 -3.29793468e-02  1.35767460e-03  5.41973263e-02\n",
      "   -9.93952155e-04 -5.72260320e-02  1.95624456e-02  4.44019437e-02\n",
      "    2.51608267e-02 -4.55735959e-02  4.45254445e-02  5.54268062e-03\n",
      "   -5.88941723e-02  7.58235902e-02  7.05389082e-02 -4.32096496e-02\n",
      "   -2.44340561e-02  2.00246647e-02 -5.95235862e-02 -2.35270038e-02\n",
      "   -4.31978814e-02  6.51837736e-02 -6.11486286e-02  3.89544591e-02\n",
      "   -4.54889201e-02  7.66833425e-02  1.55792460e-02 -3.46697383e-02]]\n",
      "\n",
      " [[-7.06717446e-02 -5.59838116e-03 -5.85005507e-02 -3.81846838e-02\n",
      "   -7.35513493e-02  3.89738008e-02  2.97254249e-02  5.75349331e-02\n",
      "    4.16341275e-02 -4.92099077e-02  5.79795688e-02 -3.35383452e-02\n",
      "   -3.23519520e-02  6.96254820e-02  7.37015009e-02 -6.53156713e-02\n",
      "   -2.24829391e-02 -5.25329560e-02  1.37429088e-02 -1.03617832e-02\n",
      "    4.28931117e-02  8.12885910e-03 -6.95135817e-02 -6.72161579e-02\n",
      "   -7.43722394e-02 -4.48058061e-02  4.45671156e-02  6.34045154e-02\n",
      "   -3.54562551e-02 -1.98622756e-02  2.66630203e-03 -7.54121467e-02]]\n",
      "\n",
      " [[ 4.03493643e-02  4.24105078e-02 -5.07033169e-02  1.35026872e-04\n",
      "   -2.08678097e-02 -2.81895325e-02 -2.10838020e-02 -1.65967122e-02\n",
      "   -5.49828932e-02 -1.12869889e-02  2.95925438e-02 -7.38546848e-02\n",
      "    3.96356955e-02  5.69872111e-02 -4.36723530e-02 -6.42715693e-02\n",
      "    3.03932279e-02 -2.74886712e-02 -7.01561421e-02  3.80062759e-02\n",
      "    5.54654002e-03 -2.35644057e-02  2.28966027e-02 -1.90224499e-03\n",
      "    4.34485823e-02  6.52114749e-02 -1.35067925e-02 -7.26956204e-02\n",
      "    3.13247591e-02 -7.07697049e-02 -6.98180944e-02 -4.00576070e-02]]\n",
      "\n",
      " [[-2.98488885e-02 -7.64589831e-02  2.65912563e-02 -1.09992623e-02\n",
      "    2.78672799e-02  4.65367734e-02 -3.98311801e-02 -7.01552704e-02\n",
      "    6.16296828e-02  5.26589900e-03  3.29215080e-02  3.10104862e-02\n",
      "    4.10276726e-02  2.30141282e-02 -4.51386236e-02  6.33366704e-02\n",
      "   -2.08988227e-02 -7.94276595e-03 -6.99939579e-02 -2.50848383e-02\n",
      "   -5.03886938e-02 -6.25885427e-02 -1.13897994e-02  5.72734177e-02\n",
      "   -3.46869268e-02 -5.05673885e-03 -5.25702238e-02  4.24702466e-03\n",
      "    1.43271685e-02 -7.16998801e-02 -3.30384448e-02  7.26449788e-02]]\n",
      "\n",
      " [[ 5.35321534e-02  2.49783173e-02  4.41965461e-03 -5.59028648e-02\n",
      "    3.79606709e-02 -4.02838811e-02  2.82381475e-03  4.08097655e-02\n",
      "   -1.51563361e-02  5.25287092e-02 -2.16559954e-02  7.69715607e-02\n",
      "    2.77188867e-02  3.78453210e-02  3.38416845e-02  4.83590961e-02\n",
      "    6.90769702e-02 -3.18219848e-02 -6.95708245e-02  4.25817147e-02\n",
      "   -6.81188405e-02 -7.58373216e-02  6.16251081e-02 -2.75729373e-02\n",
      "   -2.02741213e-02  4.09030169e-02  3.16477716e-02 -6.80831522e-02\n",
      "   -1.50812417e-03  4.92827445e-02 -3.91541198e-02  6.24803901e-02]]\n",
      "\n",
      " [[-4.67980355e-02 -1.33413970e-02  6.59960508e-02  4.79630083e-02\n",
      "    7.28502125e-02 -3.75698917e-02  4.52864766e-02  7.17066824e-02\n",
      "    1.10637546e-02  4.06319350e-02  5.69300801e-02 -6.08734787e-03\n",
      "    6.98983669e-02 -6.31617904e-02  7.92043656e-03 -2.73998044e-02\n",
      "   -2.09981985e-02  1.63587853e-02  4.37509269e-03  4.04747576e-03\n",
      "   -6.37989491e-02 -6.80054724e-02  6.54407293e-02 -7.84916431e-03\n",
      "   -6.33487403e-02  9.03464854e-03  4.83650267e-02  1.23349205e-02\n",
      "   -2.35242583e-02  3.91231403e-02 -4.68848832e-02 -4.71538119e-02]]\n",
      "\n",
      " [[ 2.08062083e-02  6.97244555e-02 -5.83218634e-02 -5.62783107e-02\n",
      "   -6.30144179e-02 -6.27800748e-02  4.27243337e-02  6.33494854e-02\n",
      "    3.32735181e-02  6.26112223e-02 -5.20912036e-02  7.42528737e-02\n",
      "   -5.61948866e-03 -7.04537481e-02  5.80848902e-02  4.66013849e-02\n",
      "   -2.99155042e-02 -5.01555502e-02  4.94401604e-02 -5.22086397e-02\n",
      "   -3.92922498e-02 -3.60058248e-02  5.52368611e-02 -3.94045599e-02\n",
      "   -6.42816648e-02  1.27426982e-02 -6.53423220e-02  2.68806219e-02\n",
      "    8.57940316e-03  4.10848111e-03 -2.09618770e-02 -6.82504773e-02]]\n",
      "\n",
      " [[-1.35389358e-01  2.69462407e-01 -1.29067618e-02 -3.49188536e-01\n",
      "   -6.77436367e-02 -1.62680745e-02 -1.55256078e-01 -2.25449763e-02\n",
      "   -6.31948784e-02 -7.28854239e-02 -8.95466432e-02 -2.29988247e-03\n",
      "   -9.33243185e-02 -3.19574843e-04 -3.38271149e-02 -9.17080492e-02\n",
      "   -8.91859978e-02  1.94075897e-01 -1.11704484e-01 -7.29160979e-02\n",
      "   -1.99954107e-01 -2.82948047e-01 -1.42481560e-02 -9.17835981e-02\n",
      "    3.08813274e-01  2.84986079e-01 -1.47508904e-02 -1.65076964e-02\n",
      "    3.51025075e-01  1.95842624e-01 -1.10116452e-01 -3.03338151e-02]]\n",
      "\n",
      " [[-1.10610165e-01  6.40972018e-01 -1.20868459e-01  3.76142889e-01\n",
      "   -7.47325420e-02 -5.72311357e-02 -9.39134732e-02  1.89574644e-01\n",
      "   -8.14683437e-02 -5.94397672e-02 -4.71440330e-02  2.79958884e-04\n",
      "   -9.70553532e-02  2.04330876e-01 -1.47199318e-01 -1.00981377e-01\n",
      "   -6.56170622e-02 -1.74317732e-01 -6.70766830e-02 -6.25855625e-02\n",
      "   -3.85356359e-02 -1.22506671e-01 -1.41444594e-01 -9.03966725e-02\n",
      "   -9.22201127e-02 -4.71650846e-02 -4.45005931e-02 -1.00691274e-01\n",
      "   -1.21657513e-01  5.95577657e-02  8.57289415e-03 -1.19510427e-01]]\n",
      "\n",
      " [[ 2.59770866e-04 -2.74668503e+00  3.43138576e-01 -2.27904111e-01\n",
      "   -7.58262053e-02 -4.63369116e-02 -1.19126262e-02 -2.14495867e-01\n",
      "   -2.07516924e-02 -3.51122878e-02  2.67283410e-01 -1.05942367e-02\n",
      "   -7.17020535e-04 -2.35866934e-01  2.29456544e-01  1.02634775e-03\n",
      "    2.94631720e-01 -7.46340826e-02 -1.79878287e-02  3.23830485e-01\n",
      "   -1.67047773e-02  1.66919202e-01 -2.40737223e-04  5.89440169e-04\n",
      "   -2.00384200e-01 -2.64754951e-01 -2.92732567e-03  3.27149242e-01\n",
      "   -2.15674832e-01 -2.51629680e-01 -1.32029057e-01  3.14327449e-01]]]\n",
      "------------------\n",
      "Weight 1 :\n",
      "[-0.06613105  0.07844511  0.00151683  0.0352812   0.          0.\n",
      " -0.00464981 -0.15978575 -0.00404283  0.         -0.0124444  -0.00442272\n",
      " -0.00988355 -0.2627551   0.00665633 -0.06170285 -0.00569371 -0.34260887\n",
      " -0.00377969 -0.00768908 -0.00374729 -0.47273698 -0.04485603 -0.07102702\n",
      " -0.342737   -0.28385186  0.          0.00433504 -0.3089538  -0.19068627\n",
      " -0.07369378  0.00980587]\n",
      "------------------\n",
      "Weight 2 :\n",
      "[[-4.05149609e-02 -5.78480633e-03 -2.26494446e-01 ...  5.71465045e-02\n",
      "   5.56231756e-03 -1.86192542e-01]\n",
      " [-2.61194742e-04 -9.24284831e-02  4.75906700e-01 ...  1.10444868e+00\n",
      "  -7.26867467e-02 -6.31359875e-01]\n",
      " [-2.68423587e-01  3.12826425e-01  6.86645061e-02 ... -1.25627160e-01\n",
      "   2.55444914e-01 -4.78990823e-01]\n",
      " ...\n",
      " [ 2.16132388e-01  1.00603521e+00  1.53885856e-01 ...  1.06114373e-01\n",
      "   9.83574912e-02 -2.71825809e-02]\n",
      " [ 1.77469682e-02  2.76824217e-02  7.86867365e-02 ...  9.77227613e-02\n",
      "   3.42270061e-02 -3.37859220e-03]\n",
      " [-1.08826995e-01  3.66995424e-01 -7.43919834e-02 ... -6.92177936e-02\n",
      "   8.72518420e-02 -3.69433850e-01]]\n",
      "------------------\n",
      "Weight 3 :\n",
      "[[ 0.1792624   1.1026952  -0.21208194 ... -0.37590158  0.5021219\n",
      "  -0.5257683 ]\n",
      " [-0.25492245  0.02967714 -0.27528182 ... -0.16644096 -0.279712\n",
      "  -0.08416269]\n",
      " [ 0.23207986  0.18430741 -0.03183699 ... -0.06264454 -0.27379653\n",
      "  -0.14591207]\n",
      " ...\n",
      " [ 0.07047827  0.028334   -0.04913499 ...  0.02847466 -0.03819056\n",
      "   0.11395841]\n",
      " [ 0.40359023  0.27189195 -0.11215846 ... -0.19008411 -0.00567989\n",
      "  -0.02317501]\n",
      " [ 0.09589643  0.45955405 -0.47578758 ... -0.686309    0.04822259\n",
      "  -0.34349412]]\n",
      "------------------\n",
      "Weight 4 :\n",
      "[ 6.38598025e-01  1.30732521e-01 -6.34935439e-01 -6.53543890e-01\n",
      " -1.47743061e-01 -6.13620877e-01 -6.42574370e-01 -1.05274725e+00\n",
      "  1.74154434e-02  1.01063162e-01 -2.54031837e-01  2.38661170e-01\n",
      "  6.07128263e-01 -8.29344511e-01 -1.53156483e+00 -6.63205922e-01\n",
      " -8.24333131e-01 -6.78599775e-01 -4.19963896e-01 -5.06499410e-01\n",
      " -8.92398357e-01  2.64478624e-01 -3.50247830e-01 -1.28971651e-01\n",
      " -4.47549045e-01 -7.60305822e-01 -3.94795209e-01 -9.75734055e-01\n",
      " -2.04396859e-01 -3.55385780e-01 -4.06681746e-01  2.35183969e-01\n",
      " -6.38990775e-02 -3.28818172e-01 -2.27547869e-01 -1.16392696e+00\n",
      "  1.45766795e-01 -5.20089984e-01 -2.83556640e-01  2.49897495e-01\n",
      " -7.33185351e-01 -8.13595235e-01 -4.70764697e-01 -6.89430475e-01\n",
      " -1.00076067e+00 -5.18465459e-01 -1.15440583e+00 -8.28745067e-01\n",
      " -3.64636779e-01 -6.26215637e-01 -7.47230053e-01  5.96382320e-01\n",
      " -3.77290398e-01 -2.96737760e-01 -7.93383896e-01 -6.02667093e-01\n",
      " -2.64963895e-01 -9.08437431e-01 -5.59949167e-02 -9.65863168e-01\n",
      " -4.63752031e-01 -8.74477506e-01 -4.61173624e-01 -8.25446725e-01\n",
      "  2.58527458e-01 -8.41916919e-01  4.56007384e-02 -2.40224048e-01\n",
      " -8.48624825e-01 -3.94047260e-01 -7.73446560e-01 -5.75436354e-01\n",
      " -1.39452860e-01 -8.86936545e-01 -5.72652340e-01 -4.34414804e-01\n",
      " -6.22672856e-01 -3.63038152e-01  4.19669569e-01 -9.28427041e-01\n",
      "  2.99207717e-02 -8.51373494e-01 -3.91516745e-01 -5.74731827e-01\n",
      " -2.66921461e-01 -3.74498636e-01 -4.37549859e-01 -3.96071553e-01\n",
      " -5.71720839e-01 -1.00134444e+00 -3.10186744e-01 -8.34133983e-01\n",
      " -2.68127501e-01  1.04084134e-01 -3.27035248e-01 -9.30221140e-01\n",
      "  6.54694289e-02  5.76792434e-02 -1.30732641e-01 -6.05253637e-01\n",
      " -3.90545100e-01 -2.72843242e-01 -2.72876292e-01 -8.73171091e-01\n",
      " -1.20389950e-03 -9.12513316e-01  3.49577934e-01 -7.08358884e-01\n",
      " -7.54949510e-01  2.85455167e-01 -6.21404111e-01 -3.77747655e-01\n",
      " -2.96208739e-01  8.32369328e-01 -3.55337888e-01 -6.58312261e-01\n",
      " -2.87702709e-01  1.12266645e-01 -7.89262354e-01 -1.22761931e-02\n",
      "  4.07270163e-01 -1.12243664e+00 -5.41134715e-01  3.50778908e-01\n",
      "  2.39037290e-01 -7.01857448e-01 -1.68991499e-02  1.45351410e-01\n",
      "  9.17894781e-01  3.05213749e-01  9.19689596e-01  9.08794045e-01\n",
      "  1.09876931e+00  7.23745823e-01  4.76376116e-01  6.39638245e-01\n",
      "  9.52427387e-01  8.75231266e-01  5.68926275e-01  5.22357047e-01\n",
      "  1.12356043e+00  1.08163309e+00  8.64181995e-01  9.11013246e-01\n",
      "  4.73929763e-01  7.88258016e-01  1.04415369e+00  4.83192652e-01\n",
      "  7.99630225e-01  1.05234110e+00  7.87657499e-01  7.47481346e-01\n",
      "  8.80119622e-01  5.10223627e-01  4.31594193e-01  7.35950053e-01\n",
      "  1.12268972e+00  8.74645114e-01  8.44250500e-01  9.12880957e-01\n",
      "  4.68361527e-01  1.04291677e+00  2.96991259e-01  5.65437078e-01\n",
      "  8.44426751e-01  1.05946171e+00  9.22049761e-01  7.08088636e-01\n",
      "  5.06038725e-01  6.21681273e-01  1.03531122e+00  8.15054536e-01\n",
      "  7.65616298e-01  1.10223210e+00  9.54549253e-01  8.00566494e-01\n",
      "  1.04949033e+00  1.10632694e+00  4.38171446e-01  9.17249978e-01\n",
      "  9.24128771e-01  9.29877281e-01  6.74529433e-01  1.03213143e+00\n",
      "  7.43750215e-01  7.62042880e-01  1.02451003e+00  5.41732311e-01\n",
      "  1.19637203e+00  5.77746928e-01  1.18598568e+00  9.47232544e-01\n",
      "  6.20944440e-01  8.69306743e-01  6.58339739e-01  8.05457294e-01\n",
      "  7.31985033e-01  8.79606426e-01  3.95408690e-01  9.46380556e-01\n",
      "  9.11358714e-01  6.87578976e-01  3.48379761e-01  1.15684938e+00\n",
      "  9.81777608e-01  8.06710601e-01  6.23995066e-01  8.78379166e-01\n",
      "  9.15470243e-01  7.04730749e-01  1.17644131e+00  9.26860273e-01\n",
      "  6.76031590e-01  1.13023162e+00  7.77007043e-01  6.21999085e-01\n",
      "  1.08132005e+00  5.95517755e-01  8.78509879e-01  7.70970643e-01\n",
      "  7.66494036e-01  1.10198724e+00  7.18274057e-01  6.58700526e-01\n",
      "  6.96577966e-01  9.27316725e-01  4.83007908e-01  8.57969820e-01\n",
      "  1.22871661e+00  5.47161698e-01  4.61067617e-01  9.73731458e-01\n",
      "  9.48531210e-01  7.32739389e-01  8.30657899e-01  9.31392610e-01\n",
      "  1.10498738e+00  7.91406751e-01  5.37600517e-01  9.15427864e-01\n",
      "  9.73789811e-01  7.20302165e-01  6.43887222e-01  7.18353033e-01\n",
      "  8.31350505e-01  1.04740584e+00  8.02220345e-01  1.04133761e+00\n",
      "  6.11359656e-01  9.02345002e-01  7.78215110e-01  7.39936590e-01\n",
      "  8.07642758e-01  5.39661646e-01  7.59232283e-01  6.14476919e-01\n",
      "  7.78898075e-02 -1.29310653e-01  8.03017169e-02 -3.15531231e-02\n",
      "  1.50386512e-01 -9.82759818e-02 -1.50209228e-02  7.37015158e-02\n",
      "  1.77104801e-01 -2.56790698e-01  9.27448645e-02 -1.79257349e-03\n",
      "  1.55107364e-01 -7.89592862e-02  3.47494408e-02 -1.78522095e-02\n",
      "  1.04322724e-01 -1.05017334e-01 -2.67082676e-02 -7.80683160e-02\n",
      "  1.00397497e-01  1.87211230e-01  3.28818038e-02 -1.02109224e-01\n",
      " -1.25460356e-01  7.79518634e-02  1.31986976e-01  5.12860641e-02\n",
      "  3.28709155e-01 -1.97820500e-01 -3.03381160e-02 -2.94898510e-01\n",
      "  5.16858995e-02 -1.46077707e-01  1.04939818e-01  1.64387956e-01\n",
      " -1.17474928e-01  2.21774623e-01  1.28781170e-01 -2.05411583e-01\n",
      " -7.64013082e-02  6.42900988e-02  2.67809983e-02 -1.04076847e-01\n",
      " -1.73495784e-01 -1.88268393e-01  7.54975155e-02  4.61228229e-02\n",
      " -1.24401916e-02 -2.69287556e-01  9.76404324e-02 -2.62329519e-01\n",
      "  2.83421397e-01  2.48541925e-02 -6.27993196e-02  1.11926422e-01\n",
      "  1.06810175e-01 -1.00737683e-01 -1.71787411e-01  1.27018675e-01\n",
      " -2.46644944e-01 -4.69377972e-02  2.19958827e-01  1.08819313e-01\n",
      " -1.83934495e-01 -7.00900257e-02  1.91190273e-01 -1.13211781e-01\n",
      " -2.96564744e-04 -6.11566566e-02  1.34774059e-01  5.52265979e-02\n",
      "  1.91067919e-01 -1.00459732e-01  6.19981661e-02 -2.20161781e-01\n",
      " -8.77257735e-02 -9.60770436e-03 -2.57073015e-01  1.03254020e-01\n",
      " -2.45282426e-02  9.94491354e-02 -2.55370080e-01 -8.90722424e-02\n",
      " -1.65935054e-01 -2.07740158e-01  9.30857882e-02  1.31015763e-01\n",
      "  1.84823349e-01  4.67996560e-02  1.04653880e-01  6.39820546e-02\n",
      "  1.41738638e-01  2.95522839e-01  3.66198540e-01 -1.20012328e-01\n",
      " -2.06109881e-01 -2.53035307e-01 -2.02118605e-01 -5.47216311e-02\n",
      "  1.53108314e-01  9.14018229e-02  1.47958949e-01  6.59126788e-02\n",
      "  2.31563941e-01 -8.20729807e-02  2.59882718e-01 -2.22814269e-02\n",
      "  1.67410418e-01 -2.55877405e-01 -1.62030220e-01  7.28857443e-02\n",
      "  3.04223876e-02 -2.25747123e-01 -6.37524202e-03 -1.15245476e-01\n",
      "  5.91287501e-02  3.00159305e-01 -1.06517367e-01 -2.35609487e-01\n",
      " -5.03911786e-02  2.07264692e-01 -3.45902666e-02 -1.32120907e-01\n",
      "  2.95035064e-01  6.35881424e-02  1.74358711e-01 -1.51716799e-01\n",
      "  7.34041333e-01  4.78976108e-02 -6.51111484e-01 -8.73058379e-01\n",
      " -4.94680032e-02 -5.07706523e-01 -5.20069957e-01 -1.41504312e+00\n",
      "  1.25258297e-01  2.74877220e-01 -3.15097332e-01 -4.38997775e-01\n",
      " -9.65409502e-02 -7.37402022e-01 -9.14912879e-01 -5.68778098e-01\n",
      " -7.36623049e-01 -8.25458646e-01 -2.79131144e-01 -5.84343672e-01\n",
      " -6.69367611e-01  3.48014683e-01 -1.28315046e-01 -2.81176865e-01\n",
      " -6.88159287e-01 -6.22236133e-01 -5.09640753e-01 -1.30536830e+00\n",
      " -1.68674171e-01 -5.84783494e-01 -1.13566302e-01  3.35350782e-01\n",
      "  2.48319045e-01 -5.30353725e-01 -5.12279451e-01 -9.95988488e-01\n",
      "  1.12732247e-01 -5.00400245e-01  1.42794654e-01  1.54750437e-01\n",
      " -6.50155425e-01 -6.17969930e-01 -5.10058284e-01 -8.44399750e-01\n",
      " -6.77862704e-01 -4.98753160e-01 -1.00090170e+00 -7.21891701e-01\n",
      " -5.52811384e-01 -6.79550827e-01 -7.26854384e-01  1.05646610e-01\n",
      "  4.96244192e-01 -2.82864928e-01 -7.35960364e-01 -5.12522221e-01\n",
      " -5.82242012e-01 -8.28102946e-01  1.39642805e-01 -8.64732742e-01\n",
      " -5.86170197e-01 -6.81076586e-01 -5.60858548e-01 -8.08248043e-01\n",
      "  2.83656716e-01 -7.13754654e-01 -2.64713168e-01 -3.20015013e-01\n",
      " -5.83389878e-01 -5.66259801e-01 -8.39858651e-01 -7.62770891e-01\n",
      "  1.68610010e-02 -7.25989103e-01 -4.71817076e-01 -3.24176818e-01\n",
      " -6.66392148e-01 -3.90333056e-01  2.08527222e-01 -7.21105874e-01\n",
      "  1.74640685e-01 -5.12428641e-01 -6.71149850e-01 -5.67735493e-01\n",
      " -4.97677684e-01 -3.84148091e-01 -3.15360427e-01 -5.96419275e-01\n",
      " -5.68072081e-01 -8.31252575e-01 -4.99224722e-01 -7.56011188e-01\n",
      " -3.70420158e-01  3.49355787e-01  7.19171882e-01 -7.39527345e-01\n",
      "  1.18289091e-01  1.69727996e-01 -1.26055449e-01 -5.95902741e-01\n",
      " -4.94922936e-01 -1.04770291e+00  1.71279218e-02 -7.54470587e-01\n",
      "  1.22124236e-02 -6.87544346e-01  3.18494678e-01 -8.46292675e-01\n",
      " -6.86101496e-01  3.67124319e-01 -5.83443403e-01 -5.81214905e-01\n",
      " -2.98192948e-01  9.43915248e-01 -6.36975050e-01 -9.64621425e-01\n",
      " -7.42693841e-02  2.90716946e-01 -7.18294740e-01  1.06331401e-01\n",
      " -5.23707986e-01 -6.70327306e-01 -4.24878836e-01  3.08759034e-01\n",
      "  5.14854431e-01 -6.51353538e-01  1.21491954e-01  6.07607253e-02]\n",
      "------------------\n",
      "Weight 5 :\n",
      "[[-0.0843718  -0.07147084  0.11554036 ...  0.7863695   0.23751074\n",
      "  -1.0149521 ]\n",
      " [ 0.28694317  0.22682989 -0.41300336 ...  0.00784772 -0.23457989\n",
      "  -0.15082762]\n",
      " [ 0.14587829 -0.16345721 -0.27279058 ... -0.07355765  0.26966143\n",
      "   0.13928588]\n",
      " ...\n",
      " [-0.2856086  -0.5266594   0.05408552 ... -0.0877531  -0.08764951\n",
      "  -0.0335483 ]\n",
      " [-0.0931338   0.7064032   0.8141955  ... -0.27389207 -0.14203097\n",
      "   0.43689495]\n",
      " [-0.40148413  0.22173871  0.2588049  ... -0.5593638   0.32272986\n",
      "  -0.02976936]]\n",
      "------------------\n",
      "Weight 6 :\n",
      "[[ 0.16771412 -0.0424303   0.01803928 ... -0.2516509  -0.27494037\n",
      "  -0.15427974]\n",
      " [-0.05006068 -0.05391655  0.14457579 ... -0.13520458  0.3512528\n",
      "  -0.1276879 ]\n",
      " [ 0.03102611  0.25072655  0.33499387 ... -0.15566526  0.28773746\n",
      "  -0.09062321]\n",
      " ...\n",
      " [ 0.17523664 -0.27916166  0.06211378 ... -0.13183837  0.28955814\n",
      "  -0.18350337]\n",
      " [ 0.1110422   0.7157765  -0.5246471  ...  0.20650966  0.11882507\n",
      "  -0.04445471]\n",
      " [-0.10120005  0.21697603 -0.1025054  ...  0.09294035  0.22518127\n",
      "  -0.09449345]]\n",
      "------------------\n",
      "Weight 7 :\n",
      "[ 0.17977476 -0.4035594   0.01743279 -0.4311177   0.37323004 -0.00792537\n",
      "  0.05509212 -0.16442463  0.0903267   0.3060835  -0.12597309 -0.20812355\n",
      " -0.34436446  0.34361035 -0.09832283 -0.11515924  0.37326863 -0.2651166\n",
      " -0.14155108 -0.04173827 -0.06246444  0.07222582 -0.3589594  -0.25200993\n",
      " -0.08603784 -0.1202435  -0.34103808  0.00515457 -0.47353607  0.38273883\n",
      " -0.06656709 -0.03446738 -0.06453997 -0.07252049  0.12212822 -0.18543781\n",
      "  0.29036528  0.15476252 -0.45616356 -0.08761035  0.3466504   0.2744684\n",
      "  0.18549642 -0.27848288 -0.00484551 -0.5301952   0.7354691  -0.47893563\n",
      " -0.45315075 -0.5487637  -0.34772518  0.10579559 -0.05542661 -0.23762989\n",
      " -0.02780646 -0.11532084  0.21823737 -0.14197066 -0.08097333 -0.16866596\n",
      "  0.40277156  0.13962327  0.4450889  -0.5320643   0.994673    1.0043743\n",
      "  0.84456515  1.0713619   0.8317943   0.9911339   0.5999527   0.8696659\n",
      "  1.151933    0.89802414  0.7659327   0.9008611   0.8553085   0.4956577\n",
      "  1.0530522   1.1060629   1.0282567   1.0142016   0.75596535  0.75587153\n",
      "  0.9543353   1.1146582   1.0824064   0.66050416  1.2096074   0.7077856\n",
      "  0.96560055  1.0114981   0.78170854  0.689592    0.7828745   0.9546504\n",
      "  1.0159405   0.92587996  1.1242831   0.99479705  0.71656567  0.9910595\n",
      "  0.85402     1.2063241   0.79490274  0.6310846   1.0239154   1.017938\n",
      "  0.81560796  0.88528425  0.5826898   1.0464258   1.0187471   0.95032555\n",
      "  0.8034517   1.1138829   1.1635356   0.8958558   0.89073944  1.0797255\n",
      "  0.7843734   0.9793104   0.85379726  0.92335284  1.730193    0.68291765\n",
      "  0.8884028   0.898398    0.5122821  -0.05436929  0.11338671  0.21873425\n",
      " -0.08524437  0.15743075  0.10208621  0.08842886 -0.35283062  0.26424393\n",
      " -0.203616   -0.04086689  0.02354429 -0.29451317 -0.19120654  0.30596888\n",
      " -0.26040015  0.2504209  -0.04324537  0.15579955  0.5246044  -0.31592494\n",
      " -0.16347377 -0.11983542  0.301401    0.12496603  0.02090223 -0.30700436\n",
      "  0.0685413   0.17508446  0.244162    0.24188024  0.14687324  0.00610461\n",
      " -0.3289554  -0.24182823 -0.33601704 -0.14870113 -0.28908163  0.36788115\n",
      "  0.3430166  -0.25550207  0.03709726 -0.0295353  -0.19623493 -0.03989896\n",
      " -0.63526493  0.22849444 -0.1413649  -0.09220126  0.09236595 -0.29122087\n",
      "  0.26264992 -0.30723584 -0.0410232   0.17887312  0.2062057  -0.16900295\n",
      "  0.0189558  -0.03534946 -0.6733737   0.6074554  -0.23974572 -0.13356596\n",
      "  0.23700693 -0.15107502  0.26827398 -0.38620698  0.7377149  -0.32141003\n",
      "  0.7419756  -0.04723556 -0.08541664  0.26267144 -0.3322221   0.15272547\n",
      " -0.02614386  0.67401123 -0.23376915  0.00495618  0.4641074  -0.22181164\n",
      "  0.23813847 -0.13362797 -0.24223404 -0.1095623   0.0716278  -0.22634338\n",
      "  0.00717563  0.37983602 -0.37117457  0.3029678  -0.41383368  0.7190437\n",
      "  0.24937889 -0.13390362 -0.21690002 -0.14856952  0.18154977  0.0290264\n",
      "  0.5303389   0.15752517 -0.30727276 -0.131323    0.23692206  0.49328768\n",
      "  0.21074878 -0.5851375   0.18301937 -0.30589673 -0.00811572 -0.39242744\n",
      " -0.21400301 -0.302091   -0.16007325 -0.06204519 -0.05371464 -0.15100992\n",
      "  0.00244308  0.01722969  0.03180283  0.00893053 -0.0570077  -0.04021788\n",
      "  0.34318748 -0.13047956  0.10001224 -0.308464  ]\n",
      "------------------\n",
      "Weight 8 :\n",
      "[[-0.5701282  -0.45878425 -0.49677384 ... -0.61039495  0.19997486\n",
      "  -0.64958686]\n",
      " [ 0.19665071  0.15063722  0.15807833 ...  0.5029643   0.05682348\n",
      "   0.13956985]\n",
      " [ 0.2962693   0.33666468 -0.02156884 ...  0.18000707 -0.09490304\n",
      "   0.21521147]\n",
      " ...\n",
      " [ 0.6193777   1.4344507   0.1253     ...  0.4675412   0.26579356\n",
      "   0.374194  ]\n",
      " [ 0.33830193 -0.3721189  -0.23921669 ... -0.19028169 -0.3413478\n",
      "  -0.23498444]\n",
      " [ 0.38174543  0.44249973  0.03857058 ...  0.3593896  -0.35119107\n",
      "   0.09250642]]\n",
      "------------------\n",
      "Weight 9 :\n",
      "[[ 0.10547796  0.03230948 -0.08032527 ...  0.03865166  0.12262574\n",
      "   0.00863776]\n",
      " [-0.06890313 -0.0190651   0.2547867  ...  0.29769036 -0.11182607\n",
      "   0.04974468]\n",
      " [-0.14080876  0.26331776  0.22082661 ...  0.09886922 -0.5434175\n",
      "   0.08520929]\n",
      " ...\n",
      " [ 0.29660973  0.0422513  -0.18123184 ...  0.28109577 -0.592608\n",
      "  -0.02599967]\n",
      " [-0.00232488 -0.30562916 -0.24908423 ... -0.02350728  0.29117066\n",
      "  -0.13378966]\n",
      " [ 0.12492286  0.13272877  0.06123176 ... -0.09921329 -0.24035825\n",
      "  -0.02471262]]\n",
      "------------------\n",
      "Weight 10 :\n",
      "[ 2.27054343e-01  5.56548357e-01 -9.49069578e-03  9.46752355e-02\n",
      " -1.59727573e-01  3.63887399e-01  5.68827428e-02 -1.01285309e-01\n",
      "  9.94442627e-02  1.62490327e-02  1.18657167e-03  9.37150195e-02\n",
      " -2.25162119e-01  6.76931292e-02 -1.12091921e-01  5.09091914e-01\n",
      "  5.08627892e-02 -1.30631700e-01 -5.66075463e-03 -2.73143947e-01\n",
      " -1.26885965e-01 -4.21106853e-02  2.09509686e-01 -2.12838259e-02\n",
      "  2.26972178e-01 -7.03507066e-02  5.37827134e-01 -1.59128845e-01\n",
      " -3.58736157e-01 -4.09614921e-01 -3.32548112e-01  7.72180438e-01\n",
      "  9.49689686e-01  9.07135010e-01  1.01339459e+00  9.40146565e-01\n",
      "  1.01254058e+00  9.90220964e-01  1.11130381e+00  9.66467619e-01\n",
      "  8.92996609e-01  8.84584129e-01  1.21765590e+00  8.95853281e-01\n",
      "  9.71969903e-01  1.04146671e+00  1.07222652e+00  7.87308216e-01\n",
      "  8.09819877e-01  8.41776848e-01  9.22008574e-01  9.61332500e-01\n",
      "  9.98883843e-01  8.32948148e-01  6.31197393e-01  9.83343720e-01\n",
      "  8.76808167e-01  1.00453496e+00  5.27035832e-01  9.27945077e-01\n",
      "  5.94404340e-01  7.93712199e-01  1.15679741e+00  8.88477325e-01\n",
      " -8.52452368e-02 -3.03090572e-01 -2.54911073e-02 -5.13255633e-02\n",
      "  2.22173065e-01 -6.92812800e-01  5.90155311e-02 -1.15450367e-01\n",
      " -7.50953197e-01 -1.22848444e-01 -2.29193226e-01  6.42309263e-02\n",
      "  2.81576067e-02  7.43342042e-02 -7.26361498e-02  7.92588711e-01\n",
      "  4.97556925e-02  7.56997392e-02  4.95476462e-02  5.76312095e-02\n",
      " -1.79228276e-01  3.09022635e-01 -4.14200634e-01 -2.00242236e-01\n",
      " -1.84384003e-01  1.34668157e-01  2.19981000e-01  1.75929636e-01\n",
      "  2.77597129e-01  7.22693354e-02 -3.75298619e-01  1.82652175e-02\n",
      "  2.65088845e-02  3.38026196e-01 -1.64176628e-01  6.28065243e-02\n",
      " -1.00244954e-01  9.99206230e-02  2.69204527e-02 -2.45601103e-01\n",
      "  9.24185570e-03 -1.17015034e-01 -1.13191254e-01  1.18682915e-02\n",
      " -4.30925097e-03  2.70800292e-02 -1.27329841e-01  9.78569463e-02\n",
      " -5.67521621e-03 -6.16220646e-02 -5.00628091e-02 -2.97902375e-01\n",
      " -4.96832915e-02  1.37814492e-01 -2.04409450e-01 -5.30920923e-02\n",
      "  5.47310673e-02 -1.81565359e-02  1.36353940e-01 -9.83493105e-02\n",
      " -2.70515531e-01 -1.67987496e-01  2.67179757e-02  3.84963192e-02]\n",
      "------------------\n",
      "Weight 11 :\n",
      "[[-9.24384743e-02 -7.82243069e-03  1.15779586e-01  6.29517734e-01\n",
      "   1.29987148e-03  2.73215562e-01  7.61428952e-01 -4.98915404e-01]\n",
      " [-1.68028548e-01  1.58803135e-01  5.77675886e-02 -6.18871510e-01\n",
      "  -4.66669351e-03  3.89077991e-01 -8.19054961e-01 -1.36466637e-01]\n",
      " [ 5.21557808e-01  1.25181973e-01  6.24076743e-03 -8.21756661e-01\n",
      "   8.27903926e-01  8.27903897e-02 -7.78290153e-01  2.80627102e-01]\n",
      " [-1.71673387e-01  2.34618813e-01  1.05625674e-01 -7.89275050e-01\n",
      "  -5.85139021e-02  1.00932166e-01 -1.04202557e+00 -1.20794408e-01]\n",
      " [-2.14182243e-01  2.20062524e-01 -2.31986150e-01 -9.00608122e-01\n",
      "   3.24315727e-01  7.42888808e-01 -1.03492582e+00 -3.12775709e-02]\n",
      " [ 1.34293914e-01 -9.52363238e-02  2.42579073e-01 -9.05345261e-01\n",
      "   2.21939012e-01  7.04867169e-02 -7.36051679e-01 -3.42790224e-02]\n",
      " [ 4.17564720e-01 -2.50379503e-01  3.68386179e-01 -8.82953823e-01\n",
      "   2.91428596e-01  3.44847083e-01 -8.61103773e-01  5.59694827e-01]\n",
      " [-3.62852663e-01  3.13825130e-01  2.85329342e-01  1.02114916e+00\n",
      "  -2.16242492e-01 -2.04257965e-01  1.13654804e+00  4.26439732e-01]\n",
      " [-2.08514184e-02  3.38565975e-01 -8.86810869e-02 -8.18219006e-01\n",
      "  -2.36903168e-02 -1.33118834e-02 -3.11842293e-01 -3.44517350e-01]\n",
      " [ 3.97099972e-01 -1.53940141e-01 -4.28527683e-01 -5.82482755e-01\n",
      "   1.04514802e+00 -1.76794484e-01 -7.13947535e-01  9.46820855e-01]\n",
      " [-4.12868589e-01  3.63276660e-01  2.34414190e-02  9.71533000e-01\n",
      "  -3.98054361e-01  4.61728573e-01  8.69883299e-01 -3.55828941e-01]\n",
      " [ 3.79278064e-02 -1.70961529e-01  1.28196161e-02 -9.61773992e-01\n",
      "   4.19167757e-01 -3.57191026e-01 -8.61825228e-01  3.44135821e-01]\n",
      " [-8.34136233e-02 -2.35379040e-01  2.22245585e-02 -8.57184529e-01\n",
      "   1.74663171e-01  5.95559359e-01 -1.04688752e+00  2.28455380e-01]\n",
      " [ 5.57575107e-01  1.82618067e-01  3.36157203e-01 -9.68300879e-01\n",
      "   1.07261680e-01  2.81634361e-01 -7.86851227e-01  3.89166743e-01]\n",
      " [-4.41195339e-01  3.75996418e-02  3.41061428e-02  8.65381300e-01\n",
      "  -5.22679746e-01 -1.05282031e-01  9.93559361e-01 -2.46300995e-01]\n",
      " [ 2.58067548e-01  2.48648584e-01 -1.88950449e-02  7.27382720e-01\n",
      "   3.19572777e-01  1.32433712e-01  5.61507225e-01  2.70273209e-01]\n",
      " [ 1.29809650e-02 -2.18976021e-01 -2.61735674e-02 -3.54719579e-01\n",
      "  -4.52134937e-01  4.70062464e-01 -6.31614447e-01 -4.45783943e-01]\n",
      " [-2.11607993e-01 -2.33636171e-01  1.47519976e-01 -8.33286464e-01\n",
      "  -1.43768102e-01  8.67617488e-01 -7.39573121e-01 -4.11621273e-01]\n",
      " [ 1.47668734e-01  5.15265465e-02  2.42111996e-01 -9.14967418e-01\n",
      "   2.66030133e-01 -4.66178149e-01 -1.00330675e+00  7.82049716e-01]\n",
      " [ 6.51327521e-02  3.09901148e-01  3.77212524e-01 -6.83319092e-01\n",
      "   4.01072860e-01 -3.31733972e-01 -8.75054419e-01  7.20025778e-01]\n",
      " [-2.61915356e-01 -1.86996832e-01 -3.30999047e-02  5.89282274e-01\n",
      "  -3.32451403e-01 -3.59720588e-01  8.80412996e-01 -3.48956615e-01]\n",
      " [ 2.13571846e-01  1.05926469e-01  5.83423190e-02 -1.27262521e+00\n",
      "   1.00235872e-01  8.84438574e-01 -1.05296898e+00 -3.09021205e-01]\n",
      " [-3.22950125e-01  4.89538088e-02 -4.36552107e-01 -8.17930877e-01\n",
      "  -4.44777489e-01  7.74292871e-02 -4.51403648e-01 -5.90060651e-01]\n",
      " [ 1.72493055e-01  1.59845445e-02  3.17961603e-01  4.85896170e-01\n",
      "   9.00188368e-03 -3.06890726e-01  1.56892896e-01  2.19950959e-01]\n",
      " [ 2.24917188e-01  4.10489976e-01  4.66357827e-01 -6.73300862e-01\n",
      "   9.95507911e-02  1.70086756e-01 -9.63947058e-01  1.89970713e-02]\n",
      " [ 7.75137618e-02 -2.75273979e-01  2.43278220e-01 -3.17070961e-01\n",
      "  -1.66536216e-03  7.04957187e-01 -5.35571575e-01 -2.78515607e-01]\n",
      " [-1.47501245e-01 -3.03008836e-02  2.71955375e-02  1.05282068e+00\n",
      "   4.36456889e-01  1.07233293e-01  1.03863573e+00 -1.78538948e-01]\n",
      " [-9.91209596e-02  1.05446644e-01 -2.41315663e-01 -8.17691922e-01\n",
      "  -2.60398537e-01  6.96661294e-01 -2.98046738e-01  1.75647959e-01]\n",
      " [ 4.91499394e-01 -2.12716460e-01 -1.55734301e-01 -1.79372263e+00\n",
      "  -7.07339942e-02  2.38576457e-01 -1.44198442e+00 -9.09486264e-02]\n",
      " [ 7.25511014e-02  2.16814324e-01  1.04272634e-01 -1.48892272e+00\n",
      "  -6.35569334e-01  5.85193455e-01 -1.37133932e+00 -3.36593330e-01]\n",
      " [-1.46440852e+00 -6.89040869e-02  2.46975392e-01  4.78670657e-01\n",
      "  -4.06111628e-01 -2.43755385e-01  8.02447736e-01 -3.90280485e-01]\n",
      " [-3.61791164e-01  3.14879745e-01 -1.17083162e-01 -7.56921053e-01\n",
      "  -1.34544596e-02 -4.64201748e-01 -3.67996007e-01  1.04055583e-01]]\n",
      "------------------\n",
      "Weight 12 :\n",
      "[-0.02560313 -0.1848232  -0.1316791   0.4355821   0.13035908 -0.09118663\n",
      "  0.71167535  0.15914683]\n",
      "------------------\n",
      "Weight 13 :\n",
      "[[-0.38503534]\n",
      " [-0.5948264 ]\n",
      " [-0.23822136]\n",
      " [ 1.3218926 ]\n",
      " [-0.22675216]\n",
      " [ 0.36280102]\n",
      " [ 0.7136716 ]\n",
      " [-0.20035231]]\n",
      "------------------\n",
      "Weight 14 :\n",
      "[0.4325995]\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "weights = loaded_model.get_weights()\n",
    "\n",
    "for i, weight in enumerate(weights) :\n",
    "  print(f\"Weight {i} :\")\n",
    "  print(weight)\n",
    "  print(\"------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
