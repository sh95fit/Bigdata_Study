{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymysql\n",
    "import dotenv\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalization/Standardization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout, Conv1D, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 경고 무시 코드 추가\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pltconfig_default() :\n",
    "  sns.reset_defaults()\n",
    "  %matplotlib inline\n",
    "\n",
    "pltconfig_default()\n",
    "\n",
    "matplotlib.rcParams\n",
    "\n",
    "matplotlib.rcParams['font.family']\n",
    "\n",
    "current_font_list = matplotlib.rcParams['font.family']\n",
    "\n",
    "font_path = 'C:\\\\Windows\\\\Fonts\\\\batang.ttc'\n",
    "\n",
    "kfont = matplotlib.font_manager.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "matplotlib.rcParams['font.family'] = [kfont] + current_font_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "St_NotEncode_data = pd.read_pickle(\"StandardScalar_final_data\")\n",
    "\n",
    "print(St_NotEncode_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature와 Label 분리하기\n",
    "def Feature_Label(datafile) :\n",
    "    X = datafile.iloc[:,:-1]\n",
    "    y = datafile.iloc[:,-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "print(\"StandardScaler Not Encode Data\")\n",
    "SNE_X, SNE_y = Feature_Label(St_NotEncode_data)\n",
    "print(SNE_X.shape, SNE_y.shape)\n",
    "SNE_X_train, SNE_X_test, SNE_y_train, SNE_y_test = train_test_split(SNE_X, SNE_y, test_size=0.2, random_state=10, shuffle=False)\n",
    "print(SNE_X_train.shape, SNE_X_test.shape, SNE_y_train.shape, SNE_y_test.shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "\n",
    "WINDOW_SIZE=3\n",
    "BATCH_SIZE=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X 값 Window Dataset 구성\n",
    "ds_x = tf.data.Dataset.from_tensor_slices(SNE_X_train).window(WINDOW_SIZE, stride=1, shift=1, drop_remainder=True)  # 여기서는 X값만 별도로 구성하므로 WINDOW_SIZE 값에 1을 더하지 않는다!\n",
    "ds_x = ds_x.flat_map(lambda x : x.batch(WINDOW_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y 값 Window Dataset 구성\n",
    "ds_y = tf.data.Dataset.from_tensor_slices(SNE_y_train[WINDOW_SIZE:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.zip((ds_x, ds_y)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_data.take(1) :\n",
    "  print(x[:3])\n",
    "  print()\n",
    "  print(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([SNE_X_train, SNE_y_train], axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 내용을 함수 형태로 만들고 싶은 경우\n",
    "\n",
    "def windowed_dataset(x, y, window_size, batch_size, shuffle) :\n",
    "  ds_x = tf.data.Dataset.from_tensor_slices(SNE_X_train).window(window_size, stride=1, shift=1, drop_remainder=True) \n",
    "  ds_x = ds_x.flat_map(lambda x : x.batch(window_size))\n",
    "  \n",
    "  ds_y = tf.data.Dataset.from_tensor_slices(SNE_y_train[window_size:])\n",
    "  \n",
    "  ds = tf.data.Dataset.zip((ds_x, ds_y))\n",
    "  \n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(1000)\n",
    "  \n",
    "  return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = windowed_dataset(SNE_X_train, SNE_y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
    "train_data_ns = windowed_dataset(SNE_X_train, SNE_y_train, WINDOW_SIZE, BATCH_SIZE, False)\n",
    "test_data = windowed_dataset(SNE_X_test, SNE_y_test, WINDOW_SIZE, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_data.take(1) :\n",
    "  print(f'{data[0].shape}, {data[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal',\n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 8]),\n",
    "  LSTM(128, activation='tanh', return_sequences=True),  \n",
    "  LSTM(64, activation='tanh', return_sequences=True),\n",
    "  LSTM(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal', \n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 8]),\n",
    "  SimpleRNN(128, activation='tanh', return_sequences=True),  \n",
    "  SimpleRNN(64, activation='tanh', return_sequences=True),\n",
    "  SimpleRNN(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal', \n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 8]),\n",
    "  GRU(128, activation='tanh', return_sequences=True),  \n",
    "  GRU(64, activation='tanh', return_sequences=True),\n",
    "  GRU(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Huber() \n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "lstm_model.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "rnn_model.compile(loss=loss, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse', 'mae'])\n",
    "gru_model.compile(loss=loss, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "       \n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "mc_lstm = ModelCheckpoint('new_multi_lstm_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_lstm_ns = ModelCheckpoint('new_multi_lstm_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_rnn = ModelCheckpoint('new_multi_rnn_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_rnn_ns = ModelCheckpoint('new_multi_rnn_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_gru = ModelCheckpoint('new_multi_gru_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_gru_ns = ModelCheckpoint('new_multi_gru_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_history = lstm_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_lstm])\n",
    "lstm_history_ns = lstm_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_lstm_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_history = rnn_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_rnn])\n",
    "rnn_history_ns = rnn_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_rnn_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_history = gru_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_gru])\n",
    "gru_history_ns = gru_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_gru_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 결과\n",
    "\n",
    "history_list = [\"lstm_history\", \"rnn_history\", \"gru_history\", \"lstm_history_ns\", \"rnn_history_ns\", \"gru_history_ns\"]\n",
    "def result(historys) :\n",
    "  for name, history in globals().items() :\n",
    "    if name in history_list :\n",
    "      print(f\"-------------{name}-------------\")\n",
    "      val_loss = min(history.history['val_loss'])\n",
    "      val_mse = min(history.history['val_mse'])\n",
    "      val_mae = min(history.history['val_mae'])\n",
    "      print(f\"{name} Validation Loss:\", val_loss)\n",
    "      print(f\"{name} Validation MSE:\", val_mse)\n",
    "      print(f\"{name} Validation MAE:\", val_mae)\n",
    "\n",
    "result(history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "St_NotEncode_Basic_data = pd.read_pickle(\"Basic_StandardScalar_final_data\")\n",
    "  \n",
    "print(St_NotEncode_Basic_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(x, y, window_size, batch_size, shuffle) :\n",
    "  ds_x = tf.data.Dataset.from_tensor_slices(SNE_X_train).window(window_size, stride=1, shift=1, drop_remainder=True) \n",
    "  ds_x = ds_x.flat_map(lambda x : x.batch(window_size))\n",
    "  \n",
    "  ds_y = tf.data.Dataset.from_tensor_slices(SNE_y_train[window_size:])\n",
    "  \n",
    "  ds = tf.data.Dataset.zip((ds_x, ds_y))\n",
    "  \n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(1000)\n",
    "  \n",
    "  return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = windowed_dataset(SNE_X_train, SNE_y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
    "train_data_ns = windowed_dataset(SNE_X_train, SNE_y_train, WINDOW_SIZE, BATCH_SIZE, False)\n",
    "test_data = windowed_dataset(SNE_X_test, SNE_y_test, WINDOW_SIZE, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_data.take(1) :\n",
    "  print(f'{data[0].shape}, {data[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal',\n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 8]),\n",
    "  LSTM(128, activation='tanh', return_sequences=True),  \n",
    "  LSTM(64, activation='tanh', return_sequences=True),\n",
    "  LSTM(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal', \n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 8]),\n",
    "  SimpleRNN(128, activation='tanh', return_sequences=True),  \n",
    "  SimpleRNN(64, activation='tanh', return_sequences=True),\n",
    "  SimpleRNN(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = Sequential([\n",
    "  Conv1D(filters=32, kernel_size=30,\n",
    "         padding='causal', \n",
    "         activation='relu',\n",
    "         input_shape=[WINDOW_SIZE, 8]),\n",
    "  GRU(128, activation='tanh', return_sequences=True),  \n",
    "  GRU(64, activation='tanh', return_sequences=True),\n",
    "  GRU(32, activation='tanh'),\n",
    "  Dense(8, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Huber() \n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "lstm_model.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "rnn_model.compile(loss=loss, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse', 'mae'])\n",
    "gru_model.compile(loss=loss, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "       \n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "mc_lstm = ModelCheckpoint('new_multi_lstm_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_lstm_ns = ModelCheckpoint('new_multi_lstm_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_rnn = ModelCheckpoint('new_multi_rnn_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_rnn_ns = ModelCheckpoint('new_multi_rnn_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_gru = ModelCheckpoint('new_multi_gru_weight.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)\n",
    "mc_gru_ns = ModelCheckpoint('new_multi_gru_weight_ns.h5', monitor='val_loss', mode='auto', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_history = lstm_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_lstm])\n",
    "lstm_history_ns = lstm_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_lstm_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_history = rnn_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_rnn])\n",
    "rnn_history_ns = rnn_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_rnn_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_history = gru_model.fit(train_data, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_gru])\n",
    "gru_history_ns = gru_model.fit(train_data_ns, validation_data=test_data, epochs=500, callbacks=[reduce_lr, es, mc_gru_ns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 결과\n",
    "\n",
    "history_list = [\"lstm_history\", \"rnn_history\", \"gru_history\", \"lstm_history_ns\", \"rnn_history_ns\", \"gru_history_ns\"]\n",
    "def result(historys) :\n",
    "  for name, history in globals().items() :\n",
    "    if name in history_list :\n",
    "      print(f\"-------------{name}-------------\")\n",
    "      val_loss = min(history.history['val_loss'])\n",
    "      val_mse = min(history.history['val_mse'])\n",
    "      val_mae = min(history.history['val_mae'])\n",
    "      print(f\"{name} Validation Loss:\", val_loss)\n",
    "      print(f\"{name} Validation MSE:\", val_mse)\n",
    "      print(f\"{name} Validation MAE:\", val_mae)\n",
    "\n",
    "result(history_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
